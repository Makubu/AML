{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction of the problem \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea is to deal with the interaction of a brain user. \n",
    "\n",
    "To do so, a way is to work with the stimulus of the brain signals. \n",
    "\n",
    "A first method is the gaze-base interaction, which consists of reacting after an eye contact of a user to a screen. \n",
    "\n",
    "\n",
    "Nevertheless, one problem that has raised is the differentiation of a simple look and the command. It has led to issues where whenever the user looked, the command was pressed. \n",
    "\n",
    "Some methods were explored such as introducing a threshold of time : whenever a user stays on a given button more than the threshold, it would trigger it. Such method wasn't relevant as the user experience wasn't pleasant.\n",
    "Then, it has been experienced a confirmation setup, which required too much attention to the user. \n",
    "Afterthat, it has been explored the mental click. The use of motor imagery or mental concentration wasn't efficient.\n",
    "\n",
    "Moreover, another approach has been the use of EEG patterns that suits the gaze based control. This approach is a passive one, where the user doesn't need to explicitly do something. It takes into account the brain signals that accompany the brain activity.  \n",
    "\n",
    "\n",
    "Finally, the approach that has been developped is the FPR (Fixation-Related-Potential ) markers where the intention is more find a target rather than to act. This approach added to the eye tracking has been shown to be efficient to find an object, area or position of interest. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was done is a study on 8 different individuals (1 female and people from 21 to 48 yo) whom 2 of those didn't have experiences in gaze based control. \n",
    "\n",
    "They used Electroencephalogram (EEG) at 500 Hz and 24 bit voltage resolution. A pre-process was executed to synchronize them with the eye gaze data with custom programs.\n",
    "\n",
    "For the online detections, the gaze position range shouldn't exceed 2Â° for 500 - 1000 ms on X and Y. With this criteria and two others (no dwell detected in a 3x3 squared region centered in the previous click for the previous 3000 ms and no dwell detection in any position for 500 ms or 1000 ns depending on the convention), a click was simulated in the game. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaze controlled game\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to collect EEG, the experience was realised within a game. The goal is to align color balls in lines. \n",
    "\n",
    "To simulate the gaze dwells, a button was added to prevent unwanted actions. To start the gaze detection, a user should gaze dwell this button.\n",
    "\n",
    "The participants had three different options : fixation on a **button**, a **ball** and a **free cell**. The threshold was 500 or 1000 ms and lead, after each action, to a reaction according what was decided.\n",
    "\n",
    "To prevent the participants to be too fast, after 4-8 moves, there was a break of 10 seconds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User participation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The participants were seated in a chair in front of the computer. For those who have never experienced this game, they had the opportunity to play 2 games of 2 minutes before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
