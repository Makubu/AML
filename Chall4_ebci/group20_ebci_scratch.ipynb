{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Brain-Computer Interface data classification\n",
    "---\n",
    "\n",
    "In this challenge you will focus on a \"guided tour\" to building a statistical model to work with a very peculiar kind of data: as such, this project will require you to spend more time reading scientific papers than coding. The proposed field gives you an opportunity to compare traditional machine learning approaches and Deep Learning. To improve the performance of a baseline model you will have to build/modify existing neural network architectures and you will have to perform such modifications in a reasonable way (i.e. naive stacking of layers will not help). Also, you will have to pay particular attention to model regularization, because you will have a relatively small training dataset.\n",
    "\n",
    "**IMPORTANT**: please refer to the AML course guidelines concerning grading rules. Pay especially attention to the **presentation quality** item, which boils down to: don't dump a zillion of lines of code and plots in this notebook. Produce a concise summary of your findings: this notebook can exist in two versions, a \"scratch\" version that you will use to work and debug, a \"presentation\" version that you will submit. The \"presentation\" notebook should go to the point, and convay the main findings of your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Overview\n",
    "In this challenge, we propose to create a classifier for a Brain-Computer Interface (BCI). Such a device intended to map mental states to computer commands. There are plenty of ways to organize such mapping. The most important feature which characterizes a BCI is the psychophysiological paradigm. A normal person, while interacting with a computer, goes throw a very large number of mental states. It’s impossible to process and classify them all.  The “paradigm” is a way to induce BCI users to a finite number of mental states during the experiment. You may find a survey of BCI paradigms here:\n",
    "\n",
    "https://www.researchgate.net/publication/328974192_A_comprehensive_review_of_EEG-based_brain-computer_interface_paradigms\n",
    "\n",
    "The data-processing pipeline strongly differs between different paradigms. The proposed dataset corresponds to a very unique BCI paradigm: the so-called Eye-Brain-Computer Interface (EBCI). You can read more about this in the following link: \n",
    "\n",
    "https://www.frontiersin.org/articles/10.3389/fnins.2016.00528/full\n",
    "\n",
    "The dataset has been produced based on the survey cited in the link above. A good advice is to have a look at the survey, at least to understand the key ideas and differences between Event-Related-Potential (ERP and its subclass - P300), BCI, Motor Imaging BCI, and Error-Related Potential BCI. This is important because one of the easiest ways to solve the problem of this challenge is by adopting an existing BCI classifier, and adapt it to the proposed dataset. Indeed, some BCI paradigms produce data that strongly differ from EBCI data and its classifiers will work very poorly with the data we use in this challenge. IMPORTANT: you should work only with classifiers that are intended to work with electroencephalography - EEG (NOT MEG, NIRS or (f)MRI).\n",
    "\n",
    "### EBCI paradigm\n",
    "The key idea of EBCI is to replace a computer mouse with an eye tracker and electroencephalography. The eyetracker controls the position of a mouse cursor on the screen and EEG data is used to perform the equivalent of mouse clicks. When a user’s eye focuses on some element of a GUI (a button for example) the EEG classifier should take a decision, if the user has the intention to click on this GUI element or not. Possible classes could be, for example: just reading text on the button, thinking, is it worth to push this big red button, daydreaming or anything like that.\n",
    "\n",
    "### Data collecting procedure (optional reading material)\n",
    "During the experiment, a BCI user has to play a simple game - align balls of one color in lines. A transaction to collect individual data looks as follows. First, the user investigates the playing field and makes a decision about which ball to move. During this part of game, the EEG fragments corresponding to eye focus are collected and labeled as Non-Target (because the user has just been observing the playing field). Then, when a decision is made, the user focuses on the “Control mode ON” button. In this mode, each eye focus of 500 ms length is treated as a mouse click. Then the user focuses on the ball to move (this ball becomes highlighted). Then the user focuses on the free cell of the board to place this ball. These focusing actions are intentional, and the corresponding EEG fragments are labeled as Target. "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data exploration\n",
    "For classification purposes you will have only small fragments of EEG, corresponding to eye focus. In psychophysiology, such key pieces of an EEG called epochs (do not confuse with neural network training epochs). One EEG epoch corresponds to one image, for example in image classification tasks. The EEG record has 19 channels (sensors placed to different parts of the user’s head). The sensors put in 10-20 EEG system, you can find a mapping from channel index to its name and channel coordinates in the order&locations.info file. The coordinates in this file are polar. First column is angle and second column is radius. The zero angle corresponds to the line connecting the top of the head and nose if we will look on the head from above (line between Cz and NASION on the picture below). \n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                       Fig.1. 10-20 electrode placing system."
   ]
  },
  {
   "attachments": {
    "EBCI_average.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The following discussion is a summary of what is described in the BCI paradigms survey linked above. For clarifications, refer to the BCI survey.\n",
    "\n",
    "The sampling frequency of EEG equals 500 Hz. Data were processed with a band-pass filter (0.1 Hz-40 Hz). Each EEG epoch has a length of 1.5 seconds: 0.5 seconds before the focus onset and 1 second after focus onset. Focus onset is the beginning of eye focus. <font color=red>Further, we count the time from the beginning of the focus and not from the beginning of the whole EEG epoch.</font> So, focus onset is zero on the time axis. Only 500ms after focus onset of the EEG epoch corresponds to an eye focus. Additionally, the first 200ms of focus can be polluted with eye-movement artifacts. Due to the peculiarities of the data collecting procedure, these artifacts contain information about the class label. The interface should make a decision exactly after 500ms from the focus onset. Due to the data collection procedure, EEG after 500 ms contains information about labels too. \n",
    "\n",
    "In conclusion, **you can only use** the 200 ms to 500 ms time interval after the focus onset. Using all other parts of the EEG epoch will be <font color=red>considered wrong</font>, because of the label leakage problem mentioned above.\n",
    "![EBCI_average.png](attachment:EBCI_average.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       Fig.2. Data from one of the EEG channels averaged over the target and non-target EEG epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 13 subjects in the dataset (25-38): data for each subject is stored in a folder with the corresponding number. Each folder contains two .mat files - eegNT.mat with non-target EEG epochs and eegT.mat with target EEG epochs. Each file contain a three-dimensional array of shape (Time x Channels x Epochs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is strongly advised to load the data provided with the challenge using the example code available following this [link](https://github.com/LIKANblk/ebci_data_loader).\n",
    "\n",
    "Using the ```DataBuildClassifier``` class in ```data.py```, the ```get_data``` method returns a dictionary with the subject’s number as key and tuples of EEG epochs and labels as data. Here EEG epochs array dimensions will be EEG epochs (Trials) x Time dim x Channel dim. You can have a look at the ```main``` method to see this in action. \n",
    "\n",
    "You don’t have to change any parameters except ```path_to_data``` and ```resample_to```. The latter can be treated as hyperparameter for the classification problem you will work on, because most of the classifiers work with EEG data with a reduced sample rate. So you can vary this parameter from 1 to 500 Hz. But it is suggested to not resample data at a lower frequency than 80 Hz.\n",
    "\n",
    "The following code snippet is an example to help you in this preliminary data loading phase.\n",
    "\n",
    "```python\n",
    "import sys\n",
    "sys.path.append('/home/likan_blk/BCI/data_loader')  #Path to data_loader code, cloned from github\n",
    "from data import DataBuildClassifier\n",
    "\n",
    "data_loader = DataBuildClassifier('/home/likan_blk/BCI/NewData') #Path to directory with data (i.e NewData contatins 25/, 26/ ....)\n",
    "all_subjects = [25, 26,27,28,29,30,32,33,34,35,36,37,38]\n",
    "subjects = data_loader.get_data(all_subjects,shuffle=False, windows=[(0.2,0.5)],baseline_window=(0.2,0.3),resample_to=500)\n",
    "print(subjects.keys())\n",
    "X,y = subjects[25]\n",
    "print(X.shape) #EEG epochs (Trials) x Time x Channels\n",
    "print(y.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious, the ```windows``` parameter contains a list of tuples with start and end time offset(s) of window(s) used for classification (in seconds). In your case, you will use only one window - [(0.2,0.5)].\n",
    "\n",
    "\n",
    "```baseline_window``` - tuple with start and end time offset(s) of window(s) used for baseline correction (in seconds). You can learn more about the baseline correction concept in the Data Preprocessing section.\n",
    "* In general, the only thing you have to do in this section is to plot averaged EEG epochs for each subject for Pz channel with the chosen after Parameters Optimization section sample rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Preprocessing\n",
    "The data you will work with already overcomes basic EEG preprocessing steps - band-pass filtering, electrode referencing, baseline correction and resampling. The only thing you can play with is resampling, implemented in the ```DataBuildClassifier``` class and controlled via the ```resample_to``` parameter. The dimensionality of the data is very large, comparing to the size of the dataset, so it is wise to tune this parameter during Hyperparameter (HP) search once a base model is working for you.\n",
    "\n",
    "\n",
    "_Baseline correction_ is a technique used instead of Standardization or Normalization in EEG processing: it uses a section of the EEG epoch, in which the signal can be considered stable and artifact-free. The signal amplitude is averaged in this interval and subtracted from the corresponding EEG channel. So, this procedure is performed channel-wise.  It is strongly advised to keep the default value of this parameter  ```baseline_window=(0.2,0.3)```. As such, the baseline correction time interval is from 200ms to 300ms. Also, you should not use additional Standardization or Normalization.\n",
    "\n",
    "NOTE: the above discussion pertains to code that is already implemented for you, in the ```get_data``` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model selection\n",
    "**[Hint]** *This is the main part of the challenge. Please, have a look at the paper linked for the baseline architecture suggested at the end of the cell. Make sure that you understand the roles of each part of the proposed Neural Network. To create your own model please use hints from the Extensions and hints section at the end of the notebook.*\n",
    "\n",
    "In this section, we first introduce the basic building blocks to build your statistical model for EEG data. We use *pipeline* as a synonim to represent a compound block of operations that consitute the model.\n",
    "The proposed method is tailored to BCI with short EEG intervals, but its structure is general. Note also that the first stages of the following pipeline can be considered as being part of a Deep neural network architecture.\n",
    "\n",
    " __Spatial filters -> Temporal filters -> Classifier__ \n",
    " \n",
    "\n",
    "__The spatial filter__ is a simple weighted average of the EEG channels. The idea of this filter is to construct a _source_ representation of information, observed on sensors. It is assumed that there are some number of sources of a useful signal (less than the number of channels) that exist in the brain and several spatial filters working in parallel can recover such sources. However, in practice, for the classification task underlying this challenge, such sources are virtual.  The procedure of recovering real, physiologically sensible sources is out of the scope of this challenge.\n",
    "\n",
    "There are plenty of different ways to learn the weights of such filters. There are supervised and unsupervised approaches. The simplest example of learning such a filter is a Common Spatial Patterns - a supervised version of Principal Component Analysis (https://www.youtube.com/watch?v=zsOULC16USU). If you prefer to adopt a Deep Learning approach, such a filter can be considered as a convolutional layer with shape: (Channels x 1) (First dimension - spatial and second dimension - temporal). Such filter convolves all channels together for each point in the time domain,  therefore weight sharing is performed over the temporal dimension. Such a layer trained together with other parts of the classification pipeline via standard backpropagation.\n",
    "\n",
    "\n",
    "__The temporal filter__ is used because different sets of sources produce signals within different frequency bands. A straightforward approach is using several bandpass frequency filters (for example - Butterworth filter of order 5) in parallel. A Deep learning approach can learn more complicated temporal patterns: in this case, you can use a convolutional layer of size (1 x T) (first dimension - spatial and second one - temporal). The length of the filter T, depends on the frequency band you want to learn. In this case, weight sharing is performed both over spatial and temporal dimensions (because T is less then EEG epoch length). T can be chosen during hyperparameter optimization.\n",
    "\n",
    "Note that __Temporal__ and __Spatial__ filter blocks can be swapped i.e some pipelines starts from temporal filters block, and some start from spatial filter blocks.\n",
    "\n",
    "\n",
    "**Classifier**. On top of this pipeline it is possible to use simple linear classifiers, Linear Discriminant Analysis (LDA), SVMs, single layer perceptrons. For example, our colleagues from INRIA proposed to use Rhiemanian geometry for this purpose (https://hal.inria.fr/hal-01394253/document). You can easily find their implementation on GitHub (https://github.com/alexandrebarachant/pyRiemann). \n",
    "\n",
    "The vanilla example of the described pipeline is Filter Bank Common Spatial Patterns (FBCSP)  - (https://www.researchgate.net/publication/281076368_A_Tutorial_on_EEG_Signal_Processing_Techniques_for_Mental_State_Recognition_in_Brain-Computer_Interfaces)\n",
    "\n",
    "All approaches described above have already been implemented for EEG data. You can find them on GitHub and use them as building blocks for your pipeline. \n",
    "\n",
    "**Baseline solution**: For a baseline solution to the model architecture task, you can use the EEGNET_v4 classifier (https://arxiv.org/abs/1611.08024) with tuned hyperparameters. It’s a pretty straightforward implementation of the EEG classification pipeline in terms of convolutional neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parameter Optimisation\n",
    "To tune the hyperparameters of your model (especially those that are specific to time domain) you should be familiar with Signal Processing concepts. As you can see after the Data Exploration section, patterns are characterized by low-frequency oscillations and you have to be sure that you model is capable to work with low-frequencies. \n",
    "If you a not familiar with signal processing, it is always possible to use a brute force approach like Random Search or some more sophisticated techniques like the ones implemented in Optuna (https://optuna.org/) or SMAC (https://github.com/automl/SMAC3). \n",
    "\n",
    "**NOTE**: You should explain the technique of your choice to achieve parameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model evaluation\n",
    "**[Hint]** *Your performance metric is the AUC, which you should use to report your results and eventually compare variants of the baseline pipeline*\n",
    "\n",
    "\n",
    "The testing set is the last 20% EEG epochs of each class of each subject. The code for dividing into train and test data is available from the data exploration section. **NOTE**: the shuffle parameter should be set to ```False``` because EEG epochs are sorted w.r.t time. \n",
    "\n",
    "To evaluate the performance of your model, you should use the ROC AUC as a classification metric. \n",
    "\n",
    "**IMPORTANT**: Do not use the test set for hyperparameter tuning. You should use your training data (the first 80% EEG epochs) and partition it to carve out a  validation set, to be used for hyperparameter tuning. Also, you can use cross-validation.\n",
    "\n",
    "**IMPORTANT**: It is strongly adviced to perform classification experiments **WITHIN** subjects. This means that you will have to train a classifier for each subject separately. However, **the hyperparameters of your pipeline should be the same for all subjects**. \n",
    "\n",
    "Due to physiological reasons, data from each subject represents a separate data domain. In general, it is very difficult to merge data from different subjects without additional methods such as Transfer Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model explanation (optional)\n",
    "This section is additional and related to Parameter Optimization. It will help you to explain the parameters of the model. The easiest thing you can do here is a visualization of the distribution of weights of spatial filters on “human head” like plots (see fig.1) and frequency response of temporal filters closest to network input. If you see the noisy pictures it means that your model focused on artifacts and not the real signal. \n",
    "\n",
    "Another way to prove, that your model is sensible is the t-SNE visualization of the features before the last layer/classifier. Also, you can use techniques from used https://arxiv.org/abs/1611.08024 (section 3.3, DeepLift package) to construct a saliency maps for input data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Baseline\n",
    "Here https://github.com/LIKANblk/AML_EEG_challenge.git you can find PyTorch implementation of the network architecture described in https://arxiv.org/abs/1611.08024. The hyperparameters of the network are tuned to work with EBCI data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Extensions and hints (optional)\n",
    "There are numerous ways to create a model that improves over the baseline suggested above. First of all, it should be noted that this challenge is more about **regularization** rather than a simple classification challenge. Indeed, training sets are small, compared to the dimensionality of each EEG epoch. So it is a good practice to prefer simple models with a small number of trainable parameters. \n",
    "\n",
    "To improve over the baseline, here are a few ideas:\n",
    "\n",
    "1. Make EEGNET_v4 better: \n",
    "    - Careful hyperparameter (HP) tuning. Baseline solution already uses some non-default values for HP, but you can revisit all HP values of the model (step sizes, padding values, filter sizes for pooling and convolution layers, etc)\n",
    "    - You can improve the model with more sophisticated regularization techniques: augment training data with adversarial examples https://arxiv.org/abs/1412.6572 ; Variational Dropout; etc...\n",
    "    - You can make model slightly deeper with an additional separable-convolution block or one block from MobileNet; or wider, to teach network work with temporal patterns of different scales (https://arxiv.org/abs/1603.06995)\n",
    "2. Try neural network architectures from other types of BCI (https://iopscience.iop.org/article/10.1088/1741-2552/ab260c), (https://iopscience.iop.org/article/10.1088/1741-2552/ab0ab5). Chose only networks, suited to work with **short** EEG patterns - ERP, P300, or networks, suited for low-frequency oscillations  (ERN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import DataBuildClassifier\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.utils import single_auc_loging\n",
    "from src.utils import prepare_dirs,write_results_table, separte_last_block\n",
    "from src.model_torch import train_model_eegnet\n",
    "import shutil\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_res_dir = './res/' #Path to save results and training|testing statistics\n",
    "all_subjects = [25,26,27,28,29,30,32,33,34,35,36,37,38]\n",
    "data = DataBuildClassifier('./eeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'resample_to': 369,\n",
    "                 'D': 3,\n",
    "                 'F1': 12,\n",
    "                 'dropoutRate1': 0.52,\n",
    "                 'dropoutRate2': 0.36,\n",
    "                 'lr': 0.00066,\n",
    "                 'norm_rate': 0.275\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = data.get_data(all_subjects,shuffle=False, windows=[(0.2,0.5)],baseline_window=(0.2,0.3),resample_to=params['resample_to'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following block contains a defenition of crossvalidation function and loop, where model created and consequentially applied for each subjects data and per-subject training information saved to corresponding folders in experiment_res_dir. Please look carefully to cv_per_subj_test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_per_subj_test(x,y,params,path_to_subj, test_on_last_block=False, plot_fold_history=False):\n",
    "    model_path = os.path.join(path_to_subj,'checkpoints')\n",
    "    best_val_epochs = []\n",
    "    best_val_aucs = []\n",
    "    folds = 4  # To preserve split as 0.6 0.2 0.2\n",
    "    if test_on_last_block:\n",
    "        x_tr,y_tr,x_tst,y_tst = separte_last_block(x,y,test_size=0.2)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=folds, shuffle=True)\n",
    "    cv_splits = list(cv.split(x_tr, y_tr))\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv_splits):\n",
    "        fold_model_path = os.path.join(model_path, '%d' % fold)\n",
    "        os.makedirs(fold_model_path)\n",
    "        x_tr_fold, y_tr_fold = x_tr[train_idx], y_tr[train_idx]\n",
    "        x_val_fold, y_val_fold = x_tr[val_idx], y_tr[val_idx]\n",
    "        val_history, fold_model = train_model_eegnet(x_tr_fold,y_tr_fold,params,(x_val_fold,y_val_fold),epochs=200,\n",
    "                                                     batch_size=32, shuffle=True,\n",
    "                                                     model_path=os.path.join(fold_model_path,'model{}'.format(fold)))\n",
    "        best_val_epochs.append(np.argmax(val_history['val_auc']) + 1)  # epochs count from 1 (not from 0)\n",
    "        best_val_aucs.append(np.max(val_history['val_auc']))\n",
    "        if plot_fold_history:\n",
    "            single_auc_loging(val_history, 'fold %d' % fold, fold_model_path)\n",
    "\n",
    "    if test_on_last_block:\n",
    "        test_history, final_model = train_model_eegnet(x_tr, y_tr, params, epochs=int(np.mean(best_val_epochs)),\n",
    "                                                       validation_data=(x_tst, y_tst), batch_size=32, shuffle=True,\n",
    "                                                       model_path=os.path.join(path_to_subj,'naive_model'))\n",
    "\n",
    "    single_auc_loging(test_history, 'test_history', path_to_save=path_to_subj)\n",
    "    with codecs.open('%s/res.txt' % path_to_subj, 'w', encoding='utf8') as f:\n",
    "        f.write(u'Val auc %.02f±%.02f\\n' % (np.mean(best_val_aucs),np.std(best_val_aucs)))\n",
    "        f.write('Test auc naive %.02f\\n' % (test_history['val_auc'][-1]))\n",
    "\n",
    "    return {'val_auc':test_history['val_auc'][-1]}, final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test the classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch %d: train loss %f 0 0.6926099913460868\n",
      "Epoch 0: val loss 0.691724\n",
      "\n",
      "Epoch %d: train loss %f 1 0.68243921654565\n",
      "Epoch 1: val loss 0.687896\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6626698289598737\n",
      "Epoch 2: val loss 0.682885\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6572358012199402\n",
      "Epoch 3: val loss 0.675362\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6266382677214486\n",
      "Epoch 4: val loss 0.667810\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6023279215608325\n",
      "Epoch 5: val loss 0.658729\n",
      "\n",
      "Epoch %d: train loss %f 6 0.651098370552063\n",
      "Epoch 6: val loss 0.652065\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5743466785975865\n",
      "Epoch 7: val loss 0.641933\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5544978507927486\n",
      "Epoch 8: val loss 0.631241\n",
      "\n",
      "Epoch %d: train loss %f 9 0.6070108073098319\n",
      "Epoch 9: val loss 0.625580\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5680450328758785\n",
      "Epoch 10: val loss 0.622162\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5677573255130223\n",
      "Epoch 11: val loss 0.607458\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5453748830727169\n",
      "Epoch 12: val loss 0.604141\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5159005224704742\n",
      "Epoch 13: val loss 0.626611\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5690475744860513\n",
      "Epoch 14: val loss 0.615339\n",
      "\n",
      "Epoch %d: train loss %f 15 0.581679322889873\n",
      "Epoch 15: val loss 0.625319\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5377634252820697\n",
      "Epoch 16: val loss 0.588585\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5276728783335004\n",
      "Epoch 17: val loss 0.568216\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5695332161017826\n",
      "Epoch 18: val loss 0.594348\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5377411076000759\n",
      "Epoch 19: val loss 0.589748\n",
      "\n",
      "Epoch %d: train loss %f 20 0.49136573927743094\n",
      "Epoch 20: val loss 0.571549\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5175402845655169\n",
      "Epoch 21: val loss 0.577409\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5009047261306218\n",
      "Epoch 22: val loss 0.634826\n",
      "\n",
      "Epoch %d: train loss %f 23 0.4767913690635136\n",
      "Epoch 23: val loss 0.606909\n",
      "\n",
      "Epoch %d: train loss %f 24 0.48365360498428345\n",
      "Epoch 24: val loss 0.594657\n",
      "\n",
      "Epoch %d: train loss %f 25 0.5236198646681649\n",
      "Epoch 25: val loss 0.586094\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4517619503395898\n",
      "Epoch 26: val loss 0.579810\n",
      "\n",
      "Epoch %d: train loss %f 27 0.47726050870759146\n",
      "Epoch 27: val loss 0.594726\n",
      "\n",
      "Epoch %d: train loss %f 28 0.47831609419413973\n",
      "Epoch 28: val loss 0.554066\n",
      "\n",
      "Epoch %d: train loss %f 29 0.47820766483034405\n",
      "Epoch 29: val loss 0.551224\n",
      "\n",
      "Epoch %d: train loss %f 30 0.521143240588052\n",
      "Epoch 30: val loss 0.579964\n",
      "\n",
      "Epoch %d: train loss %f 31 0.45499888488224577\n",
      "Epoch 31: val loss 0.591831\n",
      "\n",
      "Epoch %d: train loss %f 32 0.45147064328193665\n",
      "Epoch 32: val loss 0.611057\n",
      "\n",
      "Epoch %d: train loss %f 33 0.4587906684194292\n",
      "Epoch 33: val loss 0.587684\n",
      "\n",
      "Epoch %d: train loss %f 34 0.5144981614180973\n",
      "Epoch 34: val loss 0.584340\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4600969212395804\n",
      "Epoch 35: val loss 0.598189\n",
      "\n",
      "Epoch %d: train loss %f 36 0.49713090913636343\n",
      "Epoch 36: val loss 0.604263\n",
      "\n",
      "Epoch %d: train loss %f 37 0.43646434800965445\n",
      "Epoch 37: val loss 0.557552\n",
      "\n",
      "Epoch %d: train loss %f 38 0.4657683713095529\n",
      "Epoch 38: val loss 0.594645\n",
      "\n",
      "Epoch %d: train loss %f 39 0.5003080027444022\n",
      "Epoch 39: val loss 0.636566\n",
      "\n",
      "Epoch %d: train loss %f 40 0.42046256150518146\n",
      "Epoch 40: val loss 0.581984\n",
      "\n",
      "Epoch %d: train loss %f 41 0.476069837808609\n",
      "Epoch 41: val loss 0.572125\n",
      "\n",
      "Epoch %d: train loss %f 42 0.45973997030939373\n",
      "Epoch 42: val loss 0.578744\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4511801174708775\n",
      "Epoch 43: val loss 0.563836\n",
      "\n",
      "Epoch %d: train loss %f 44 0.5121089092322758\n",
      "Epoch 44: val loss 0.613934\n",
      "\n",
      "Epoch %d: train loss %f 45 0.438774864588465\n",
      "Epoch 45: val loss 0.584448\n",
      "\n",
      "Epoch %d: train loss %f 46 0.5257123836449215\n",
      "Epoch 46: val loss 0.578734\n",
      "\n",
      "Epoch %d: train loss %f 47 0.5499345362186432\n",
      "Epoch 47: val loss 0.617746\n",
      "\n",
      "Epoch %d: train loss %f 48 0.47316936935697285\n",
      "Epoch 48: val loss 0.575418\n",
      "\n",
      "Epoch %d: train loss %f 49 0.5609291195869446\n",
      "Epoch 49: val loss 0.612699\n",
      "\n",
      "Epoch %d: train loss %f 50 0.4295585517372404\n",
      "Epoch 50: val loss 0.664737\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4876438762460436\n",
      "Epoch 51: val loss 0.636109\n",
      "\n",
      "Epoch %d: train loss %f 52 0.4582693449088505\n",
      "Epoch 52: val loss 0.620400\n",
      "\n",
      "Epoch %d: train loss %f 53 0.4388072192668915\n",
      "Epoch 53: val loss 0.595076\n",
      "\n",
      "Epoch %d: train loss %f 54 0.42183627613953184\n",
      "Epoch 54: val loss 0.604955\n",
      "\n",
      "Epoch %d: train loss %f 55 0.4392399489879608\n",
      "Epoch 55: val loss 0.621191\n",
      "\n",
      "Epoch %d: train loss %f 56 0.49564866934503826\n",
      "Epoch 56: val loss 0.599887\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4011229681117194\n",
      "Epoch 57: val loss 0.630418\n",
      "\n",
      "Epoch %d: train loss %f 58 0.4121001192501613\n",
      "Epoch 58: val loss 0.629850\n",
      "\n",
      "Epoch %d: train loss %f 59 0.38549510496003286\n",
      "Epoch 59: val loss 0.636636\n",
      "\n",
      "Epoch %d: train loss %f 60 0.3948872664145061\n",
      "Epoch 60: val loss 0.626459\n",
      "\n",
      "Epoch %d: train loss %f 61 0.3811163625546864\n",
      "Epoch 61: val loss 0.632414\n",
      "\n",
      "Epoch %d: train loss %f 62 0.4044413822037833\n",
      "Epoch 62: val loss 0.670758\n",
      "\n",
      "Epoch %d: train loss %f 63 0.4969611380781446\n",
      "Epoch 63: val loss 0.608041\n",
      "\n",
      "Epoch %d: train loss %f 64 0.42842760256358553\n",
      "Epoch 64: val loss 0.626556\n",
      "\n",
      "Epoch %d: train loss %f 65 0.42621313674109323\n",
      "Epoch 65: val loss 0.698195\n",
      "\n",
      "Epoch %d: train loss %f 66 0.4329842584473746\n",
      "Epoch 66: val loss 0.647347\n",
      "\n",
      "Epoch %d: train loss %f 67 0.4063558748790196\n",
      "Epoch 67: val loss 0.645425\n",
      "\n",
      "Epoch %d: train loss %f 68 0.41856235691479277\n",
      "Epoch 68: val loss 0.676482\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4730003220694406\n",
      "Epoch 69: val loss 0.627465\n",
      "\n",
      "Epoch %d: train loss %f 70 0.5142473067556109\n",
      "Epoch 70: val loss 0.633626\n",
      "\n",
      "Epoch %d: train loss %f 71 0.40246737003326416\n",
      "Epoch 71: val loss 0.624318\n",
      "\n",
      "Epoch %d: train loss %f 72 0.4067702463694981\n",
      "Epoch 72: val loss 0.650453\n",
      "\n",
      "Epoch %d: train loss %f 73 0.3760590595858438\n",
      "Epoch 73: val loss 0.614182\n",
      "\n",
      "Epoch %d: train loss %f 74 0.48103449174336027\n",
      "Epoch 74: val loss 0.662143\n",
      "\n",
      "Epoch %d: train loss %f 75 0.47409567662647795\n",
      "Epoch 75: val loss 0.735415\n",
      "\n",
      "Epoch %d: train loss %f 76 0.4296700656414032\n",
      "Epoch 76: val loss 0.655014\n",
      "\n",
      "Epoch %d: train loss %f 77 0.4691618766103472\n",
      "Epoch 77: val loss 0.711134\n",
      "\n",
      "Epoch %d: train loss %f 78 0.41759791118758066\n",
      "Epoch 78: val loss 0.720873\n",
      "\n",
      "Epoch %d: train loss %f 79 0.3687221599476678\n",
      "Epoch 79: val loss 0.667580\n",
      "\n",
      "Epoch %d: train loss %f 80 0.47027958716664997\n",
      "Epoch 80: val loss 0.672658\n",
      "\n",
      "Epoch %d: train loss %f 81 0.3854373778615679\n",
      "Epoch 81: val loss 0.658341\n",
      "\n",
      "Epoch %d: train loss %f 82 0.4049141364438193\n",
      "Epoch 82: val loss 0.686363\n",
      "\n",
      "Epoch %d: train loss %f 83 0.43697150690214975\n",
      "Epoch 83: val loss 0.680096\n",
      "\n",
      "Epoch %d: train loss %f 84 0.42311518107141766\n",
      "Epoch 84: val loss 0.645508\n",
      "\n",
      "Epoch %d: train loss %f 85 0.3840323303427015\n",
      "Epoch 85: val loss 0.626692\n",
      "\n",
      "Epoch %d: train loss %f 86 0.36138678235667093\n",
      "Epoch 86: val loss 0.665246\n",
      "\n",
      "Epoch %d: train loss %f 87 0.37835762330463957\n",
      "Epoch 87: val loss 0.655855\n",
      "\n",
      "Epoch %d: train loss %f 88 0.3941785522869655\n",
      "Epoch 88: val loss 0.661460\n",
      "\n",
      "Epoch %d: train loss %f 89 0.446242766720908\n",
      "Epoch 89: val loss 0.702021\n",
      "\n",
      "Epoch %d: train loss %f 90 0.37460618359701975\n",
      "Epoch 90: val loss 0.666185\n",
      "\n",
      "Epoch %d: train loss %f 91 0.39681987038680483\n",
      "Epoch 91: val loss 0.679795\n",
      "\n",
      "Epoch %d: train loss %f 92 0.350790873169899\n",
      "Epoch 92: val loss 0.828466\n",
      "\n",
      "Epoch %d: train loss %f 93 0.46985338415418354\n",
      "Epoch 93: val loss 0.746606\n",
      "\n",
      "Epoch %d: train loss %f 94 0.3823713404791696\n",
      "Epoch 94: val loss 0.725879\n",
      "\n",
      "Epoch %d: train loss %f 95 0.3828174237694059\n",
      "Epoch 95: val loss 0.699874\n",
      "\n",
      "Epoch %d: train loss %f 96 0.4235993581158774\n",
      "Epoch 96: val loss 0.715135\n",
      "\n",
      "Epoch %d: train loss %f 97 0.35934106579848696\n",
      "Epoch 97: val loss 0.723811\n",
      "\n",
      "Epoch %d: train loss %f 98 0.4396433319364275\n",
      "Epoch 98: val loss 0.695154\n",
      "\n",
      "Epoch %d: train loss %f 99 0.38941086190087454\n",
      "Epoch 99: val loss 0.681858\n",
      "\n",
      "Epoch %d: train loss %f 100 0.3516046277114323\n",
      "Epoch 100: val loss 0.689529\n",
      "\n",
      "Epoch %d: train loss %f 101 0.3567675564970289\n",
      "Epoch 101: val loss 0.692704\n",
      "\n",
      "Epoch %d: train loss %f 102 0.3489240620817457\n",
      "Epoch 102: val loss 0.669274\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3633487480027335\n",
      "Epoch 103: val loss 0.712410\n",
      "\n",
      "Epoch %d: train loss %f 104 0.35135678734098164\n",
      "Epoch 104: val loss 0.678936\n",
      "\n",
      "Epoch %d: train loss %f 105 0.38837973347731997\n",
      "Epoch 105: val loss 0.663903\n",
      "\n",
      "Epoch %d: train loss %f 106 0.355089042867933\n",
      "Epoch 106: val loss 0.705792\n",
      "\n",
      "Epoch %d: train loss %f 107 0.40085463864462717\n",
      "Epoch 107: val loss 0.762604\n",
      "\n",
      "Epoch %d: train loss %f 108 0.3717328267438071\n",
      "Epoch 108: val loss 0.687657\n",
      "\n",
      "Epoch %d: train loss %f 109 0.42771507799625397\n",
      "Epoch 109: val loss 0.711557\n",
      "\n",
      "Epoch %d: train loss %f 110 0.39628446102142334\n",
      "Epoch 110: val loss 0.751044\n",
      "\n",
      "Epoch %d: train loss %f 111 0.39006764548165457\n",
      "Epoch 111: val loss 0.740551\n",
      "\n",
      "Epoch %d: train loss %f 112 0.3649629035166332\n",
      "Epoch 112: val loss 0.635531\n",
      "\n",
      "Epoch %d: train loss %f 113 0.389953545161656\n",
      "Epoch 113: val loss 0.653741\n",
      "\n",
      "Epoch %d: train loss %f 114 0.32951429912022184\n",
      "Epoch 114: val loss 0.686138\n",
      "\n",
      "Epoch %d: train loss %f 115 0.33222584213529316\n",
      "Epoch 115: val loss 0.682290\n",
      "\n",
      "Epoch %d: train loss %f 116 0.33885729100023\n",
      "Epoch 116: val loss 0.746148\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3602418750524521\n",
      "Epoch 117: val loss 0.811844\n",
      "\n",
      "Epoch %d: train loss %f 118 0.3584056464689119\n",
      "Epoch 118: val loss 0.663961\n",
      "\n",
      "Epoch %d: train loss %f 119 0.34624896730695454\n",
      "Epoch 119: val loss 0.696363\n",
      "\n",
      "Epoch %d: train loss %f 120 0.4377482214144298\n",
      "Epoch 120: val loss 0.817562\n",
      "\n",
      "Epoch %d: train loss %f 121 0.43068720187459675\n",
      "Epoch 121: val loss 0.910875\n",
      "\n",
      "Epoch %d: train loss %f 122 0.36718736801828655\n",
      "Epoch 122: val loss 0.647691\n",
      "\n",
      "Epoch %d: train loss %f 123 0.4786440942968641\n",
      "Epoch 123: val loss 0.652465\n",
      "\n",
      "Epoch %d: train loss %f 124 0.4383956789970398\n",
      "Epoch 124: val loss 0.736232\n",
      "\n",
      "Epoch %d: train loss %f 125 0.4212230443954468\n",
      "Epoch 125: val loss 0.838201\n",
      "\n",
      "Epoch %d: train loss %f 126 0.39623785870415823\n",
      "Epoch 126: val loss 0.649165\n",
      "\n",
      "Epoch %d: train loss %f 127 0.35080850550106596\n",
      "Epoch 127: val loss 0.644983\n",
      "\n",
      "Epoch %d: train loss %f 128 0.35230698542935507\n",
      "Epoch 128: val loss 0.697174\n",
      "\n",
      "Epoch %d: train loss %f 129 0.3472824735300882\n",
      "Epoch 129: val loss 0.681974\n",
      "\n",
      "Epoch %d: train loss %f 130 0.347616651228496\n",
      "Epoch 130: val loss 0.656321\n",
      "\n",
      "Epoch %d: train loss %f 131 0.329232571380479\n",
      "Epoch 131: val loss 0.684858\n",
      "\n",
      "Epoch %d: train loss %f 132 0.3247385323047638\n",
      "Epoch 132: val loss 0.680875\n",
      "\n",
      "Epoch %d: train loss %f 133 0.38003626465797424\n",
      "Epoch 133: val loss 0.704315\n",
      "\n",
      "Epoch %d: train loss %f 134 0.36253642610141207\n",
      "Epoch 134: val loss 0.712600\n",
      "\n",
      "Epoch %d: train loss %f 135 0.39404464619500296\n",
      "Epoch 135: val loss 0.727322\n",
      "\n",
      "Epoch %d: train loss %f 136 0.3443393451826913\n",
      "Epoch 136: val loss 0.764015\n",
      "\n",
      "Epoch %d: train loss %f 137 0.3439256399869919\n",
      "Epoch 137: val loss 0.699996\n",
      "\n",
      "Epoch %d: train loss %f 138 0.34197282791137695\n",
      "Epoch 138: val loss 0.757048\n",
      "\n",
      "Epoch %d: train loss %f 139 0.3161780259438923\n",
      "Epoch 139: val loss 0.722406\n",
      "\n",
      "Epoch %d: train loss %f 140 0.332870677113533\n",
      "Epoch 140: val loss 0.703392\n",
      "\n",
      "Epoch %d: train loss %f 141 0.4307842765535627\n",
      "Epoch 141: val loss 0.718858\n",
      "\n",
      "Epoch %d: train loss %f 142 0.2958970751081194\n",
      "Epoch 142: val loss 0.715576\n",
      "\n",
      "Epoch %d: train loss %f 143 0.2949131897517613\n",
      "Epoch 143: val loss 0.737911\n",
      "\n",
      "Epoch %d: train loss %f 144 0.40139774339539663\n",
      "Epoch 144: val loss 0.727324\n",
      "\n",
      "Epoch %d: train loss %f 145 0.5003460326365062\n",
      "Epoch 145: val loss 0.680390\n",
      "\n",
      "Epoch %d: train loss %f 146 0.35301260224410463\n",
      "Epoch 146: val loss 0.747954\n",
      "\n",
      "Epoch %d: train loss %f 147 0.33664982872349875\n",
      "Epoch 147: val loss 0.745907\n",
      "\n",
      "Epoch %d: train loss %f 148 0.3301753657204764\n",
      "Epoch 148: val loss 0.673265\n",
      "\n",
      "Epoch %d: train loss %f 149 0.43294115577425274\n",
      "Epoch 149: val loss 0.684593\n",
      "\n",
      "Epoch %d: train loss %f 150 0.41376117723328726\n",
      "Epoch 150: val loss 0.740789\n",
      "\n",
      "Epoch %d: train loss %f 151 0.454237910253661\n",
      "Epoch 151: val loss 0.694406\n",
      "\n",
      "Epoch %d: train loss %f 152 0.3109678030014038\n",
      "Epoch 152: val loss 0.720209\n",
      "\n",
      "Epoch %d: train loss %f 153 0.30517890410763876\n",
      "Epoch 153: val loss 0.741238\n",
      "\n",
      "Epoch %d: train loss %f 154 0.4727481816496168\n",
      "Epoch 154: val loss 0.721313\n",
      "\n",
      "Epoch %d: train loss %f 155 0.4148705473967961\n",
      "Epoch 155: val loss 0.753558\n",
      "\n",
      "Epoch %d: train loss %f 156 0.5198190765721458\n",
      "Epoch 156: val loss 0.875476\n",
      "\n",
      "Epoch %d: train loss %f 157 0.38464548545224325\n",
      "Epoch 157: val loss 0.653854\n",
      "\n",
      "Epoch %d: train loss %f 158 0.3948817891733987\n",
      "Epoch 158: val loss 0.640707\n",
      "\n",
      "Epoch %d: train loss %f 159 0.35855525732040405\n",
      "Epoch 159: val loss 0.698251\n",
      "\n",
      "Epoch %d: train loss %f 160 0.3223475494555065\n",
      "Epoch 160: val loss 0.698428\n",
      "\n",
      "Epoch %d: train loss %f 161 0.4126697267804827\n",
      "Epoch 161: val loss 0.690003\n",
      "\n",
      "Epoch %d: train loss %f 162 0.3819537503378732\n",
      "Epoch 162: val loss 0.699825\n",
      "\n",
      "Epoch %d: train loss %f 163 0.35047114746911184\n",
      "Epoch 163: val loss 0.696055\n",
      "\n",
      "Epoch %d: train loss %f 164 0.4781746097973415\n",
      "Epoch 164: val loss 0.699532\n",
      "\n",
      "Epoch %d: train loss %f 165 0.33585746799196514\n",
      "Epoch 165: val loss 0.628156\n",
      "\n",
      "Epoch %d: train loss %f 166 0.3614996714251382\n",
      "Epoch 166: val loss 0.703313\n",
      "\n",
      "Epoch %d: train loss %f 167 0.3706140305314745\n",
      "Epoch 167: val loss 0.717633\n",
      "\n",
      "Epoch %d: train loss %f 168 0.34080733571733746\n",
      "Epoch 168: val loss 0.684125\n",
      "\n",
      "Epoch %d: train loss %f 169 0.3637999530349459\n",
      "Epoch 169: val loss 0.687343\n",
      "\n",
      "Epoch %d: train loss %f 170 0.3443684194769178\n",
      "Epoch 170: val loss 0.677256\n",
      "\n",
      "Epoch %d: train loss %f 171 0.3618945138795035\n",
      "Epoch 171: val loss 0.657531\n",
      "\n",
      "Epoch %d: train loss %f 172 0.302799505846841\n",
      "Epoch 172: val loss 0.679431\n",
      "\n",
      "Epoch %d: train loss %f 173 0.3064661898783275\n",
      "Epoch 173: val loss 0.717772\n",
      "\n",
      "Epoch %d: train loss %f 174 0.3348212795598166\n",
      "Epoch 174: val loss 0.686891\n",
      "\n",
      "Epoch %d: train loss %f 175 0.3385579138994217\n",
      "Epoch 175: val loss 0.693107\n",
      "\n",
      "Epoch %d: train loss %f 176 0.4035260081291199\n",
      "Epoch 176: val loss 0.761529\n",
      "\n",
      "Epoch %d: train loss %f 177 0.32233777003628866\n",
      "Epoch 177: val loss 0.677131\n",
      "\n",
      "Epoch %d: train loss %f 178 0.3890525017465864\n",
      "Epoch 178: val loss 0.657551\n",
      "\n",
      "Epoch %d: train loss %f 179 0.33800523621695383\n",
      "Epoch 179: val loss 0.667224\n",
      "\n",
      "Epoch %d: train loss %f 180 0.31884714322430746\n",
      "Epoch 180: val loss 0.671483\n",
      "\n",
      "Epoch %d: train loss %f 181 0.31507555714675356\n",
      "Epoch 181: val loss 0.657140\n",
      "\n",
      "Epoch %d: train loss %f 182 0.34676895822797504\n",
      "Epoch 182: val loss 0.702832\n",
      "\n",
      "Epoch %d: train loss %f 183 0.2923478420291628\n",
      "Epoch 183: val loss 0.706682\n",
      "\n",
      "Epoch %d: train loss %f 184 0.371899745294026\n",
      "Epoch 184: val loss 0.664480\n",
      "\n",
      "Epoch %d: train loss %f 185 0.4395849449293954\n",
      "Epoch 185: val loss 0.750575\n",
      "\n",
      "Epoch %d: train loss %f 186 0.30314200477940695\n",
      "Epoch 186: val loss 0.716914\n",
      "\n",
      "Epoch %d: train loss %f 187 0.4107659373964582\n",
      "Epoch 187: val loss 0.771449\n",
      "\n",
      "Epoch %d: train loss %f 188 0.34397032856941223\n",
      "Epoch 188: val loss 0.698890\n",
      "\n",
      "Epoch %d: train loss %f 189 0.36469145119190216\n",
      "Epoch 189: val loss 0.715854\n",
      "\n",
      "Epoch %d: train loss %f 190 0.33572472419057575\n",
      "Epoch 190: val loss 0.703051\n",
      "\n",
      "Epoch %d: train loss %f 191 0.2992653740303857\n",
      "Epoch 191: val loss 0.715231\n",
      "\n",
      "Epoch %d: train loss %f 192 0.3423126893384116\n",
      "Epoch 192: val loss 0.687442\n",
      "\n",
      "Epoch %d: train loss %f 193 0.3124408892222813\n",
      "Epoch 193: val loss 0.682724\n",
      "\n",
      "Epoch %d: train loss %f 194 0.2956075242587498\n",
      "Epoch 194: val loss 0.764328\n",
      "\n",
      "Epoch %d: train loss %f 195 0.3139296578509467\n",
      "Epoch 195: val loss 0.717157\n",
      "\n",
      "Epoch %d: train loss %f 196 0.2869337669440678\n",
      "Epoch 196: val loss 0.678471\n",
      "\n",
      "Epoch %d: train loss %f 197 0.3376417245183672\n",
      "Epoch 197: val loss 0.748782\n",
      "\n",
      "Epoch %d: train loss %f 198 0.34322182834148407\n",
      "Epoch 198: val loss 0.736556\n",
      "\n",
      "Epoch %d: train loss %f 199 0.4235602766275406\n",
      "Epoch 199: val loss 0.733481\n",
      "\n",
      "Epoch %d: train loss %f 0 0.7140265362603324\n",
      "Epoch 0: val loss 0.711486\n",
      "\n",
      "Epoch %d: train loss %f 1 0.7098791939871651\n",
      "Epoch 1: val loss 0.707349\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6946577259472438\n",
      "Epoch 2: val loss 0.702502\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6854633688926697\n",
      "Epoch 3: val loss 0.694145\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6745005931173053\n",
      "Epoch 4: val loss 0.678826\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6544197372027806\n",
      "Epoch 5: val loss 0.661203\n",
      "\n",
      "Epoch %d: train loss %f 6 0.67111394235066\n",
      "Epoch 6: val loss 0.641971\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6202617798532758\n",
      "Epoch 7: val loss 0.634229\n",
      "\n",
      "Epoch %d: train loss %f 8 0.6257710116250175\n",
      "Epoch 8: val loss 0.618144\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5953654476574489\n",
      "Epoch 9: val loss 0.602752\n",
      "\n",
      "Epoch %d: train loss %f 10 0.6195119193622044\n",
      "Epoch 10: val loss 0.589256\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5704141003744942\n",
      "Epoch 11: val loss 0.581372\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5653595158032009\n",
      "Epoch 12: val loss 0.588446\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5425332827227456\n",
      "Epoch 13: val loss 0.576503\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5212420097419194\n",
      "Epoch 14: val loss 0.575325\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5720482170581818\n",
      "Epoch 15: val loss 0.569055\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5682403062071119\n",
      "Epoch 16: val loss 0.561625\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5685546185289111\n",
      "Epoch 17: val loss 0.568425\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5265240456376757\n",
      "Epoch 18: val loss 0.559487\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5121601692267826\n",
      "Epoch 19: val loss 0.559363\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5011002165930611\n",
      "Epoch 20: val loss 0.560508\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5277061334678105\n",
      "Epoch 21: val loss 0.572488\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5395513985838208\n",
      "Epoch 22: val loss 0.562641\n",
      "\n",
      "Epoch %d: train loss %f 23 0.5557069310120174\n",
      "Epoch 23: val loss 0.568165\n",
      "\n",
      "Epoch %d: train loss %f 24 0.5045963440622602\n",
      "Epoch 24: val loss 0.591746\n",
      "\n",
      "Epoch %d: train loss %f 25 0.5014449613434928\n",
      "Epoch 25: val loss 0.589971\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4911150336265564\n",
      "Epoch 26: val loss 0.575396\n",
      "\n",
      "Epoch %d: train loss %f 27 0.4975207916327885\n",
      "Epoch 27: val loss 0.575873\n",
      "\n",
      "Epoch %d: train loss %f 28 0.5021421100412097\n",
      "Epoch 28: val loss 0.576622\n",
      "\n",
      "Epoch %d: train loss %f 29 0.48032294852393015\n",
      "Epoch 29: val loss 0.582083\n",
      "\n",
      "Epoch %d: train loss %f 30 0.5565766734736306\n",
      "Epoch 30: val loss 0.575100\n",
      "\n",
      "Epoch %d: train loss %f 31 0.5077138372829982\n",
      "Epoch 31: val loss 0.595020\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4706932689462389\n",
      "Epoch 32: val loss 0.610736\n",
      "\n",
      "Epoch %d: train loss %f 33 0.5167683703558785\n",
      "Epoch 33: val loss 0.598798\n",
      "\n",
      "Epoch %d: train loss %f 34 0.5022894995553153\n",
      "Epoch 34: val loss 0.582812\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4607993108885629\n",
      "Epoch 35: val loss 0.579496\n",
      "\n",
      "Epoch %d: train loss %f 36 0.47570705839565824\n",
      "Epoch 36: val loss 0.577467\n",
      "\n",
      "Epoch %d: train loss %f 37 0.46509772326265064\n",
      "Epoch 37: val loss 0.581796\n",
      "\n",
      "Epoch %d: train loss %f 38 0.48692205974033903\n",
      "Epoch 38: val loss 0.584067\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4165935346058437\n",
      "Epoch 39: val loss 0.572585\n",
      "\n",
      "Epoch %d: train loss %f 40 0.4208404059920992\n",
      "Epoch 40: val loss 0.572893\n",
      "\n",
      "Epoch %d: train loss %f 41 0.49908876419067383\n",
      "Epoch 41: val loss 0.577353\n",
      "\n",
      "Epoch %d: train loss %f 42 0.458353544984545\n",
      "Epoch 42: val loss 0.567936\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4462193804127829\n",
      "Epoch 43: val loss 0.579710\n",
      "\n",
      "Epoch %d: train loss %f 44 0.5255398963178907\n",
      "Epoch 44: val loss 0.594804\n",
      "\n",
      "Epoch %d: train loss %f 45 0.49944562571389334\n",
      "Epoch 45: val loss 0.590051\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4500328515257154\n",
      "Epoch 46: val loss 0.599648\n",
      "\n",
      "Epoch %d: train loss %f 47 0.4339471480676106\n",
      "Epoch 47: val loss 0.592533\n",
      "\n",
      "Epoch %d: train loss %f 48 0.43312290736607145\n",
      "Epoch 48: val loss 0.584972\n",
      "\n",
      "Epoch %d: train loss %f 49 0.49085357785224915\n",
      "Epoch 49: val loss 0.593382\n",
      "\n",
      "Epoch %d: train loss %f 50 0.533805217061724\n",
      "Epoch 50: val loss 0.585718\n",
      "\n",
      "Epoch %d: train loss %f 51 0.484369124685015\n",
      "Epoch 51: val loss 0.622765\n",
      "\n",
      "Epoch %d: train loss %f 52 0.4572507781641824\n",
      "Epoch 52: val loss 0.581455\n",
      "\n",
      "Epoch %d: train loss %f 53 0.4896153041294643\n",
      "Epoch 53: val loss 0.597968\n",
      "\n",
      "Epoch %d: train loss %f 54 0.41111600611891064\n",
      "Epoch 54: val loss 0.589207\n",
      "\n",
      "Epoch %d: train loss %f 55 0.4581663863999503\n",
      "Epoch 55: val loss 0.590719\n",
      "\n",
      "Epoch %d: train loss %f 56 0.4346991607121059\n",
      "Epoch 56: val loss 0.587815\n",
      "\n",
      "Epoch %d: train loss %f 57 0.42595344356128145\n",
      "Epoch 57: val loss 0.581120\n",
      "\n",
      "Epoch %d: train loss %f 58 0.5103900006839207\n",
      "Epoch 58: val loss 0.586399\n",
      "\n",
      "Epoch %d: train loss %f 59 0.42426481417247225\n",
      "Epoch 59: val loss 0.617717\n",
      "\n",
      "Epoch %d: train loss %f 60 0.48906158975192476\n",
      "Epoch 60: val loss 0.580202\n",
      "\n",
      "Epoch %d: train loss %f 61 0.41407825904233114\n",
      "Epoch 61: val loss 0.598745\n",
      "\n",
      "Epoch %d: train loss %f 62 0.44022569911820547\n",
      "Epoch 62: val loss 0.605615\n",
      "\n",
      "Epoch %d: train loss %f 63 0.441878399678639\n",
      "Epoch 63: val loss 0.599718\n",
      "\n",
      "Epoch %d: train loss %f 64 0.43080052733421326\n",
      "Epoch 64: val loss 0.600341\n",
      "\n",
      "Epoch %d: train loss %f 65 0.4538831412792206\n",
      "Epoch 65: val loss 0.603542\n",
      "\n",
      "Epoch %d: train loss %f 66 0.4622803713594164\n",
      "Epoch 66: val loss 0.604177\n",
      "\n",
      "Epoch %d: train loss %f 67 0.4524520252432142\n",
      "Epoch 67: val loss 0.600504\n",
      "\n",
      "Epoch %d: train loss %f 68 0.4257446357182094\n",
      "Epoch 68: val loss 0.604409\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4034692070313862\n",
      "Epoch 69: val loss 0.617190\n",
      "\n",
      "Epoch %d: train loss %f 70 0.49148132119859966\n",
      "Epoch 70: val loss 0.620968\n",
      "\n",
      "Epoch %d: train loss %f 71 0.35306782381875174\n",
      "Epoch 71: val loss 0.617032\n",
      "\n",
      "Epoch %d: train loss %f 72 0.38734699147088186\n",
      "Epoch 72: val loss 0.613407\n",
      "\n",
      "Epoch %d: train loss %f 73 0.49937463658196585\n",
      "Epoch 73: val loss 0.606999\n",
      "\n",
      "Epoch %d: train loss %f 74 0.40050650920186726\n",
      "Epoch 74: val loss 0.620029\n",
      "\n",
      "Epoch %d: train loss %f 75 0.4680801246847425\n",
      "Epoch 75: val loss 0.628838\n",
      "\n",
      "Epoch %d: train loss %f 76 0.4102855111871447\n",
      "Epoch 76: val loss 0.631451\n",
      "\n",
      "Epoch %d: train loss %f 77 0.39705944061279297\n",
      "Epoch 77: val loss 0.651176\n",
      "\n",
      "Epoch %d: train loss %f 78 0.40467151573726107\n",
      "Epoch 78: val loss 0.629411\n",
      "\n",
      "Epoch %d: train loss %f 79 0.38896840810775757\n",
      "Epoch 79: val loss 0.628249\n",
      "\n",
      "Epoch %d: train loss %f 80 0.3648292337145124\n",
      "Epoch 80: val loss 0.620751\n",
      "\n",
      "Epoch %d: train loss %f 81 0.40798909962177277\n",
      "Epoch 81: val loss 0.618230\n",
      "\n",
      "Epoch %d: train loss %f 82 0.35326355908598217\n",
      "Epoch 82: val loss 0.617604\n",
      "\n",
      "Epoch %d: train loss %f 83 0.41494737778391155\n",
      "Epoch 83: val loss 0.625781\n",
      "\n",
      "Epoch %d: train loss %f 84 0.38508238962718416\n",
      "Epoch 84: val loss 0.627397\n",
      "\n",
      "Epoch %d: train loss %f 85 0.4044855960777828\n",
      "Epoch 85: val loss 0.638116\n",
      "\n",
      "Epoch %d: train loss %f 86 0.40976402163505554\n",
      "Epoch 86: val loss 0.621777\n",
      "\n",
      "Epoch %d: train loss %f 87 0.4276351162365505\n",
      "Epoch 87: val loss 0.627124\n",
      "\n",
      "Epoch %d: train loss %f 88 0.4106739674295698\n",
      "Epoch 88: val loss 0.636397\n",
      "\n",
      "Epoch %d: train loss %f 89 0.42878718035561697\n",
      "Epoch 89: val loss 0.637454\n",
      "\n",
      "Epoch %d: train loss %f 90 0.4344931628022875\n",
      "Epoch 90: val loss 0.645334\n",
      "\n",
      "Epoch %d: train loss %f 91 0.3839832416602543\n",
      "Epoch 91: val loss 0.627388\n",
      "\n",
      "Epoch %d: train loss %f 92 0.36264448932238985\n",
      "Epoch 92: val loss 0.627332\n",
      "\n",
      "Epoch %d: train loss %f 93 0.4039151242801121\n",
      "Epoch 93: val loss 0.617235\n",
      "\n",
      "Epoch %d: train loss %f 94 0.3705295366900308\n",
      "Epoch 94: val loss 0.623360\n",
      "\n",
      "Epoch %d: train loss %f 95 0.41624030470848083\n",
      "Epoch 95: val loss 0.616938\n",
      "\n",
      "Epoch %d: train loss %f 96 0.35994166135787964\n",
      "Epoch 96: val loss 0.612307\n",
      "\n",
      "Epoch %d: train loss %f 97 0.382875885282244\n",
      "Epoch 97: val loss 0.637512\n",
      "\n",
      "Epoch %d: train loss %f 98 0.6376666298934391\n",
      "Epoch 98: val loss 0.686467\n",
      "\n",
      "Epoch %d: train loss %f 99 0.43106127211025785\n",
      "Epoch 99: val loss 0.690178\n",
      "\n",
      "Epoch %d: train loss %f 100 0.5524170185838427\n",
      "Epoch 100: val loss 0.656232\n",
      "\n",
      "Epoch %d: train loss %f 101 0.4739836241517748\n",
      "Epoch 101: val loss 0.624152\n",
      "\n",
      "Epoch %d: train loss %f 102 0.41582455805369783\n",
      "Epoch 102: val loss 0.674034\n",
      "\n",
      "Epoch %d: train loss %f 103 0.5468640923500061\n",
      "Epoch 103: val loss 0.669254\n",
      "\n",
      "Epoch %d: train loss %f 104 0.38489204219409395\n",
      "Epoch 104: val loss 0.633212\n",
      "\n",
      "Epoch %d: train loss %f 105 0.3937941810914448\n",
      "Epoch 105: val loss 0.632438\n",
      "\n",
      "Epoch %d: train loss %f 106 0.36255008833748953\n",
      "Epoch 106: val loss 0.638530\n",
      "\n",
      "Epoch %d: train loss %f 107 0.358666860631534\n",
      "Epoch 107: val loss 0.628187\n",
      "\n",
      "Epoch %d: train loss %f 108 0.35491688336644855\n",
      "Epoch 108: val loss 0.632175\n",
      "\n",
      "Epoch %d: train loss %f 109 0.372166354741369\n",
      "Epoch 109: val loss 0.635446\n",
      "\n",
      "Epoch %d: train loss %f 110 0.4218613292489733\n",
      "Epoch 110: val loss 0.630416\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3827318719455174\n",
      "Epoch 111: val loss 0.645624\n",
      "\n",
      "Epoch %d: train loss %f 112 0.3534760390009199\n",
      "Epoch 112: val loss 0.658838\n",
      "\n",
      "Epoch %d: train loss %f 113 0.3670902805668967\n",
      "Epoch 113: val loss 0.643434\n",
      "\n",
      "Epoch %d: train loss %f 114 0.3748802883284433\n",
      "Epoch 114: val loss 0.634528\n",
      "\n",
      "Epoch %d: train loss %f 115 0.3683386466332844\n",
      "Epoch 115: val loss 0.649602\n",
      "\n",
      "Epoch %d: train loss %f 116 0.35357348195144106\n",
      "Epoch 116: val loss 0.677472\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3861997979027884\n",
      "Epoch 117: val loss 0.657601\n",
      "\n",
      "Epoch %d: train loss %f 118 0.4359260009867804\n",
      "Epoch 118: val loss 0.693377\n",
      "\n",
      "Epoch %d: train loss %f 119 0.39112179619925364\n",
      "Epoch 119: val loss 0.687470\n",
      "\n",
      "Epoch %d: train loss %f 120 0.46084222410406384\n",
      "Epoch 120: val loss 0.688133\n",
      "\n",
      "Epoch %d: train loss %f 121 0.45167819942746845\n",
      "Epoch 121: val loss 0.697401\n",
      "\n",
      "Epoch %d: train loss %f 122 0.3785084273133959\n",
      "Epoch 122: val loss 0.697053\n",
      "\n",
      "Epoch %d: train loss %f 123 0.35293195077351164\n",
      "Epoch 123: val loss 0.684140\n",
      "\n",
      "Epoch %d: train loss %f 124 0.38121023774147034\n",
      "Epoch 124: val loss 0.658370\n",
      "\n",
      "Epoch %d: train loss %f 125 0.3477309388773782\n",
      "Epoch 125: val loss 0.647430\n",
      "\n",
      "Epoch %d: train loss %f 126 0.37010246089526583\n",
      "Epoch 126: val loss 0.651888\n",
      "\n",
      "Epoch %d: train loss %f 127 0.41303942033222746\n",
      "Epoch 127: val loss 0.666728\n",
      "\n",
      "Epoch %d: train loss %f 128 0.33971347553389414\n",
      "Epoch 128: val loss 0.682301\n",
      "\n",
      "Epoch %d: train loss %f 129 0.33163715260369436\n",
      "Epoch 129: val loss 0.693597\n",
      "\n",
      "Epoch %d: train loss %f 130 0.38346923674855915\n",
      "Epoch 130: val loss 0.679273\n",
      "\n",
      "Epoch %d: train loss %f 131 0.3298065470797675\n",
      "Epoch 131: val loss 0.666688\n",
      "\n",
      "Epoch %d: train loss %f 132 0.38666946973119465\n",
      "Epoch 132: val loss 0.669849\n",
      "\n",
      "Epoch %d: train loss %f 133 0.36148126636232647\n",
      "Epoch 133: val loss 0.679462\n",
      "\n",
      "Epoch %d: train loss %f 134 0.4324860189642225\n",
      "Epoch 134: val loss 0.656632\n",
      "\n",
      "Epoch %d: train loss %f 135 0.3340021414416177\n",
      "Epoch 135: val loss 0.647156\n",
      "\n",
      "Epoch %d: train loss %f 136 0.49126855390412466\n",
      "Epoch 136: val loss 0.675093\n",
      "\n",
      "Epoch %d: train loss %f 137 0.34826193537030903\n",
      "Epoch 137: val loss 0.701755\n",
      "\n",
      "Epoch %d: train loss %f 138 0.3987924414021628\n",
      "Epoch 138: val loss 0.715067\n",
      "\n",
      "Epoch %d: train loss %f 139 0.3971554253782545\n",
      "Epoch 139: val loss 0.711169\n",
      "\n",
      "Epoch %d: train loss %f 140 0.36306196451187134\n",
      "Epoch 140: val loss 0.697230\n",
      "\n",
      "Epoch %d: train loss %f 141 0.30445034163338797\n",
      "Epoch 141: val loss 0.703648\n",
      "\n",
      "Epoch %d: train loss %f 142 0.41653117537498474\n",
      "Epoch 142: val loss 0.701040\n",
      "\n",
      "Epoch %d: train loss %f 143 0.47422891003744944\n",
      "Epoch 143: val loss 0.665136\n",
      "\n",
      "Epoch %d: train loss %f 144 0.37843222277505056\n",
      "Epoch 144: val loss 0.669736\n",
      "\n",
      "Epoch %d: train loss %f 145 0.324253437774522\n",
      "Epoch 145: val loss 0.689257\n",
      "\n",
      "Epoch %d: train loss %f 146 0.4086275781903948\n",
      "Epoch 146: val loss 0.678062\n",
      "\n",
      "Epoch %d: train loss %f 147 0.3429611452988216\n",
      "Epoch 147: val loss 0.664942\n",
      "\n",
      "Epoch %d: train loss %f 148 0.3343223141772406\n",
      "Epoch 148: val loss 0.664091\n",
      "\n",
      "Epoch %d: train loss %f 149 0.3212237741265978\n",
      "Epoch 149: val loss 0.680307\n",
      "\n",
      "Epoch %d: train loss %f 150 0.32034869279180256\n",
      "Epoch 150: val loss 0.700401\n",
      "\n",
      "Epoch %d: train loss %f 151 0.3224461248942784\n",
      "Epoch 151: val loss 0.705552\n",
      "\n",
      "Epoch %d: train loss %f 152 0.3750639685562679\n",
      "Epoch 152: val loss 0.717417\n",
      "\n",
      "Epoch %d: train loss %f 153 0.35196520388126373\n",
      "Epoch 153: val loss 0.753592\n",
      "\n",
      "Epoch %d: train loss %f 154 0.34213107398578096\n",
      "Epoch 154: val loss 0.716780\n",
      "\n",
      "Epoch %d: train loss %f 155 0.3436523292745863\n",
      "Epoch 155: val loss 0.746156\n",
      "\n",
      "Epoch %d: train loss %f 156 0.33186131928648266\n",
      "Epoch 156: val loss 0.767397\n",
      "\n",
      "Epoch %d: train loss %f 157 0.3293932591165815\n",
      "Epoch 157: val loss 0.783478\n",
      "\n",
      "Epoch %d: train loss %f 158 0.34303279646805357\n",
      "Epoch 158: val loss 0.761402\n",
      "\n",
      "Epoch %d: train loss %f 159 0.33591400512627195\n",
      "Epoch 159: val loss 0.764377\n",
      "\n",
      "Epoch %d: train loss %f 160 0.4443809304918562\n",
      "Epoch 160: val loss 0.756039\n",
      "\n",
      "Epoch %d: train loss %f 161 0.4367470123938152\n",
      "Epoch 161: val loss 0.764363\n",
      "\n",
      "Epoch %d: train loss %f 162 0.37819976253168924\n",
      "Epoch 162: val loss 0.769445\n",
      "\n",
      "Epoch %d: train loss %f 163 0.390068986586162\n",
      "Epoch 163: val loss 0.712072\n",
      "\n",
      "Epoch %d: train loss %f 164 0.3613880340542112\n",
      "Epoch 164: val loss 0.720406\n",
      "\n",
      "Epoch %d: train loss %f 165 0.35015850407736643\n",
      "Epoch 165: val loss 0.729110\n",
      "\n",
      "Epoch %d: train loss %f 166 0.3120330572128296\n",
      "Epoch 166: val loss 0.722964\n",
      "\n",
      "Epoch %d: train loss %f 167 0.31139256272997173\n",
      "Epoch 167: val loss 0.719532\n",
      "\n",
      "Epoch %d: train loss %f 168 0.3688421015228544\n",
      "Epoch 168: val loss 0.717405\n",
      "\n",
      "Epoch %d: train loss %f 169 0.34278970105307444\n",
      "Epoch 169: val loss 0.725020\n",
      "\n",
      "Epoch %d: train loss %f 170 0.33758649230003357\n",
      "Epoch 170: val loss 0.716440\n",
      "\n",
      "Epoch %d: train loss %f 171 0.32643366498606546\n",
      "Epoch 171: val loss 0.726459\n",
      "\n",
      "Epoch %d: train loss %f 172 0.35868352225848604\n",
      "Epoch 172: val loss 0.779596\n",
      "\n",
      "Epoch %d: train loss %f 173 0.37040772821222034\n",
      "Epoch 173: val loss 0.771314\n",
      "\n",
      "Epoch %d: train loss %f 174 0.39799875020980835\n",
      "Epoch 174: val loss 0.757122\n",
      "\n",
      "Epoch %d: train loss %f 175 0.33735733585698263\n",
      "Epoch 175: val loss 0.719659\n",
      "\n",
      "Epoch %d: train loss %f 176 0.3482529563563211\n",
      "Epoch 176: val loss 0.735178\n",
      "\n",
      "Epoch %d: train loss %f 177 0.28949505516460966\n",
      "Epoch 177: val loss 0.717293\n",
      "\n",
      "Epoch %d: train loss %f 178 0.3280251622200012\n",
      "Epoch 178: val loss 0.722921\n",
      "\n",
      "Epoch %d: train loss %f 179 0.3295642989022391\n",
      "Epoch 179: val loss 0.740247\n",
      "\n",
      "Epoch %d: train loss %f 180 0.34082965765680584\n",
      "Epoch 180: val loss 0.739404\n",
      "\n",
      "Epoch %d: train loss %f 181 0.33221121558121275\n",
      "Epoch 181: val loss 0.737188\n",
      "\n",
      "Epoch %d: train loss %f 182 0.34520987953458515\n",
      "Epoch 182: val loss 0.762204\n",
      "\n",
      "Epoch %d: train loss %f 183 0.3488957818065371\n",
      "Epoch 183: val loss 0.788234\n",
      "\n",
      "Epoch %d: train loss %f 184 0.4001536284174238\n",
      "Epoch 184: val loss 0.792159\n",
      "\n",
      "Epoch %d: train loss %f 185 0.34694971782820566\n",
      "Epoch 185: val loss 0.802161\n",
      "\n",
      "Epoch %d: train loss %f 186 0.3641124665737152\n",
      "Epoch 186: val loss 0.756108\n",
      "\n",
      "Epoch %d: train loss %f 187 0.32297213056257795\n",
      "Epoch 187: val loss 0.799240\n",
      "\n",
      "Epoch %d: train loss %f 188 0.3233487478324345\n",
      "Epoch 188: val loss 0.739388\n",
      "\n",
      "Epoch %d: train loss %f 189 0.3709876984357834\n",
      "Epoch 189: val loss 0.755900\n",
      "\n",
      "Epoch %d: train loss %f 190 0.3663102111646107\n",
      "Epoch 190: val loss 0.773793\n",
      "\n",
      "Epoch %d: train loss %f 191 0.3490984205688749\n",
      "Epoch 191: val loss 0.761034\n",
      "\n",
      "Epoch %d: train loss %f 192 0.3773750790527889\n",
      "Epoch 192: val loss 0.767772\n",
      "\n",
      "Epoch %d: train loss %f 193 0.3173482971532004\n",
      "Epoch 193: val loss 0.762740\n",
      "\n",
      "Epoch %d: train loss %f 194 0.3802503730569567\n",
      "Epoch 194: val loss 0.740294\n",
      "\n",
      "Epoch %d: train loss %f 195 0.32833594935280935\n",
      "Epoch 195: val loss 0.807690\n",
      "\n",
      "Epoch %d: train loss %f 196 0.46229252219200134\n",
      "Epoch 196: val loss 0.729455\n",
      "\n",
      "Epoch %d: train loss %f 197 0.2909643224307469\n",
      "Epoch 197: val loss 0.727711\n",
      "\n",
      "Epoch %d: train loss %f 198 0.3277487967695509\n",
      "Epoch 198: val loss 0.735244\n",
      "\n",
      "Epoch %d: train loss %f 199 0.32555884974343435\n",
      "Epoch 199: val loss 0.741784\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6941575407981873\n",
      "Epoch 0: val loss 0.690719\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6894099542072841\n",
      "Epoch 1: val loss 0.688111\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6791241935321263\n",
      "Epoch 2: val loss 0.685011\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6731049248150417\n",
      "Epoch 3: val loss 0.680065\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6629878878593445\n",
      "Epoch 4: val loss 0.671744\n",
      "\n",
      "Epoch %d: train loss %f 5 0.661039582320622\n",
      "Epoch 5: val loss 0.659351\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6411385791642326\n",
      "Epoch 6: val loss 0.642856\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6218873943601336\n",
      "Epoch 7: val loss 0.626749\n",
      "\n",
      "Epoch %d: train loss %f 8 0.6099319117409843\n",
      "Epoch 8: val loss 0.605850\n",
      "\n",
      "Epoch %d: train loss %f 9 0.657646826335362\n",
      "Epoch 9: val loss 0.594637\n",
      "\n",
      "Epoch %d: train loss %f 10 0.6100750820977348\n",
      "Epoch 10: val loss 0.594481\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5600952761513847\n",
      "Epoch 11: val loss 0.581940\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5548959715025765\n",
      "Epoch 12: val loss 0.562172\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5525010057858059\n",
      "Epoch 13: val loss 0.560304\n",
      "\n",
      "Epoch %d: train loss %f 14 0.551802933216095\n",
      "Epoch 14: val loss 0.568362\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5548212826251984\n",
      "Epoch 15: val loss 0.564734\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5446266787392753\n",
      "Epoch 16: val loss 0.567043\n",
      "\n",
      "Epoch %d: train loss %f 17 0.537516964333398\n",
      "Epoch 17: val loss 0.560733\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5245515576430729\n",
      "Epoch 18: val loss 0.565097\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5964572557381221\n",
      "Epoch 19: val loss 0.571925\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5148713035242898\n",
      "Epoch 20: val loss 0.573068\n",
      "\n",
      "Epoch %d: train loss %f 21 0.519781619310379\n",
      "Epoch 21: val loss 0.571484\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5172365605831146\n",
      "Epoch 22: val loss 0.567303\n",
      "\n",
      "Epoch %d: train loss %f 23 0.5087827273777553\n",
      "Epoch 23: val loss 0.564660\n",
      "\n",
      "Epoch %d: train loss %f 24 0.4806819920028959\n",
      "Epoch 24: val loss 0.563735\n",
      "\n",
      "Epoch %d: train loss %f 25 0.49304001671927317\n",
      "Epoch 25: val loss 0.567904\n",
      "\n",
      "Epoch %d: train loss %f 26 0.49380757553236826\n",
      "Epoch 26: val loss 0.595727\n",
      "\n",
      "Epoch %d: train loss %f 27 0.5121612974575588\n",
      "Epoch 27: val loss 0.593255\n",
      "\n",
      "Epoch %d: train loss %f 28 0.538907106433596\n",
      "Epoch 28: val loss 0.588223\n",
      "\n",
      "Epoch %d: train loss %f 29 0.49541272861616953\n",
      "Epoch 29: val loss 0.610036\n",
      "\n",
      "Epoch %d: train loss %f 30 0.49123177783829824\n",
      "Epoch 30: val loss 0.582266\n",
      "\n",
      "Epoch %d: train loss %f 31 0.47276261874607634\n",
      "Epoch 31: val loss 0.576340\n",
      "\n",
      "Epoch %d: train loss %f 32 0.46901965354170116\n",
      "Epoch 32: val loss 0.599628\n",
      "\n",
      "Epoch %d: train loss %f 33 0.5270985662937164\n",
      "Epoch 33: val loss 0.584807\n",
      "\n",
      "Epoch %d: train loss %f 34 0.482289446251733\n",
      "Epoch 34: val loss 0.586731\n",
      "\n",
      "Epoch %d: train loss %f 35 0.5727029527936663\n",
      "Epoch 35: val loss 0.589523\n",
      "\n",
      "Epoch %d: train loss %f 36 0.49535538043294636\n",
      "Epoch 36: val loss 0.593095\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4667011116232191\n",
      "Epoch 37: val loss 0.577239\n",
      "\n",
      "Epoch %d: train loss %f 38 0.47154171551976887\n",
      "Epoch 38: val loss 0.578729\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4476308950356075\n",
      "Epoch 39: val loss 0.596107\n",
      "\n",
      "Epoch %d: train loss %f 40 0.4641715373311724\n",
      "Epoch 40: val loss 0.601486\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4683243589741843\n",
      "Epoch 41: val loss 0.620616\n",
      "\n",
      "Epoch %d: train loss %f 42 0.48331003955432345\n",
      "Epoch 42: val loss 0.611484\n",
      "\n",
      "Epoch %d: train loss %f 43 0.48599880933761597\n",
      "Epoch 43: val loss 0.605786\n",
      "\n",
      "Epoch %d: train loss %f 44 0.4787931271961757\n",
      "Epoch 44: val loss 0.611498\n",
      "\n",
      "Epoch %d: train loss %f 45 0.4206174058573587\n",
      "Epoch 45: val loss 0.601930\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4577953432287489\n",
      "Epoch 46: val loss 0.599430\n",
      "\n",
      "Epoch %d: train loss %f 47 0.44290028725351605\n",
      "Epoch 47: val loss 0.598806\n",
      "\n",
      "Epoch %d: train loss %f 48 0.5332547639097486\n",
      "Epoch 48: val loss 0.591060\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4408246193613325\n",
      "Epoch 49: val loss 0.601994\n",
      "\n",
      "Epoch %d: train loss %f 50 0.4963637718132564\n",
      "Epoch 50: val loss 0.614429\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4482540786266327\n",
      "Epoch 51: val loss 0.592416\n",
      "\n",
      "Epoch %d: train loss %f 52 0.4959875004632132\n",
      "Epoch 52: val loss 0.604583\n",
      "\n",
      "Epoch %d: train loss %f 53 0.4385209892477308\n",
      "Epoch 53: val loss 0.595625\n",
      "\n",
      "Epoch %d: train loss %f 54 0.42573435817446026\n",
      "Epoch 54: val loss 0.609874\n",
      "\n",
      "Epoch %d: train loss %f 55 0.5050791501998901\n",
      "Epoch 55: val loss 0.629885\n",
      "\n",
      "Epoch %d: train loss %f 56 0.42787520374570576\n",
      "Epoch 56: val loss 0.664944\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4569969517844064\n",
      "Epoch 57: val loss 0.619980\n",
      "\n",
      "Epoch %d: train loss %f 58 0.43173363379069735\n",
      "Epoch 58: val loss 0.594792\n",
      "\n",
      "Epoch %d: train loss %f 59 0.5171725962843213\n",
      "Epoch 59: val loss 0.580548\n",
      "\n",
      "Epoch %d: train loss %f 60 0.4905401127679007\n",
      "Epoch 60: val loss 0.595982\n",
      "\n",
      "Epoch %d: train loss %f 61 0.4187536750520979\n",
      "Epoch 61: val loss 0.598782\n",
      "\n",
      "Epoch %d: train loss %f 62 0.4825300659452166\n",
      "Epoch 62: val loss 0.624509\n",
      "\n",
      "Epoch %d: train loss %f 63 0.5044580272265843\n",
      "Epoch 63: val loss 0.614364\n",
      "\n",
      "Epoch %d: train loss %f 64 0.4612341438020979\n",
      "Epoch 64: val loss 0.625922\n",
      "\n",
      "Epoch %d: train loss %f 65 0.4360148438385555\n",
      "Epoch 65: val loss 0.616319\n",
      "\n",
      "Epoch %d: train loss %f 66 0.4726332894393376\n",
      "Epoch 66: val loss 0.592980\n",
      "\n",
      "Epoch %d: train loss %f 67 0.4528907537460327\n",
      "Epoch 67: val loss 0.580497\n",
      "\n",
      "Epoch %d: train loss %f 68 0.4397840542452676\n",
      "Epoch 68: val loss 0.575273\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4129321447440556\n",
      "Epoch 69: val loss 0.597373\n",
      "\n",
      "Epoch %d: train loss %f 70 0.4304823321955545\n",
      "Epoch 70: val loss 0.596406\n",
      "\n",
      "Epoch %d: train loss %f 71 0.40562025138310026\n",
      "Epoch 71: val loss 0.620513\n",
      "\n",
      "Epoch %d: train loss %f 72 0.4561816283634731\n",
      "Epoch 72: val loss 0.596120\n",
      "\n",
      "Epoch %d: train loss %f 73 0.390097085918699\n",
      "Epoch 73: val loss 0.592126\n",
      "\n",
      "Epoch %d: train loss %f 74 0.458374627998897\n",
      "Epoch 74: val loss 0.595602\n",
      "\n",
      "Epoch %d: train loss %f 75 0.40517657995224\n",
      "Epoch 75: val loss 0.610482\n",
      "\n",
      "Epoch %d: train loss %f 76 0.38216497004032135\n",
      "Epoch 76: val loss 0.607799\n",
      "\n",
      "Epoch %d: train loss %f 77 0.4153189403670175\n",
      "Epoch 77: val loss 0.616133\n",
      "\n",
      "Epoch %d: train loss %f 78 0.360368695642267\n",
      "Epoch 78: val loss 0.602836\n",
      "\n",
      "Epoch %d: train loss %f 79 0.44144532084465027\n",
      "Epoch 79: val loss 0.597907\n",
      "\n",
      "Epoch %d: train loss %f 80 0.3799006768635341\n",
      "Epoch 80: val loss 0.604495\n",
      "\n",
      "Epoch %d: train loss %f 81 0.41983682768685476\n",
      "Epoch 81: val loss 0.612403\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3763417090688433\n",
      "Epoch 82: val loss 0.643529\n",
      "\n",
      "Epoch %d: train loss %f 83 0.41761509009769987\n",
      "Epoch 83: val loss 0.645429\n",
      "\n",
      "Epoch %d: train loss %f 84 0.45751956530979704\n",
      "Epoch 84: val loss 0.636205\n",
      "\n",
      "Epoch %d: train loss %f 85 0.482405070747648\n",
      "Epoch 85: val loss 0.696156\n",
      "\n",
      "Epoch %d: train loss %f 86 0.47467721360070364\n",
      "Epoch 86: val loss 0.608523\n",
      "\n",
      "Epoch %d: train loss %f 87 0.3886889474732535\n",
      "Epoch 87: val loss 0.604653\n",
      "\n",
      "Epoch %d: train loss %f 88 0.42502297673906597\n",
      "Epoch 88: val loss 0.611607\n",
      "\n",
      "Epoch %d: train loss %f 89 0.4453963169029781\n",
      "Epoch 89: val loss 0.630897\n",
      "\n",
      "Epoch %d: train loss %f 90 0.41094513876097544\n",
      "Epoch 90: val loss 0.642377\n",
      "\n",
      "Epoch %d: train loss %f 91 0.33131287353379385\n",
      "Epoch 91: val loss 0.657514\n",
      "\n",
      "Epoch %d: train loss %f 92 0.42710993119648527\n",
      "Epoch 92: val loss 0.648766\n",
      "\n",
      "Epoch %d: train loss %f 93 0.41117559586252483\n",
      "Epoch 93: val loss 0.657010\n",
      "\n",
      "Epoch %d: train loss %f 94 0.4154379836150578\n",
      "Epoch 94: val loss 0.665047\n",
      "\n",
      "Epoch %d: train loss %f 95 0.3762845184121813\n",
      "Epoch 95: val loss 0.668814\n",
      "\n",
      "Epoch %d: train loss %f 96 0.41260395305497305\n",
      "Epoch 96: val loss 0.671997\n",
      "\n",
      "Epoch %d: train loss %f 97 0.3687150776386261\n",
      "Epoch 97: val loss 0.696216\n",
      "\n",
      "Epoch %d: train loss %f 98 0.37959830675806316\n",
      "Epoch 98: val loss 0.665015\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3905026231493269\n",
      "Epoch 99: val loss 0.663087\n",
      "\n",
      "Epoch %d: train loss %f 100 0.3860123498099191\n",
      "Epoch 100: val loss 0.657797\n",
      "\n",
      "Epoch %d: train loss %f 101 0.41705974510737825\n",
      "Epoch 101: val loss 0.647814\n",
      "\n",
      "Epoch %d: train loss %f 102 0.40651204543454306\n",
      "Epoch 102: val loss 0.712874\n",
      "\n",
      "Epoch %d: train loss %f 103 0.35991304899965015\n",
      "Epoch 103: val loss 0.659835\n",
      "\n",
      "Epoch %d: train loss %f 104 0.38127693959644865\n",
      "Epoch 104: val loss 0.676109\n",
      "\n",
      "Epoch %d: train loss %f 105 0.4210804317678724\n",
      "Epoch 105: val loss 0.730183\n",
      "\n",
      "Epoch %d: train loss %f 106 0.32922638739858356\n",
      "Epoch 106: val loss 0.673764\n",
      "\n",
      "Epoch %d: train loss %f 107 0.3743277021816799\n",
      "Epoch 107: val loss 0.663227\n",
      "\n",
      "Epoch %d: train loss %f 108 0.354776348386492\n",
      "Epoch 108: val loss 0.689661\n",
      "\n",
      "Epoch %d: train loss %f 109 0.34658150162015644\n",
      "Epoch 109: val loss 0.669294\n",
      "\n",
      "Epoch %d: train loss %f 110 0.38001854930605206\n",
      "Epoch 110: val loss 0.639715\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3920674111161913\n",
      "Epoch 111: val loss 0.646482\n",
      "\n",
      "Epoch %d: train loss %f 112 0.3896266669034958\n",
      "Epoch 112: val loss 0.668909\n",
      "\n",
      "Epoch %d: train loss %f 113 0.3496204933949879\n",
      "Epoch 113: val loss 0.742698\n",
      "\n",
      "Epoch %d: train loss %f 114 0.3475824807371412\n",
      "Epoch 114: val loss 0.659447\n",
      "\n",
      "Epoch %d: train loss %f 115 0.34427917642252787\n",
      "Epoch 115: val loss 0.681801\n",
      "\n",
      "Epoch %d: train loss %f 116 0.38983300754002165\n",
      "Epoch 116: val loss 0.722773\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3182878962584904\n",
      "Epoch 117: val loss 0.746687\n",
      "\n",
      "Epoch %d: train loss %f 118 0.3502834737300873\n",
      "Epoch 118: val loss 0.751978\n",
      "\n",
      "Epoch %d: train loss %f 119 0.3593530910355704\n",
      "Epoch 119: val loss 0.714648\n",
      "\n",
      "Epoch %d: train loss %f 120 0.41132165278707233\n",
      "Epoch 120: val loss 0.710903\n",
      "\n",
      "Epoch %d: train loss %f 121 0.3198952738727842\n",
      "Epoch 121: val loss 0.732576\n",
      "\n",
      "Epoch %d: train loss %f 122 0.35305191363607136\n",
      "Epoch 122: val loss 0.728375\n",
      "\n",
      "Epoch %d: train loss %f 123 0.4174370212214334\n",
      "Epoch 123: val loss 0.736746\n",
      "\n",
      "Epoch %d: train loss %f 124 0.3239820088659014\n",
      "Epoch 124: val loss 0.731680\n",
      "\n",
      "Epoch %d: train loss %f 125 0.3550995758601597\n",
      "Epoch 125: val loss 0.733087\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3352916198117392\n",
      "Epoch 126: val loss 0.726248\n",
      "\n",
      "Epoch %d: train loss %f 127 0.34954486574445454\n",
      "Epoch 127: val loss 0.701481\n",
      "\n",
      "Epoch %d: train loss %f 128 0.3381264805793762\n",
      "Epoch 128: val loss 0.682791\n",
      "\n",
      "Epoch %d: train loss %f 129 0.38668374930109295\n",
      "Epoch 129: val loss 0.707145\n",
      "\n",
      "Epoch %d: train loss %f 130 0.34953019874436514\n",
      "Epoch 130: val loss 0.738862\n",
      "\n",
      "Epoch %d: train loss %f 131 0.32019601336547304\n",
      "Epoch 131: val loss 0.753354\n",
      "\n",
      "Epoch %d: train loss %f 132 0.39453108821596417\n",
      "Epoch 132: val loss 0.738394\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3522703605038779\n",
      "Epoch 133: val loss 0.716593\n",
      "\n",
      "Epoch %d: train loss %f 134 0.3833030717713492\n",
      "Epoch 134: val loss 0.697245\n",
      "\n",
      "Epoch %d: train loss %f 135 0.3795369991234371\n",
      "Epoch 135: val loss 0.723695\n",
      "\n",
      "Epoch %d: train loss %f 136 0.37951299122401644\n",
      "Epoch 136: val loss 0.688035\n",
      "\n",
      "Epoch %d: train loss %f 137 0.30232700279780794\n",
      "Epoch 137: val loss 0.730456\n",
      "\n",
      "Epoch %d: train loss %f 138 0.30621713612760815\n",
      "Epoch 138: val loss 0.721033\n",
      "\n",
      "Epoch %d: train loss %f 139 0.33974060841969084\n",
      "Epoch 139: val loss 0.761726\n",
      "\n",
      "Epoch %d: train loss %f 140 0.35941618255206514\n",
      "Epoch 140: val loss 0.804662\n",
      "\n",
      "Epoch %d: train loss %f 141 0.35734239646366667\n",
      "Epoch 141: val loss 0.748389\n",
      "\n",
      "Epoch %d: train loss %f 142 0.38226899930409025\n",
      "Epoch 142: val loss 0.741114\n",
      "\n",
      "Epoch %d: train loss %f 143 0.3219170570373535\n",
      "Epoch 143: val loss 0.715059\n",
      "\n",
      "Epoch %d: train loss %f 144 0.3747692427464894\n",
      "Epoch 144: val loss 0.732654\n",
      "\n",
      "Epoch %d: train loss %f 145 0.44104634651115965\n",
      "Epoch 145: val loss 0.782675\n",
      "\n",
      "Epoch %d: train loss %f 146 0.35905630035059793\n",
      "Epoch 146: val loss 0.790434\n",
      "\n",
      "Epoch %d: train loss %f 147 0.4036672115325928\n",
      "Epoch 147: val loss 0.764977\n",
      "\n",
      "Epoch %d: train loss %f 148 0.30680488901478903\n",
      "Epoch 148: val loss 0.731603\n",
      "\n",
      "Epoch %d: train loss %f 149 0.3349509962967464\n",
      "Epoch 149: val loss 0.716656\n",
      "\n",
      "Epoch %d: train loss %f 150 0.30956374747412546\n",
      "Epoch 150: val loss 0.706867\n",
      "\n",
      "Epoch %d: train loss %f 151 0.33979770115443636\n",
      "Epoch 151: val loss 0.696574\n",
      "\n",
      "Epoch %d: train loss %f 152 0.38648027181625366\n",
      "Epoch 152: val loss 0.676962\n",
      "\n",
      "Epoch %d: train loss %f 153 0.3368965302194868\n",
      "Epoch 153: val loss 0.696382\n",
      "\n",
      "Epoch %d: train loss %f 154 0.3138780202716589\n",
      "Epoch 154: val loss 0.727816\n",
      "\n",
      "Epoch %d: train loss %f 155 0.34248256257602144\n",
      "Epoch 155: val loss 0.721674\n",
      "\n",
      "Epoch %d: train loss %f 156 0.33741968870162964\n",
      "Epoch 156: val loss 0.725701\n",
      "\n",
      "Epoch %d: train loss %f 157 0.3060468775885446\n",
      "Epoch 157: val loss 0.695267\n",
      "\n",
      "Epoch %d: train loss %f 158 0.3009883144072124\n",
      "Epoch 158: val loss 0.692097\n",
      "\n",
      "Epoch %d: train loss %f 159 0.39100716369492666\n",
      "Epoch 159: val loss 0.718379\n",
      "\n",
      "Epoch %d: train loss %f 160 0.36119028074400766\n",
      "Epoch 160: val loss 0.703682\n",
      "\n",
      "Epoch %d: train loss %f 161 0.346809127501079\n",
      "Epoch 161: val loss 0.728305\n",
      "\n",
      "Epoch %d: train loss %f 162 0.3006064295768738\n",
      "Epoch 162: val loss 0.835991\n",
      "\n",
      "Epoch %d: train loss %f 163 0.3476632173572268\n",
      "Epoch 163: val loss 0.805166\n",
      "\n",
      "Epoch %d: train loss %f 164 0.33178920830999103\n",
      "Epoch 164: val loss 0.774569\n",
      "\n",
      "Epoch %d: train loss %f 165 0.31788087423358646\n",
      "Epoch 165: val loss 0.727381\n",
      "\n",
      "Epoch %d: train loss %f 166 0.2844263081039701\n",
      "Epoch 166: val loss 0.722366\n",
      "\n",
      "Epoch %d: train loss %f 167 0.2878994941711426\n",
      "Epoch 167: val loss 0.725062\n",
      "\n",
      "Epoch %d: train loss %f 168 0.26637438365391325\n",
      "Epoch 168: val loss 0.743775\n",
      "\n",
      "Epoch %d: train loss %f 169 0.4279041162558964\n",
      "Epoch 169: val loss 0.749602\n",
      "\n",
      "Epoch %d: train loss %f 170 0.3174835294485092\n",
      "Epoch 170: val loss 0.741672\n",
      "\n",
      "Epoch %d: train loss %f 171 0.3147828642811094\n",
      "Epoch 171: val loss 0.754045\n",
      "\n",
      "Epoch %d: train loss %f 172 0.28612091924463\n",
      "Epoch 172: val loss 0.778694\n",
      "\n",
      "Epoch %d: train loss %f 173 0.30795930113111225\n",
      "Epoch 173: val loss 0.749714\n",
      "\n",
      "Epoch %d: train loss %f 174 0.3152408003807068\n",
      "Epoch 174: val loss 0.774344\n",
      "\n",
      "Epoch %d: train loss %f 175 0.3306955099105835\n",
      "Epoch 175: val loss 0.789010\n",
      "\n",
      "Epoch %d: train loss %f 176 0.29206663583006176\n",
      "Epoch 176: val loss 0.807690\n",
      "\n",
      "Epoch %d: train loss %f 177 0.2923269280498581\n",
      "Epoch 177: val loss 0.790070\n",
      "\n",
      "Epoch %d: train loss %f 178 0.2946635516626494\n",
      "Epoch 178: val loss 0.759024\n",
      "\n",
      "Epoch %d: train loss %f 179 0.32245352438517977\n",
      "Epoch 179: val loss 0.764297\n",
      "\n",
      "Epoch %d: train loss %f 180 0.330158714737211\n",
      "Epoch 180: val loss 0.824179\n",
      "\n",
      "Epoch %d: train loss %f 181 0.2991029407296862\n",
      "Epoch 181: val loss 0.816996\n",
      "\n",
      "Epoch %d: train loss %f 182 0.3156411647796631\n",
      "Epoch 182: val loss 0.820270\n",
      "\n",
      "Epoch %d: train loss %f 183 0.30075429167066303\n",
      "Epoch 183: val loss 0.839489\n",
      "\n",
      "Epoch %d: train loss %f 184 0.3358999469450542\n",
      "Epoch 184: val loss 0.802580\n",
      "\n",
      "Epoch %d: train loss %f 185 0.3841093821184976\n",
      "Epoch 185: val loss 0.753530\n",
      "\n",
      "Epoch %d: train loss %f 186 0.4055339906896864\n",
      "Epoch 186: val loss 0.866108\n",
      "\n",
      "Epoch %d: train loss %f 187 0.3373782954045704\n",
      "Epoch 187: val loss 0.824590\n",
      "\n",
      "Epoch %d: train loss %f 188 0.42715366397585186\n",
      "Epoch 188: val loss 0.796236\n",
      "\n",
      "Epoch %d: train loss %f 189 0.35252339925084797\n",
      "Epoch 189: val loss 0.778690\n",
      "\n",
      "Epoch %d: train loss %f 190 0.3683565216405051\n",
      "Epoch 190: val loss 0.754476\n",
      "\n",
      "Epoch %d: train loss %f 191 0.3217278059039797\n",
      "Epoch 191: val loss 0.712019\n",
      "\n",
      "Epoch %d: train loss %f 192 0.2987083239214761\n",
      "Epoch 192: val loss 0.789208\n",
      "\n",
      "Epoch %d: train loss %f 193 0.33636939951351713\n",
      "Epoch 193: val loss 0.764670\n",
      "\n",
      "Epoch %d: train loss %f 194 0.29018630513123106\n",
      "Epoch 194: val loss 0.781814\n",
      "\n",
      "Epoch %d: train loss %f 195 0.3879887993846621\n",
      "Epoch 195: val loss 0.763476\n",
      "\n",
      "Epoch %d: train loss %f 196 0.3842614953007017\n",
      "Epoch 196: val loss 0.791386\n",
      "\n",
      "Epoch %d: train loss %f 197 0.33174422809055876\n",
      "Epoch 197: val loss 0.832206\n",
      "\n",
      "Epoch %d: train loss %f 198 0.3211701810359955\n",
      "Epoch 198: val loss 0.789454\n",
      "\n",
      "Epoch %d: train loss %f 199 0.2938863384936537\n",
      "Epoch 199: val loss 0.738934\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6905302405357361\n",
      "Epoch 0: val loss 0.689232\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6822455269949776\n",
      "Epoch 1: val loss 0.686468\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6705596787588937\n",
      "Epoch 2: val loss 0.682204\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6656821284975324\n",
      "Epoch 3: val loss 0.675037\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6475341064589364\n",
      "Epoch 4: val loss 0.663230\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6323979837553841\n",
      "Epoch 5: val loss 0.648511\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6123606988361904\n",
      "Epoch 6: val loss 0.637264\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6091948321887425\n",
      "Epoch 7: val loss 0.626056\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5861486792564392\n",
      "Epoch 8: val loss 0.619041\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5838030108383724\n",
      "Epoch 9: val loss 0.608868\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5645343405859811\n",
      "Epoch 10: val loss 0.593876\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5971224350588662\n",
      "Epoch 11: val loss 0.599465\n",
      "\n",
      "Epoch %d: train loss %f 12 0.530218494789941\n",
      "Epoch 12: val loss 0.617623\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5846521982124874\n",
      "Epoch 13: val loss 0.599960\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5280723529202598\n",
      "Epoch 14: val loss 0.592556\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5195507279464177\n",
      "Epoch 15: val loss 0.583285\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5101940376417977\n",
      "Epoch 16: val loss 0.589973\n",
      "\n",
      "Epoch %d: train loss %f 17 0.4886234828404018\n",
      "Epoch 17: val loss 0.594488\n",
      "\n",
      "Epoch %d: train loss %f 18 0.4838493764400482\n",
      "Epoch 18: val loss 0.605784\n",
      "\n",
      "Epoch %d: train loss %f 19 0.49718767404556274\n",
      "Epoch 19: val loss 0.601344\n",
      "\n",
      "Epoch %d: train loss %f 20 0.4865217421736036\n",
      "Epoch 20: val loss 0.596284\n",
      "\n",
      "Epoch %d: train loss %f 21 0.44965548387595583\n",
      "Epoch 21: val loss 0.603477\n",
      "\n",
      "Epoch %d: train loss %f 22 0.4796172295297895\n",
      "Epoch 22: val loss 0.665972\n",
      "\n",
      "Epoch %d: train loss %f 23 0.528902245419366\n",
      "Epoch 23: val loss 0.663066\n",
      "\n",
      "Epoch %d: train loss %f 24 0.4814277972493853\n",
      "Epoch 24: val loss 0.659690\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4556814006396702\n",
      "Epoch 25: val loss 0.622901\n",
      "\n",
      "Epoch %d: train loss %f 26 0.460356525012425\n",
      "Epoch 26: val loss 0.622525\n",
      "\n",
      "Epoch %d: train loss %f 27 0.4573059550353459\n",
      "Epoch 27: val loss 0.645906\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4506138265132904\n",
      "Epoch 28: val loss 0.653167\n",
      "\n",
      "Epoch %d: train loss %f 29 0.43357210074152264\n",
      "Epoch 29: val loss 0.681947\n",
      "\n",
      "Epoch %d: train loss %f 30 0.41986253431865145\n",
      "Epoch 30: val loss 0.691349\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4765057989529201\n",
      "Epoch 31: val loss 0.757907\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4096079298428127\n",
      "Epoch 32: val loss 0.709192\n",
      "\n",
      "Epoch %d: train loss %f 33 0.3853645282132285\n",
      "Epoch 33: val loss 0.709094\n",
      "\n",
      "Epoch %d: train loss %f 34 0.3828144520521164\n",
      "Epoch 34: val loss 0.737678\n",
      "\n",
      "Epoch %d: train loss %f 35 0.45859385813985554\n",
      "Epoch 35: val loss 0.710231\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4368114024400711\n",
      "Epoch 36: val loss 0.685187\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4163425692490169\n",
      "Epoch 37: val loss 0.760535\n",
      "\n",
      "Epoch %d: train loss %f 38 0.42776104382106234\n",
      "Epoch 38: val loss 0.742622\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4462047985621861\n",
      "Epoch 39: val loss 0.715203\n",
      "\n",
      "Epoch %d: train loss %f 40 0.4777083822659084\n",
      "Epoch 40: val loss 0.744479\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4076112040451595\n",
      "Epoch 41: val loss 0.692012\n",
      "\n",
      "Epoch %d: train loss %f 42 0.45721170731953215\n",
      "Epoch 42: val loss 0.699912\n",
      "\n",
      "Epoch %d: train loss %f 43 0.38496607542037964\n",
      "Epoch 43: val loss 0.709988\n",
      "\n",
      "Epoch %d: train loss %f 44 0.49984459791864666\n",
      "Epoch 44: val loss 0.706501\n",
      "\n",
      "Epoch %d: train loss %f 45 0.38463733877454487\n",
      "Epoch 45: val loss 0.782823\n",
      "\n",
      "Epoch %d: train loss %f 46 0.38554165405886515\n",
      "Epoch 46: val loss 0.737091\n",
      "\n",
      "Epoch %d: train loss %f 47 0.37856015988758634\n",
      "Epoch 47: val loss 0.744040\n",
      "\n",
      "Epoch %d: train loss %f 48 0.4136331379413605\n",
      "Epoch 48: val loss 0.835611\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4738107579095023\n",
      "Epoch 49: val loss 0.859014\n",
      "\n",
      "Epoch %d: train loss %f 50 0.381802784545081\n",
      "Epoch 50: val loss 0.747137\n",
      "\n",
      "Epoch %d: train loss %f 51 0.3674276087965284\n",
      "Epoch 51: val loss 0.689355\n",
      "\n",
      "Epoch %d: train loss %f 52 0.3826233914920262\n",
      "Epoch 52: val loss 0.699720\n",
      "\n",
      "Epoch %d: train loss %f 53 0.44657251451696667\n",
      "Epoch 53: val loss 0.765578\n",
      "\n",
      "Epoch %d: train loss %f 54 0.4032753791127886\n",
      "Epoch 54: val loss 0.723834\n",
      "\n",
      "Epoch %d: train loss %f 55 0.39252321634973797\n",
      "Epoch 55: val loss 0.744947\n",
      "\n",
      "Epoch %d: train loss %f 56 0.37694244725363596\n",
      "Epoch 56: val loss 0.774011\n",
      "\n",
      "Epoch %d: train loss %f 57 0.35040428808757235\n",
      "Epoch 57: val loss 0.783119\n",
      "\n",
      "Epoch %d: train loss %f 58 0.37312298161642893\n",
      "Epoch 58: val loss 0.804330\n",
      "\n",
      "Epoch %d: train loss %f 59 0.3421738339321954\n",
      "Epoch 59: val loss 0.825385\n",
      "\n",
      "Epoch %d: train loss %f 60 0.3959976519857134\n",
      "Epoch 60: val loss 0.824412\n",
      "\n",
      "Epoch %d: train loss %f 61 0.38632779461996897\n",
      "Epoch 61: val loss 0.848468\n",
      "\n",
      "Epoch %d: train loss %f 62 0.37821644118853975\n",
      "Epoch 62: val loss 0.848732\n",
      "\n",
      "Epoch %d: train loss %f 63 0.37161314487457275\n",
      "Epoch 63: val loss 0.934126\n",
      "\n",
      "Epoch %d: train loss %f 64 0.32829810678958893\n",
      "Epoch 64: val loss 0.869030\n",
      "\n",
      "Epoch %d: train loss %f 65 0.3852554942880358\n",
      "Epoch 65: val loss 0.845821\n",
      "\n",
      "Epoch %d: train loss %f 66 0.4011867621115276\n",
      "Epoch 66: val loss 0.838040\n",
      "\n",
      "Epoch %d: train loss %f 67 0.40095133440835135\n",
      "Epoch 67: val loss 0.815136\n",
      "\n",
      "Epoch %d: train loss %f 68 0.3512393534183502\n",
      "Epoch 68: val loss 0.891140\n",
      "\n",
      "Epoch %d: train loss %f 69 0.3811515250376293\n",
      "Epoch 69: val loss 0.864072\n",
      "\n",
      "Epoch %d: train loss %f 70 0.3938117963927133\n",
      "Epoch 70: val loss 0.880259\n",
      "\n",
      "Epoch %d: train loss %f 71 0.3633854389190674\n",
      "Epoch 71: val loss 0.845870\n",
      "\n",
      "Epoch %d: train loss %f 72 0.39176713143076214\n",
      "Epoch 72: val loss 0.837867\n",
      "\n",
      "Epoch %d: train loss %f 73 0.34014900560889927\n",
      "Epoch 73: val loss 0.911236\n",
      "\n",
      "Epoch %d: train loss %f 74 0.322861590555736\n",
      "Epoch 74: val loss 0.856320\n",
      "\n",
      "Epoch %d: train loss %f 75 0.32164869138172697\n",
      "Epoch 75: val loss 0.832800\n",
      "\n",
      "Epoch %d: train loss %f 76 0.3256011775561741\n",
      "Epoch 76: val loss 0.906489\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3749204618590219\n",
      "Epoch 77: val loss 0.880687\n",
      "\n",
      "Epoch %d: train loss %f 78 0.4068738179547446\n",
      "Epoch 78: val loss 0.823998\n",
      "\n",
      "Epoch %d: train loss %f 79 0.35429599455424715\n",
      "Epoch 79: val loss 0.849739\n",
      "\n",
      "Epoch %d: train loss %f 80 0.3595585354736873\n",
      "Epoch 80: val loss 0.844503\n",
      "\n",
      "Epoch %d: train loss %f 81 0.3598598263093403\n",
      "Epoch 81: val loss 0.859216\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3616123752934592\n",
      "Epoch 82: val loss 0.897894\n",
      "\n",
      "Epoch %d: train loss %f 83 0.32884311250277926\n",
      "Epoch 83: val loss 0.943749\n",
      "\n",
      "Epoch %d: train loss %f 84 0.3300075616155352\n",
      "Epoch 84: val loss 0.932635\n",
      "\n",
      "Epoch %d: train loss %f 85 0.41376250982284546\n",
      "Epoch 85: val loss 0.927288\n",
      "\n",
      "Epoch %d: train loss %f 86 0.3715341346604483\n",
      "Epoch 86: val loss 0.945028\n",
      "\n",
      "Epoch %d: train loss %f 87 0.2640987565474851\n",
      "Epoch 87: val loss 0.941873\n",
      "\n",
      "Epoch %d: train loss %f 88 0.4207858996731894\n",
      "Epoch 88: val loss 0.979546\n",
      "\n",
      "Epoch %d: train loss %f 89 0.33691422854151043\n",
      "Epoch 89: val loss 0.935986\n",
      "\n",
      "Epoch %d: train loss %f 90 0.3507224810974939\n",
      "Epoch 90: val loss 0.903300\n",
      "\n",
      "Epoch %d: train loss %f 91 0.38404638852391926\n",
      "Epoch 91: val loss 0.930843\n",
      "\n",
      "Epoch %d: train loss %f 92 0.3512674421072006\n",
      "Epoch 92: val loss 0.898295\n",
      "\n",
      "Epoch %d: train loss %f 93 0.3290713110140392\n",
      "Epoch 93: val loss 0.894663\n",
      "\n",
      "Epoch %d: train loss %f 94 0.3296970767634256\n",
      "Epoch 94: val loss 0.916309\n",
      "\n",
      "Epoch %d: train loss %f 95 0.35198784726006643\n",
      "Epoch 95: val loss 0.953974\n",
      "\n",
      "Epoch %d: train loss %f 96 0.2964685261249542\n",
      "Epoch 96: val loss 0.859001\n",
      "\n",
      "Epoch %d: train loss %f 97 0.3271844983100891\n",
      "Epoch 97: val loss 0.854492\n",
      "\n",
      "Epoch %d: train loss %f 98 0.32960561343601774\n",
      "Epoch 98: val loss 0.901880\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3085539085524423\n",
      "Epoch 99: val loss 0.885749\n",
      "\n",
      "Epoch %d: train loss %f 100 0.289958227957998\n",
      "Epoch 100: val loss 0.951249\n",
      "\n",
      "Epoch %d: train loss %f 101 0.28032289871147703\n",
      "Epoch 101: val loss 0.970972\n",
      "\n",
      "Epoch %d: train loss %f 102 0.3574490760053907\n",
      "Epoch 102: val loss 0.970988\n",
      "\n",
      "Epoch %d: train loss %f 103 0.31052066385746\n",
      "Epoch 103: val loss 0.960215\n",
      "\n",
      "Epoch %d: train loss %f 104 0.2842922934464046\n",
      "Epoch 104: val loss 0.955390\n",
      "\n",
      "Epoch %d: train loss %f 105 0.26766824828726904\n",
      "Epoch 105: val loss 1.006732\n",
      "\n",
      "Epoch %d: train loss %f 106 0.2845370407615389\n",
      "Epoch 106: val loss 0.975270\n",
      "\n",
      "Epoch %d: train loss %f 107 0.2967729313032968\n",
      "Epoch 107: val loss 0.971709\n",
      "\n",
      "Epoch %d: train loss %f 108 0.34089264486517223\n",
      "Epoch 108: val loss 0.936112\n",
      "\n",
      "Epoch %d: train loss %f 109 0.36416306240218027\n",
      "Epoch 109: val loss 0.941730\n",
      "\n",
      "Epoch %d: train loss %f 110 0.32997586897441317\n",
      "Epoch 110: val loss 0.985728\n",
      "\n",
      "Epoch %d: train loss %f 111 0.43328072343553814\n",
      "Epoch 111: val loss 0.948205\n",
      "\n",
      "Epoch %d: train loss %f 112 0.33635943276541574\n",
      "Epoch 112: val loss 0.977873\n",
      "\n",
      "Epoch %d: train loss %f 113 0.2788730891687529\n",
      "Epoch 113: val loss 0.923672\n",
      "\n",
      "Epoch %d: train loss %f 114 0.2962118110486439\n",
      "Epoch 114: val loss 0.900741\n",
      "\n",
      "Epoch %d: train loss %f 115 0.38777763502938406\n",
      "Epoch 115: val loss 0.961185\n",
      "\n",
      "Epoch %d: train loss %f 116 0.28615108345236095\n",
      "Epoch 116: val loss 0.926229\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3388682092939104\n",
      "Epoch 117: val loss 0.891358\n",
      "\n",
      "Epoch %d: train loss %f 118 0.35158769360610415\n",
      "Epoch 118: val loss 0.975746\n",
      "\n",
      "Epoch %d: train loss %f 119 0.31916703070913044\n",
      "Epoch 119: val loss 0.881696\n",
      "\n",
      "Epoch %d: train loss %f 120 0.301844697445631\n",
      "Epoch 120: val loss 0.876882\n",
      "\n",
      "Epoch %d: train loss %f 121 0.24109121305601938\n",
      "Epoch 121: val loss 0.924677\n",
      "\n",
      "Epoch %d: train loss %f 122 0.27850236211504253\n",
      "Epoch 122: val loss 0.899523\n",
      "\n",
      "Epoch %d: train loss %f 123 0.24850398089204515\n",
      "Epoch 123: val loss 0.905511\n",
      "\n",
      "Epoch %d: train loss %f 124 0.31960259803703855\n",
      "Epoch 124: val loss 0.941409\n",
      "\n",
      "Epoch %d: train loss %f 125 0.36629699383463177\n",
      "Epoch 125: val loss 0.928711\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3166565831218447\n",
      "Epoch 126: val loss 0.909017\n",
      "\n",
      "Epoch %d: train loss %f 127 0.3750117378575461\n",
      "Epoch 127: val loss 0.918038\n",
      "\n",
      "Epoch %d: train loss %f 128 0.27390238642692566\n",
      "Epoch 128: val loss 0.889027\n",
      "\n",
      "Epoch %d: train loss %f 129 0.2866009239639555\n",
      "Epoch 129: val loss 0.878131\n",
      "\n",
      "Epoch %d: train loss %f 130 0.33318451046943665\n",
      "Epoch 130: val loss 0.865753\n",
      "\n",
      "Epoch %d: train loss %f 131 0.2727742599589484\n",
      "Epoch 131: val loss 0.934340\n",
      "\n",
      "Epoch %d: train loss %f 132 0.2891299011451857\n",
      "Epoch 132: val loss 0.958231\n",
      "\n",
      "Epoch %d: train loss %f 133 0.2623915842601231\n",
      "Epoch 133: val loss 0.926909\n",
      "\n",
      "Epoch %d: train loss %f 134 0.3862889898674829\n",
      "Epoch 134: val loss 0.963928\n",
      "\n",
      "Epoch %d: train loss %f 135 0.2623689728123801\n",
      "Epoch 135: val loss 0.962846\n",
      "\n",
      "Epoch %d: train loss %f 136 0.34880482298987253\n",
      "Epoch 136: val loss 0.919498\n",
      "\n",
      "Epoch %d: train loss %f 137 0.2824979083878653\n",
      "Epoch 137: val loss 0.997173\n",
      "\n",
      "Epoch %d: train loss %f 138 0.2481777093240193\n",
      "Epoch 138: val loss 1.033347\n",
      "\n",
      "Epoch %d: train loss %f 139 0.2821072829621179\n",
      "Epoch 139: val loss 1.020903\n",
      "\n",
      "Epoch %d: train loss %f 140 0.26028590542929514\n",
      "Epoch 140: val loss 1.041426\n",
      "\n",
      "Epoch %d: train loss %f 141 0.27067769531692776\n",
      "Epoch 141: val loss 1.038404\n",
      "\n",
      "Epoch %d: train loss %f 142 0.2964991331100464\n",
      "Epoch 142: val loss 1.044650\n",
      "\n",
      "Epoch %d: train loss %f 143 0.27646435371467043\n",
      "Epoch 143: val loss 1.093569\n",
      "\n",
      "Epoch %d: train loss %f 144 0.23296319799763815\n",
      "Epoch 144: val loss 1.068290\n",
      "\n",
      "Epoch %d: train loss %f 145 0.2326541692018509\n",
      "Epoch 145: val loss 1.026699\n",
      "\n",
      "Epoch %d: train loss %f 146 0.27184041483061655\n",
      "Epoch 146: val loss 1.090737\n",
      "\n",
      "Epoch %d: train loss %f 147 0.32377807796001434\n",
      "Epoch 147: val loss 1.114806\n",
      "\n",
      "Epoch %d: train loss %f 148 0.2600403002330235\n",
      "Epoch 148: val loss 1.044195\n",
      "\n",
      "Epoch %d: train loss %f 149 0.263362209711756\n",
      "Epoch 149: val loss 1.040558\n",
      "\n",
      "Epoch %d: train loss %f 150 0.3451610952615738\n",
      "Epoch 150: val loss 1.037414\n",
      "\n",
      "Epoch %d: train loss %f 151 0.2952683802161898\n",
      "Epoch 151: val loss 1.122083\n",
      "\n",
      "Epoch %d: train loss %f 152 0.21647117180483683\n",
      "Epoch 152: val loss 1.070292\n",
      "\n",
      "Epoch %d: train loss %f 153 0.28509248580251423\n",
      "Epoch 153: val loss 1.048711\n",
      "\n",
      "Epoch %d: train loss %f 154 0.2507866195269993\n",
      "Epoch 154: val loss 1.114611\n",
      "\n",
      "Epoch %d: train loss %f 155 0.25437293520995546\n",
      "Epoch 155: val loss 1.025445\n",
      "\n",
      "Epoch %d: train loss %f 156 0.29456539877823423\n",
      "Epoch 156: val loss 1.038936\n",
      "\n",
      "Epoch %d: train loss %f 157 0.275924061025892\n",
      "Epoch 157: val loss 1.038911\n",
      "\n",
      "Epoch %d: train loss %f 158 0.2876888854163034\n",
      "Epoch 158: val loss 1.004071\n",
      "\n",
      "Epoch %d: train loss %f 159 0.2762744746037892\n",
      "Epoch 159: val loss 1.071958\n",
      "\n",
      "Epoch %d: train loss %f 160 0.29281358633722576\n",
      "Epoch 160: val loss 1.026471\n",
      "\n",
      "Epoch %d: train loss %f 161 0.2821065677063806\n",
      "Epoch 161: val loss 1.153193\n",
      "\n",
      "Epoch %d: train loss %f 162 0.2498384075505393\n",
      "Epoch 162: val loss 1.111734\n",
      "\n",
      "Epoch %d: train loss %f 163 0.28297634209905353\n",
      "Epoch 163: val loss 1.033609\n",
      "\n",
      "Epoch %d: train loss %f 164 0.2326290841613497\n",
      "Epoch 164: val loss 0.998509\n",
      "\n",
      "Epoch %d: train loss %f 165 0.2573446439845221\n",
      "Epoch 165: val loss 1.042006\n",
      "\n",
      "Epoch %d: train loss %f 166 0.26341267355850767\n",
      "Epoch 166: val loss 1.073455\n",
      "\n",
      "Epoch %d: train loss %f 167 0.22729743059192384\n",
      "Epoch 167: val loss 1.034715\n",
      "\n",
      "Epoch %d: train loss %f 168 0.24752833587782724\n",
      "Epoch 168: val loss 1.034251\n",
      "\n",
      "Epoch %d: train loss %f 169 0.22243537966694152\n",
      "Epoch 169: val loss 1.062018\n",
      "\n",
      "Epoch %d: train loss %f 170 0.2441610417195729\n",
      "Epoch 170: val loss 1.043613\n",
      "\n",
      "Epoch %d: train loss %f 171 0.28349785932472776\n",
      "Epoch 171: val loss 1.053237\n",
      "\n",
      "Epoch %d: train loss %f 172 0.29079332096236093\n",
      "Epoch 172: val loss 1.136182\n",
      "\n",
      "Epoch %d: train loss %f 173 0.3622533210686275\n",
      "Epoch 173: val loss 1.222279\n",
      "\n",
      "Epoch %d: train loss %f 174 0.22125150901930674\n",
      "Epoch 174: val loss 1.049587\n",
      "\n",
      "Epoch %d: train loss %f 175 0.2490058998976435\n",
      "Epoch 175: val loss 1.070891\n",
      "\n",
      "Epoch %d: train loss %f 176 0.282181407724108\n",
      "Epoch 176: val loss 1.018810\n",
      "\n",
      "Epoch %d: train loss %f 177 0.3173250343118395\n",
      "Epoch 177: val loss 0.989971\n",
      "\n",
      "Epoch %d: train loss %f 178 0.2200111033661025\n",
      "Epoch 178: val loss 1.105394\n",
      "\n",
      "Epoch %d: train loss %f 179 0.28341902792453766\n",
      "Epoch 179: val loss 1.115440\n",
      "\n",
      "Epoch %d: train loss %f 180 0.25856090017727446\n",
      "Epoch 180: val loss 1.045239\n",
      "\n",
      "Epoch %d: train loss %f 181 0.21170991233416966\n",
      "Epoch 181: val loss 1.033382\n",
      "\n",
      "Epoch %d: train loss %f 182 0.21327130922249385\n",
      "Epoch 182: val loss 1.044426\n",
      "\n",
      "Epoch %d: train loss %f 183 0.23289947637489863\n",
      "Epoch 183: val loss 1.101382\n",
      "\n",
      "Epoch %d: train loss %f 184 0.19053686197314942\n",
      "Epoch 184: val loss 1.117541\n",
      "\n",
      "Epoch %d: train loss %f 185 0.26659404592854635\n",
      "Epoch 185: val loss 1.161698\n",
      "\n",
      "Epoch %d: train loss %f 186 0.22741022280284337\n",
      "Epoch 186: val loss 1.231872\n",
      "\n",
      "Epoch %d: train loss %f 187 0.20352924721581594\n",
      "Epoch 187: val loss 1.136467\n",
      "\n",
      "Epoch %d: train loss %f 188 0.23785313751016343\n",
      "Epoch 188: val loss 1.161418\n",
      "\n",
      "Epoch %d: train loss %f 189 0.2543448411992618\n",
      "Epoch 189: val loss 1.145502\n",
      "\n",
      "Epoch %d: train loss %f 190 0.1980173055614744\n",
      "Epoch 190: val loss 1.131513\n",
      "\n",
      "Epoch %d: train loss %f 191 0.29830643108912874\n",
      "Epoch 191: val loss 1.192876\n",
      "\n",
      "Epoch %d: train loss %f 192 0.26225720984595163\n",
      "Epoch 192: val loss 1.086574\n",
      "\n",
      "Epoch %d: train loss %f 193 0.2516707607678005\n",
      "Epoch 193: val loss 1.091031\n",
      "\n",
      "Epoch %d: train loss %f 194 0.21138132682868413\n",
      "Epoch 194: val loss 1.111906\n",
      "\n",
      "Epoch %d: train loss %f 195 0.31218305443014416\n",
      "Epoch 195: val loss 1.103054\n",
      "\n",
      "Epoch %d: train loss %f 196 0.23277585208415985\n",
      "Epoch 196: val loss 1.103026\n",
      "\n",
      "Epoch %d: train loss %f 197 0.2510230030332293\n",
      "Epoch 197: val loss 1.093261\n",
      "\n",
      "Epoch %d: train loss %f 198 0.2788919359445572\n",
      "Epoch 198: val loss 1.134874\n",
      "\n",
      "Epoch %d: train loss %f 199 0.23841745299952372\n",
      "Epoch 199: val loss 1.250061\n",
      "\n",
      "Epoch %d: train loss %f 0 0.7023516694704691\n",
      "Epoch 0: val loss 0.700931\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6928546362453036\n",
      "Epoch 1: val loss 0.694727\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6840531494882371\n",
      "Epoch 2: val loss 0.684078\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6657704446050856\n",
      "Epoch 3: val loss 0.662311\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6441456211937798\n",
      "Epoch 4: val loss 0.627545\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6256683535046048\n",
      "Epoch 5: val loss 0.588477\n",
      "\n",
      "Epoch %d: train loss %f 6 0.603474862045712\n",
      "Epoch 6: val loss 0.572996\n",
      "\n",
      "Epoch %d: train loss %f 7 0.602857318189409\n",
      "Epoch 7: val loss 0.584960\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5603263477484385\n",
      "Epoch 8: val loss 0.610974\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5689626667234633\n",
      "Epoch 9: val loss 0.582731\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5515082213613722\n",
      "Epoch 10: val loss 0.603931\n",
      "\n",
      "Epoch %d: train loss %f 11 0.539985888534122\n",
      "Epoch 11: val loss 0.603032\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5434954596890343\n",
      "Epoch 12: val loss 0.614422\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5513221522172292\n",
      "Epoch 13: val loss 0.615503\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5341804888513353\n",
      "Epoch 14: val loss 0.593820\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5329274468951755\n",
      "Epoch 15: val loss 0.610727\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5144052472379472\n",
      "Epoch 16: val loss 0.616616\n",
      "\n",
      "Epoch %d: train loss %f 17 0.4997740421030257\n",
      "Epoch 17: val loss 0.661818\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5221492052078247\n",
      "Epoch 18: val loss 0.646930\n",
      "\n",
      "Epoch %d: train loss %f 19 0.4851534267266591\n",
      "Epoch 19: val loss 0.654342\n",
      "\n",
      "Epoch %d: train loss %f 20 0.4885067542394002\n",
      "Epoch 20: val loss 0.667124\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5036965443028344\n",
      "Epoch 21: val loss 0.670439\n",
      "\n",
      "Epoch %d: train loss %f 22 0.4934043486913045\n",
      "Epoch 22: val loss 0.672112\n",
      "\n",
      "Epoch %d: train loss %f 23 0.4967653850714366\n",
      "Epoch 23: val loss 0.638817\n",
      "\n",
      "Epoch %d: train loss %f 24 0.4845926132467058\n",
      "Epoch 24: val loss 0.634643\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4946003059546153\n",
      "Epoch 25: val loss 0.684324\n",
      "\n",
      "Epoch %d: train loss %f 26 0.5033996403217316\n",
      "Epoch 26: val loss 0.656112\n",
      "\n",
      "Epoch %d: train loss %f 27 0.48811211850908065\n",
      "Epoch 27: val loss 0.678202\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4845739205678304\n",
      "Epoch 28: val loss 0.682359\n",
      "\n",
      "Epoch %d: train loss %f 29 0.49788426359494525\n",
      "Epoch 29: val loss 0.705683\n",
      "\n",
      "Epoch %d: train loss %f 30 0.5360621809959412\n",
      "Epoch 30: val loss 0.683206\n",
      "\n",
      "Epoch %d: train loss %f 31 0.5277349286609225\n",
      "Epoch 31: val loss 0.689012\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4649463825755649\n",
      "Epoch 32: val loss 0.707096\n",
      "\n",
      "Epoch %d: train loss %f 33 0.5578212870491875\n",
      "Epoch 33: val loss 0.687046\n",
      "\n",
      "Epoch %d: train loss %f 34 0.5118202136622535\n",
      "Epoch 34: val loss 0.721967\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4700104859140184\n",
      "Epoch 35: val loss 0.714841\n",
      "\n",
      "Epoch %d: train loss %f 36 0.5142609079678854\n",
      "Epoch 36: val loss 0.745192\n",
      "\n",
      "Epoch %d: train loss %f 37 0.5007955431938171\n",
      "Epoch 37: val loss 0.726274\n",
      "\n",
      "Epoch %d: train loss %f 38 0.5085941321320004\n",
      "Epoch 38: val loss 0.732399\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4940667019950019\n",
      "Epoch 39: val loss 0.735456\n",
      "\n",
      "Epoch %d: train loss %f 40 0.49158933758735657\n",
      "Epoch 40: val loss 0.688572\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4685434069898393\n",
      "Epoch 41: val loss 0.703728\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4555300937758552\n",
      "Epoch 42: val loss 0.709537\n",
      "\n",
      "Epoch %d: train loss %f 43 0.43133049706617993\n",
      "Epoch 43: val loss 0.706342\n",
      "\n",
      "Epoch %d: train loss %f 44 0.4318664189842012\n",
      "Epoch 44: val loss 0.698948\n",
      "\n",
      "Epoch %d: train loss %f 45 0.4540555775165558\n",
      "Epoch 45: val loss 0.713247\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4432225161128574\n",
      "Epoch 46: val loss 0.729205\n",
      "\n",
      "Epoch %d: train loss %f 47 0.46250348952081466\n",
      "Epoch 47: val loss 0.739073\n",
      "\n",
      "Epoch %d: train loss %f 48 0.47935739490720963\n",
      "Epoch 48: val loss 0.774908\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4520590735806359\n",
      "Epoch 49: val loss 0.724962\n",
      "\n",
      "Epoch %d: train loss %f 50 0.4370666676097446\n",
      "Epoch 50: val loss 0.720371\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4388306240240733\n",
      "Epoch 51: val loss 0.758924\n",
      "\n",
      "Epoch %d: train loss %f 52 0.47769679294692147\n",
      "Epoch 52: val loss 0.774412\n",
      "\n",
      "Epoch %d: train loss %f 53 0.46771928668022156\n",
      "Epoch 53: val loss 0.778807\n",
      "\n",
      "Epoch %d: train loss %f 54 0.4362546569771237\n",
      "Epoch 54: val loss 0.814329\n",
      "\n",
      "Epoch %d: train loss %f 55 0.42498013708326554\n",
      "Epoch 55: val loss 0.773740\n",
      "\n",
      "Epoch %d: train loss %f 56 0.43945637676450944\n",
      "Epoch 56: val loss 0.770751\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4874398476547665\n",
      "Epoch 57: val loss 0.752456\n",
      "\n",
      "Epoch %d: train loss %f 58 0.4577632248401642\n",
      "Epoch 58: val loss 0.730108\n",
      "\n",
      "Epoch %d: train loss %f 59 0.47349000970522565\n",
      "Epoch 59: val loss 0.735723\n",
      "\n",
      "Epoch %d: train loss %f 60 0.46713441610336304\n",
      "Epoch 60: val loss 0.749881\n",
      "\n",
      "Epoch %d: train loss %f 61 0.4426266286108229\n",
      "Epoch 61: val loss 0.781180\n",
      "\n",
      "Epoch %d: train loss %f 62 0.4180940356519487\n",
      "Epoch 62: val loss 0.775907\n",
      "\n",
      "Epoch %d: train loss %f 63 0.4458978772163391\n",
      "Epoch 63: val loss 0.789934\n",
      "\n",
      "Epoch %d: train loss %f 64 0.47269606921407914\n",
      "Epoch 64: val loss 0.779079\n",
      "\n",
      "Epoch %d: train loss %f 65 0.4251365595393711\n",
      "Epoch 65: val loss 0.746632\n",
      "\n",
      "Epoch %d: train loss %f 66 0.40854977402422166\n",
      "Epoch 66: val loss 0.765642\n",
      "\n",
      "Epoch %d: train loss %f 67 0.4460095266501109\n",
      "Epoch 67: val loss 0.745982\n",
      "\n",
      "Epoch %d: train loss %f 68 0.45558487706714207\n",
      "Epoch 68: val loss 0.805608\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4624875783920288\n",
      "Epoch 69: val loss 0.846378\n",
      "\n",
      "Epoch %d: train loss %f 70 0.4038647611935933\n",
      "Epoch 70: val loss 0.776840\n",
      "\n",
      "Epoch %d: train loss %f 71 0.4715304176012675\n",
      "Epoch 71: val loss 0.750888\n",
      "\n",
      "Epoch %d: train loss %f 72 0.4595562716325124\n",
      "Epoch 72: val loss 0.784702\n",
      "\n",
      "Epoch %d: train loss %f 73 0.42970864640341866\n",
      "Epoch 73: val loss 0.777111\n",
      "\n",
      "Epoch %d: train loss %f 74 0.42943373653623795\n",
      "Epoch 74: val loss 0.779797\n",
      "\n",
      "Epoch %d: train loss %f 75 0.48356006542841595\n",
      "Epoch 75: val loss 0.774191\n",
      "\n",
      "Epoch %d: train loss %f 76 0.41287540396054584\n",
      "Epoch 76: val loss 0.803256\n",
      "\n",
      "Epoch %d: train loss %f 77 0.40516753329171074\n",
      "Epoch 77: val loss 0.794882\n",
      "\n",
      "Epoch %d: train loss %f 78 0.4129560987154643\n",
      "Epoch 78: val loss 0.768114\n",
      "\n",
      "Epoch %d: train loss %f 79 0.3851317995124393\n",
      "Epoch 79: val loss 0.759993\n",
      "\n",
      "Epoch %d: train loss %f 80 0.39607985814412433\n",
      "Epoch 80: val loss 0.772230\n",
      "\n",
      "Epoch %d: train loss %f 81 0.39438988268375397\n",
      "Epoch 81: val loss 0.793579\n",
      "\n",
      "Epoch %d: train loss %f 82 0.4042283892631531\n",
      "Epoch 82: val loss 0.800575\n",
      "\n",
      "Epoch %d: train loss %f 83 0.40261559188365936\n",
      "Epoch 83: val loss 0.797355\n",
      "\n",
      "Epoch %d: train loss %f 84 0.4038016613986757\n",
      "Epoch 84: val loss 0.791772\n",
      "\n",
      "Epoch %d: train loss %f 85 0.4137206044461992\n",
      "Epoch 85: val loss 0.812341\n",
      "\n",
      "Epoch %d: train loss %f 86 0.36682230482498807\n",
      "Epoch 86: val loss 0.834315\n",
      "\n",
      "Epoch %d: train loss %f 87 0.41841954323980546\n",
      "Epoch 87: val loss 0.895738\n",
      "\n",
      "Epoch %d: train loss %f 88 0.3931221125854386\n",
      "Epoch 88: val loss 0.856225\n",
      "\n",
      "Epoch %d: train loss %f 89 0.4162796338399251\n",
      "Epoch 89: val loss 0.799523\n",
      "\n",
      "Epoch %d: train loss %f 90 0.4318981303109063\n",
      "Epoch 90: val loss 0.808497\n",
      "\n",
      "Epoch %d: train loss %f 91 0.4203442699379391\n",
      "Epoch 91: val loss 0.832804\n",
      "\n",
      "Epoch %d: train loss %f 92 0.4161902931001451\n",
      "Epoch 92: val loss 0.812070\n",
      "\n",
      "Epoch %d: train loss %f 93 0.4018969502713945\n",
      "Epoch 93: val loss 0.861375\n",
      "\n",
      "Epoch %d: train loss %f 94 0.37879623141553664\n",
      "Epoch 94: val loss 0.816358\n",
      "\n",
      "Epoch %d: train loss %f 95 0.40042244063483345\n",
      "Epoch 95: val loss 0.841919\n",
      "\n",
      "Epoch %d: train loss %f 96 0.36667658885320026\n",
      "Epoch 96: val loss 0.800543\n",
      "\n",
      "Epoch %d: train loss %f 97 0.45516003833876717\n",
      "Epoch 97: val loss 0.836976\n",
      "\n",
      "Epoch %d: train loss %f 98 0.39692684676912093\n",
      "Epoch 98: val loss 0.786245\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3635935816499922\n",
      "Epoch 99: val loss 0.808660\n",
      "\n",
      "Epoch %d: train loss %f 100 0.39817960063616437\n",
      "Epoch 100: val loss 0.845541\n",
      "\n",
      "Epoch %d: train loss %f 101 0.4152396188841926\n",
      "Epoch 101: val loss 0.822062\n",
      "\n",
      "Epoch %d: train loss %f 0 0.7079586610198021\n",
      "Epoch 0: val loss 0.705830\n",
      "\n",
      "Epoch %d: train loss %f 1 0.7003252059221268\n",
      "Epoch 1: val loss 0.702071\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6920088827610016\n",
      "Epoch 2: val loss 0.697182\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6827445700764656\n",
      "Epoch 3: val loss 0.689562\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6721726655960083\n",
      "Epoch 4: val loss 0.675806\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6557123139500618\n",
      "Epoch 5: val loss 0.651447\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6233873292803764\n",
      "Epoch 6: val loss 0.616809\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5951116159558296\n",
      "Epoch 7: val loss 0.570753\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5541023649275303\n",
      "Epoch 8: val loss 0.530632\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5345561727881432\n",
      "Epoch 9: val loss 0.505406\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5167801268398762\n",
      "Epoch 10: val loss 0.494879\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5007393173873425\n",
      "Epoch 11: val loss 0.487616\n",
      "\n",
      "Epoch %d: train loss %f 12 0.4845500588417053\n",
      "Epoch 12: val loss 0.495163\n",
      "\n",
      "Epoch %d: train loss %f 13 0.4853024408221245\n",
      "Epoch 13: val loss 0.476664\n",
      "\n",
      "Epoch %d: train loss %f 14 0.48564397543668747\n",
      "Epoch 14: val loss 0.473323\n",
      "\n",
      "Epoch %d: train loss %f 15 0.48606352135539055\n",
      "Epoch 15: val loss 0.490557\n",
      "\n",
      "Epoch %d: train loss %f 16 0.4912108704447746\n",
      "Epoch 16: val loss 0.491155\n",
      "\n",
      "Epoch %d: train loss %f 17 0.47718125581741333\n",
      "Epoch 17: val loss 0.470572\n",
      "\n",
      "Epoch %d: train loss %f 18 0.4586019925773144\n",
      "Epoch 18: val loss 0.482448\n",
      "\n",
      "Epoch %d: train loss %f 19 0.4524844288825989\n",
      "Epoch 19: val loss 0.468059\n",
      "\n",
      "Epoch %d: train loss %f 20 0.46429265663027763\n",
      "Epoch 20: val loss 0.458168\n",
      "\n",
      "Epoch %d: train loss %f 21 0.4460253529250622\n",
      "Epoch 21: val loss 0.468925\n",
      "\n",
      "Epoch %d: train loss %f 22 0.4498162679374218\n",
      "Epoch 22: val loss 0.453396\n",
      "\n",
      "Epoch %d: train loss %f 23 0.4488941580057144\n",
      "Epoch 23: val loss 0.455061\n",
      "\n",
      "Epoch %d: train loss %f 24 0.44650447741150856\n",
      "Epoch 24: val loss 0.461379\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4524805024266243\n",
      "Epoch 25: val loss 0.445725\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4618813209235668\n",
      "Epoch 26: val loss 0.447751\n",
      "\n",
      "Epoch %d: train loss %f 27 0.4454190284013748\n",
      "Epoch 27: val loss 0.461674\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4534415304660797\n",
      "Epoch 28: val loss 0.458348\n",
      "\n",
      "Epoch %d: train loss %f 29 0.43145960569381714\n",
      "Epoch 29: val loss 0.456212\n",
      "\n",
      "Epoch %d: train loss %f 30 0.4407472424209118\n",
      "Epoch 30: val loss 0.458986\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4052327387034893\n",
      "Epoch 31: val loss 0.457629\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4183228760957718\n",
      "Epoch 32: val loss 0.456150\n",
      "\n",
      "Epoch %d: train loss %f 33 0.414882056415081\n",
      "Epoch 33: val loss 0.460285\n",
      "\n",
      "Epoch %d: train loss %f 34 0.40045029297471046\n",
      "Epoch 34: val loss 0.459652\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4138556532561779\n",
      "Epoch 35: val loss 0.469044\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4033956192433834\n",
      "Epoch 36: val loss 0.474069\n",
      "\n",
      "Epoch %d: train loss %f 37 0.40431595779955387\n",
      "Epoch 37: val loss 0.465925\n",
      "\n",
      "Epoch %d: train loss %f 38 0.39353842101991177\n",
      "Epoch 38: val loss 0.461511\n",
      "\n",
      "Epoch %d: train loss %f 39 0.404015738517046\n",
      "Epoch 39: val loss 0.477923\n",
      "\n",
      "Epoch %d: train loss %f 40 0.40299852192401886\n",
      "Epoch 40: val loss 0.483592\n",
      "\n",
      "Epoch %d: train loss %f 41 0.38650794699788094\n",
      "Epoch 41: val loss 0.488259\n",
      "\n",
      "Epoch %d: train loss %f 42 0.3838004507124424\n",
      "Epoch 42: val loss 0.478105\n",
      "\n",
      "Epoch %d: train loss %f 43 0.3786207064986229\n",
      "Epoch 43: val loss 0.477648\n",
      "\n",
      "Epoch %d: train loss %f 44 0.41972659155726433\n",
      "Epoch 44: val loss 0.470481\n",
      "\n",
      "Epoch %d: train loss %f 45 0.36095473542809486\n",
      "Epoch 45: val loss 0.479384\n",
      "\n",
      "Epoch %d: train loss %f 46 0.3996285945177078\n",
      "Epoch 46: val loss 0.477122\n",
      "\n",
      "Epoch %d: train loss %f 47 0.3576322663575411\n",
      "Epoch 47: val loss 0.493241\n",
      "\n",
      "Epoch %d: train loss %f 48 0.36457644402980804\n",
      "Epoch 48: val loss 0.504642\n",
      "\n",
      "Epoch %d: train loss %f 49 0.35333024710416794\n",
      "Epoch 49: val loss 0.490362\n",
      "\n",
      "Epoch %d: train loss %f 50 0.37463027238845825\n",
      "Epoch 50: val loss 0.479455\n",
      "\n",
      "Epoch %d: train loss %f 51 0.36888647824525833\n",
      "Epoch 51: val loss 0.501910\n",
      "\n",
      "Epoch %d: train loss %f 52 0.3812545742839575\n",
      "Epoch 52: val loss 0.502600\n",
      "\n",
      "Epoch %d: train loss %f 53 0.3675963841378689\n",
      "Epoch 53: val loss 0.488300\n",
      "\n",
      "Epoch %d: train loss %f 54 0.3612270150333643\n",
      "Epoch 54: val loss 0.485874\n",
      "\n",
      "Epoch %d: train loss %f 55 0.35209404677152634\n",
      "Epoch 55: val loss 0.506238\n",
      "\n",
      "Epoch %d: train loss %f 56 0.33725993894040585\n",
      "Epoch 56: val loss 0.523823\n",
      "\n",
      "Epoch %d: train loss %f 57 0.3078133575618267\n",
      "Epoch 57: val loss 0.538468\n",
      "\n",
      "Epoch %d: train loss %f 58 0.33789003267884254\n",
      "Epoch 58: val loss 0.536555\n",
      "\n",
      "Epoch %d: train loss %f 59 0.3221881967037916\n",
      "Epoch 59: val loss 0.543209\n",
      "\n",
      "Epoch %d: train loss %f 60 0.3485727049410343\n",
      "Epoch 60: val loss 0.551061\n",
      "\n",
      "Epoch %d: train loss %f 61 0.3400645237416029\n",
      "Epoch 61: val loss 0.547684\n",
      "\n",
      "Epoch %d: train loss %f 62 0.34310903400182724\n",
      "Epoch 62: val loss 0.553222\n",
      "\n",
      "Epoch %d: train loss %f 63 0.35739320889115334\n",
      "Epoch 63: val loss 0.545951\n",
      "\n",
      "Epoch %d: train loss %f 64 0.32032056525349617\n",
      "Epoch 64: val loss 0.538093\n",
      "\n",
      "Epoch %d: train loss %f 65 0.34730118326842785\n",
      "Epoch 65: val loss 0.553092\n",
      "\n",
      "Epoch %d: train loss %f 66 0.3466462716460228\n",
      "Epoch 66: val loss 0.541283\n",
      "\n",
      "Epoch %d: train loss %f 67 0.3323992807418108\n",
      "Epoch 67: val loss 0.541125\n",
      "\n",
      "Epoch %d: train loss %f 68 0.32880635373294353\n",
      "Epoch 68: val loss 0.542771\n",
      "\n",
      "Epoch %d: train loss %f 69 0.33614790067076683\n",
      "Epoch 69: val loss 0.548539\n",
      "\n",
      "Epoch %d: train loss %f 70 0.3311399780213833\n",
      "Epoch 70: val loss 0.567016\n",
      "\n",
      "Epoch %d: train loss %f 71 0.3031708784401417\n",
      "Epoch 71: val loss 0.573287\n",
      "\n",
      "Epoch %d: train loss %f 72 0.29874186404049397\n",
      "Epoch 72: val loss 0.575186\n",
      "\n",
      "Epoch %d: train loss %f 73 0.30730859003961086\n",
      "Epoch 73: val loss 0.572677\n",
      "\n",
      "Epoch %d: train loss %f 74 0.30111613497138023\n",
      "Epoch 74: val loss 0.557256\n",
      "\n",
      "Epoch %d: train loss %f 75 0.32640617340803146\n",
      "Epoch 75: val loss 0.561895\n",
      "\n",
      "Epoch %d: train loss %f 76 0.2798094265162945\n",
      "Epoch 76: val loss 0.578738\n",
      "\n",
      "Epoch %d: train loss %f 77 0.30458337627351284\n",
      "Epoch 77: val loss 0.583940\n",
      "\n",
      "Epoch %d: train loss %f 78 0.32717623747885227\n",
      "Epoch 78: val loss 0.592500\n",
      "\n",
      "Epoch %d: train loss %f 79 0.3159031793475151\n",
      "Epoch 79: val loss 0.601946\n",
      "\n",
      "Epoch %d: train loss %f 80 0.30142368748784065\n",
      "Epoch 80: val loss 0.586011\n",
      "\n",
      "Epoch %d: train loss %f 81 0.29569775238633156\n",
      "Epoch 81: val loss 0.569952\n",
      "\n",
      "Epoch %d: train loss %f 82 0.30336261354386806\n",
      "Epoch 82: val loss 0.572358\n",
      "\n",
      "Epoch %d: train loss %f 83 0.3076526094228029\n",
      "Epoch 83: val loss 0.596432\n",
      "\n",
      "Epoch %d: train loss %f 84 0.30494197085499763\n",
      "Epoch 84: val loss 0.588883\n",
      "\n",
      "Epoch %d: train loss %f 85 0.2788111288100481\n",
      "Epoch 85: val loss 0.592076\n",
      "\n",
      "Epoch %d: train loss %f 86 0.26613347977399826\n",
      "Epoch 86: val loss 0.593323\n",
      "\n",
      "Epoch %d: train loss %f 87 0.3052053563296795\n",
      "Epoch 87: val loss 0.613822\n",
      "\n",
      "Epoch %d: train loss %f 88 0.27860905416309834\n",
      "Epoch 88: val loss 0.629182\n",
      "\n",
      "Epoch %d: train loss %f 89 0.2737193536013365\n",
      "Epoch 89: val loss 0.632901\n",
      "\n",
      "Epoch %d: train loss %f 90 0.3041132241487503\n",
      "Epoch 90: val loss 0.639568\n",
      "\n",
      "Epoch %d: train loss %f 91 0.2563244663178921\n",
      "Epoch 91: val loss 0.639188\n",
      "\n",
      "Epoch %d: train loss %f 92 0.2835789658129215\n",
      "Epoch 92: val loss 0.634795\n",
      "\n",
      "Epoch %d: train loss %f 93 0.2784481942653656\n",
      "Epoch 93: val loss 0.633892\n",
      "\n",
      "Epoch %d: train loss %f 94 0.26507158391177654\n",
      "Epoch 94: val loss 0.637534\n",
      "\n",
      "Epoch %d: train loss %f 95 0.31847839057445526\n",
      "Epoch 95: val loss 0.679255\n",
      "\n",
      "Epoch %d: train loss %f 96 0.270158926025033\n",
      "Epoch 96: val loss 0.681173\n",
      "\n",
      "Epoch %d: train loss %f 97 0.2652976755052805\n",
      "Epoch 97: val loss 0.651749\n",
      "\n",
      "Epoch %d: train loss %f 98 0.2504113744944334\n",
      "Epoch 98: val loss 0.645623\n",
      "\n",
      "Epoch %d: train loss %f 99 0.29153757728636265\n",
      "Epoch 99: val loss 0.651099\n",
      "\n",
      "Epoch %d: train loss %f 100 0.24899969808757305\n",
      "Epoch 100: val loss 0.654582\n",
      "\n",
      "Epoch %d: train loss %f 101 0.2811285499483347\n",
      "Epoch 101: val loss 0.681944\n",
      "\n",
      "Epoch %d: train loss %f 102 0.2837067898362875\n",
      "Epoch 102: val loss 0.666526\n",
      "\n",
      "Epoch %d: train loss %f 103 0.2700225729495287\n",
      "Epoch 103: val loss 0.654943\n",
      "\n",
      "Epoch %d: train loss %f 104 0.2524546645581722\n",
      "Epoch 104: val loss 0.642774\n",
      "\n",
      "Epoch %d: train loss %f 105 0.23148104175925255\n",
      "Epoch 105: val loss 0.669713\n",
      "\n",
      "Epoch %d: train loss %f 106 0.23800565022975206\n",
      "Epoch 106: val loss 0.700147\n",
      "\n",
      "Epoch %d: train loss %f 107 0.2509881090372801\n",
      "Epoch 107: val loss 0.687470\n",
      "\n",
      "Epoch %d: train loss %f 108 0.2624137233942747\n",
      "Epoch 108: val loss 0.674805\n",
      "\n",
      "Epoch %d: train loss %f 109 0.2573716826736927\n",
      "Epoch 109: val loss 0.688498\n",
      "\n",
      "Epoch %d: train loss %f 110 0.22093265037983656\n",
      "Epoch 110: val loss 0.665034\n",
      "\n",
      "Epoch %d: train loss %f 111 0.2228217851370573\n",
      "Epoch 111: val loss 0.658954\n",
      "\n",
      "Epoch %d: train loss %f 112 0.24545317143201828\n",
      "Epoch 112: val loss 0.691336\n",
      "\n",
      "Epoch %d: train loss %f 113 0.26707463152706623\n",
      "Epoch 113: val loss 0.698626\n",
      "\n",
      "Epoch %d: train loss %f 114 0.27823260612785816\n",
      "Epoch 114: val loss 0.720973\n",
      "\n",
      "Epoch %d: train loss %f 115 0.26593622006475925\n",
      "Epoch 115: val loss 0.702350\n",
      "\n",
      "Epoch %d: train loss %f 116 0.25336621701717377\n",
      "Epoch 116: val loss 0.688404\n",
      "\n",
      "Epoch %d: train loss %f 117 0.23560460284352303\n",
      "Epoch 117: val loss 0.678852\n",
      "\n",
      "Epoch %d: train loss %f 118 0.20481976307928562\n",
      "Epoch 118: val loss 0.691017\n",
      "\n",
      "Epoch %d: train loss %f 119 0.24884146265685558\n",
      "Epoch 119: val loss 0.697926\n",
      "\n",
      "Epoch %d: train loss %f 120 0.24317256174981594\n",
      "Epoch 120: val loss 0.694064\n",
      "\n",
      "Epoch %d: train loss %f 121 0.24452148098498583\n",
      "Epoch 121: val loss 0.704230\n",
      "\n",
      "Epoch %d: train loss %f 122 0.26182676665484905\n",
      "Epoch 122: val loss 0.742733\n",
      "\n",
      "Epoch %d: train loss %f 123 0.2339819958433509\n",
      "Epoch 123: val loss 0.726146\n",
      "\n",
      "Epoch %d: train loss %f 124 0.23083209991455078\n",
      "Epoch 124: val loss 0.712264\n",
      "\n",
      "Epoch %d: train loss %f 125 0.24527338333427906\n",
      "Epoch 125: val loss 0.724038\n",
      "\n",
      "Epoch %d: train loss %f 126 0.24890088476240635\n",
      "Epoch 126: val loss 0.728569\n",
      "\n",
      "Epoch %d: train loss %f 127 0.2548125088214874\n",
      "Epoch 127: val loss 0.743552\n",
      "\n",
      "Epoch %d: train loss %f 128 0.2206469029188156\n",
      "Epoch 128: val loss 0.743607\n",
      "\n",
      "Epoch %d: train loss %f 129 0.31482565589249134\n",
      "Epoch 129: val loss 0.711597\n",
      "\n",
      "Epoch %d: train loss %f 130 0.26101888343691826\n",
      "Epoch 130: val loss 0.714225\n",
      "\n",
      "Epoch %d: train loss %f 131 0.20284503791481256\n",
      "Epoch 131: val loss 0.705507\n",
      "\n",
      "Epoch %d: train loss %f 132 0.20769946370273829\n",
      "Epoch 132: val loss 0.694468\n",
      "\n",
      "Epoch %d: train loss %f 133 0.24721679277718067\n",
      "Epoch 133: val loss 0.694027\n",
      "\n",
      "Epoch %d: train loss %f 134 0.2571283560246229\n",
      "Epoch 134: val loss 0.715134\n",
      "\n",
      "Epoch %d: train loss %f 135 0.2193081872537732\n",
      "Epoch 135: val loss 0.721363\n",
      "\n",
      "Epoch %d: train loss %f 136 0.2578658973798156\n",
      "Epoch 136: val loss 0.700097\n",
      "\n",
      "Epoch %d: train loss %f 137 0.2522546909749508\n",
      "Epoch 137: val loss 0.691647\n",
      "\n",
      "Epoch %d: train loss %f 138 0.24621316976845264\n",
      "Epoch 138: val loss 0.708486\n",
      "\n",
      "Epoch %d: train loss %f 139 0.21859492361545563\n",
      "Epoch 139: val loss 0.701850\n",
      "\n",
      "Epoch %d: train loss %f 140 0.22667227406054735\n",
      "Epoch 140: val loss 0.702212\n",
      "\n",
      "Epoch %d: train loss %f 141 0.23675060644745827\n",
      "Epoch 141: val loss 0.732721\n",
      "\n",
      "Epoch %d: train loss %f 142 0.2099661622196436\n",
      "Epoch 142: val loss 0.742817\n",
      "\n",
      "Epoch %d: train loss %f 143 0.21533622033894062\n",
      "Epoch 143: val loss 0.718328\n",
      "\n",
      "Epoch %d: train loss %f 144 0.1856455011293292\n",
      "Epoch 144: val loss 0.738888\n",
      "\n",
      "Epoch %d: train loss %f 145 0.20698538795113564\n",
      "Epoch 145: val loss 0.733681\n",
      "\n",
      "Epoch %d: train loss %f 146 0.2205882305279374\n",
      "Epoch 146: val loss 0.744193\n",
      "\n",
      "Epoch %d: train loss %f 147 0.22373792715370655\n",
      "Epoch 147: val loss 0.730657\n",
      "\n",
      "Epoch %d: train loss %f 148 0.21923827938735485\n",
      "Epoch 148: val loss 0.755260\n",
      "\n",
      "Epoch %d: train loss %f 149 0.26234027929604053\n",
      "Epoch 149: val loss 0.736415\n",
      "\n",
      "Epoch %d: train loss %f 150 0.26632831431925297\n",
      "Epoch 150: val loss 0.733139\n",
      "\n",
      "Epoch %d: train loss %f 151 0.22641117218881845\n",
      "Epoch 151: val loss 0.746324\n",
      "\n",
      "Epoch %d: train loss %f 152 0.19523650035262108\n",
      "Epoch 152: val loss 0.742720\n",
      "\n",
      "Epoch %d: train loss %f 153 0.19020290952175856\n",
      "Epoch 153: val loss 0.750997\n",
      "\n",
      "Epoch %d: train loss %f 154 0.16307751275599003\n",
      "Epoch 154: val loss 0.760988\n",
      "\n",
      "Epoch %d: train loss %f 155 0.1967124929651618\n",
      "Epoch 155: val loss 0.784366\n",
      "\n",
      "Epoch %d: train loss %f 156 0.19678271934390068\n",
      "Epoch 156: val loss 0.763761\n",
      "\n",
      "Epoch %d: train loss %f 157 0.21867123804986477\n",
      "Epoch 157: val loss 0.743114\n",
      "\n",
      "Epoch %d: train loss %f 158 0.21524787042289972\n",
      "Epoch 158: val loss 0.748016\n",
      "\n",
      "Epoch %d: train loss %f 159 0.17086671199649572\n",
      "Epoch 159: val loss 0.800536\n",
      "\n",
      "Epoch %d: train loss %f 160 0.18577009998261929\n",
      "Epoch 160: val loss 0.821479\n",
      "\n",
      "Epoch %d: train loss %f 161 0.18346656300127506\n",
      "Epoch 161: val loss 0.787805\n",
      "\n",
      "Epoch %d: train loss %f 162 0.2075502099469304\n",
      "Epoch 162: val loss 0.755867\n",
      "\n",
      "Epoch %d: train loss %f 163 0.2259658258408308\n",
      "Epoch 163: val loss 0.755266\n",
      "\n",
      "Epoch %d: train loss %f 164 0.2032527569681406\n",
      "Epoch 164: val loss 0.750132\n",
      "\n",
      "Epoch %d: train loss %f 165 0.18925295770168304\n",
      "Epoch 165: val loss 0.758232\n",
      "\n",
      "Epoch %d: train loss %f 166 0.16051506716758013\n",
      "Epoch 166: val loss 0.797107\n",
      "\n",
      "Epoch %d: train loss %f 167 0.17021574266254902\n",
      "Epoch 167: val loss 0.812169\n",
      "\n",
      "Epoch %d: train loss %f 168 0.16010543704032898\n",
      "Epoch 168: val loss 0.836241\n",
      "\n",
      "Epoch %d: train loss %f 169 0.17684121802449226\n",
      "Epoch 169: val loss 0.830382\n",
      "\n",
      "Epoch %d: train loss %f 170 0.16677198559045792\n",
      "Epoch 170: val loss 0.798435\n",
      "\n",
      "Epoch %d: train loss %f 171 0.1934984028339386\n",
      "Epoch 171: val loss 0.799463\n",
      "\n",
      "Epoch %d: train loss %f 172 0.20668267644941807\n",
      "Epoch 172: val loss 0.831963\n",
      "\n",
      "Epoch %d: train loss %f 173 0.16743532568216324\n",
      "Epoch 173: val loss 0.832078\n",
      "\n",
      "Epoch %d: train loss %f 174 0.18013204541057348\n",
      "Epoch 174: val loss 0.796216\n",
      "\n",
      "Epoch %d: train loss %f 175 0.15482601802796125\n",
      "Epoch 175: val loss 0.820205\n",
      "\n",
      "Epoch %d: train loss %f 176 0.17168965097516775\n",
      "Epoch 176: val loss 0.856901\n",
      "\n",
      "Epoch %d: train loss %f 177 0.14812427293509245\n",
      "Epoch 177: val loss 0.820592\n",
      "\n",
      "Epoch %d: train loss %f 178 0.18790154624730349\n",
      "Epoch 178: val loss 0.777282\n",
      "\n",
      "Epoch %d: train loss %f 179 0.15716442000120878\n",
      "Epoch 179: val loss 0.815541\n",
      "\n",
      "Epoch %d: train loss %f 180 0.16459602676331997\n",
      "Epoch 180: val loss 0.841724\n",
      "\n",
      "Epoch %d: train loss %f 181 0.1670694388449192\n",
      "Epoch 181: val loss 0.856351\n",
      "\n",
      "Epoch %d: train loss %f 182 0.24219908565282822\n",
      "Epoch 182: val loss 0.888382\n",
      "\n",
      "Epoch %d: train loss %f 183 0.19150657393038273\n",
      "Epoch 183: val loss 0.856305\n",
      "\n",
      "Epoch %d: train loss %f 184 0.1961818328127265\n",
      "Epoch 184: val loss 0.829481\n",
      "\n",
      "Epoch %d: train loss %f 185 0.18726198747754097\n",
      "Epoch 185: val loss 0.833801\n",
      "\n",
      "Epoch %d: train loss %f 186 0.17847444955259562\n",
      "Epoch 186: val loss 0.864226\n",
      "\n",
      "Epoch %d: train loss %f 187 0.2018715851008892\n",
      "Epoch 187: val loss 0.829340\n",
      "\n",
      "Epoch %d: train loss %f 188 0.1773534631356597\n",
      "Epoch 188: val loss 0.822553\n",
      "\n",
      "Epoch %d: train loss %f 189 0.16758162062615156\n",
      "Epoch 189: val loss 0.819110\n",
      "\n",
      "Epoch %d: train loss %f 190 0.16830961406230927\n",
      "Epoch 190: val loss 0.867805\n",
      "\n",
      "Epoch %d: train loss %f 191 0.1942020608112216\n",
      "Epoch 191: val loss 0.851717\n",
      "\n",
      "Epoch %d: train loss %f 192 0.14700770378112793\n",
      "Epoch 192: val loss 0.844411\n",
      "\n",
      "Epoch %d: train loss %f 193 0.22961144614964724\n",
      "Epoch 193: val loss 0.896755\n",
      "\n",
      "Epoch %d: train loss %f 194 0.1786532010883093\n",
      "Epoch 194: val loss 0.866246\n",
      "\n",
      "Epoch %d: train loss %f 195 0.18723095022141933\n",
      "Epoch 195: val loss 0.844325\n",
      "\n",
      "Epoch %d: train loss %f 196 0.1968038072809577\n",
      "Epoch 196: val loss 0.840036\n",
      "\n",
      "Epoch %d: train loss %f 197 0.15439965948462486\n",
      "Epoch 197: val loss 0.846221\n",
      "\n",
      "Epoch %d: train loss %f 198 0.18748248368501663\n",
      "Epoch 198: val loss 0.870361\n",
      "\n",
      "Epoch %d: train loss %f 199 0.17374092526733875\n",
      "Epoch 199: val loss 0.864897\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6878488212823868\n",
      "Epoch 0: val loss 0.686870\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6793821230530739\n",
      "Epoch 1: val loss 0.682582\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6701508015394211\n",
      "Epoch 2: val loss 0.676395\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6579848751425743\n",
      "Epoch 3: val loss 0.667131\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6440533176064491\n",
      "Epoch 4: val loss 0.652349\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6221107989549637\n",
      "Epoch 5: val loss 0.630621\n",
      "\n",
      "Epoch %d: train loss %f 6 0.5958287045359612\n",
      "Epoch 6: val loss 0.600380\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5588957443833351\n",
      "Epoch 7: val loss 0.564187\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5143493823707104\n",
      "Epoch 8: val loss 0.533011\n",
      "\n",
      "Epoch %d: train loss %f 9 0.4951949529349804\n",
      "Epoch 9: val loss 0.520799\n",
      "\n",
      "Epoch %d: train loss %f 10 0.4895118288695812\n",
      "Epoch 10: val loss 0.513742\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5019446648657322\n",
      "Epoch 11: val loss 0.508747\n",
      "\n",
      "Epoch %d: train loss %f 12 0.4830636903643608\n",
      "Epoch 12: val loss 0.512772\n",
      "\n",
      "Epoch %d: train loss %f 13 0.487357709556818\n",
      "Epoch 13: val loss 0.508629\n",
      "\n",
      "Epoch %d: train loss %f 14 0.47378065809607506\n",
      "Epoch 14: val loss 0.496862\n",
      "\n",
      "Epoch %d: train loss %f 15 0.4642617255449295\n",
      "Epoch 15: val loss 0.510596\n",
      "\n",
      "Epoch %d: train loss %f 16 0.467427346855402\n",
      "Epoch 16: val loss 0.498343\n",
      "\n",
      "Epoch %d: train loss %f 17 0.4486219584941864\n",
      "Epoch 17: val loss 0.500912\n",
      "\n",
      "Epoch %d: train loss %f 18 0.44746487960219383\n",
      "Epoch 18: val loss 0.518136\n",
      "\n",
      "Epoch %d: train loss %f 19 0.41429076343774796\n",
      "Epoch 19: val loss 0.498090\n",
      "\n",
      "Epoch %d: train loss %f 20 0.43003664910793304\n",
      "Epoch 20: val loss 0.504714\n",
      "\n",
      "Epoch %d: train loss %f 21 0.44595127180218697\n",
      "Epoch 21: val loss 0.509545\n",
      "\n",
      "Epoch %d: train loss %f 22 0.4377475567162037\n",
      "Epoch 22: val loss 0.509963\n",
      "\n",
      "Epoch %d: train loss %f 23 0.42763207852840424\n",
      "Epoch 23: val loss 0.520254\n",
      "\n",
      "Epoch %d: train loss %f 24 0.4231051728129387\n",
      "Epoch 24: val loss 0.512726\n",
      "\n",
      "Epoch %d: train loss %f 25 0.41763508319854736\n",
      "Epoch 25: val loss 0.499537\n",
      "\n",
      "Epoch %d: train loss %f 26 0.42199090868234634\n",
      "Epoch 26: val loss 0.505844\n",
      "\n",
      "Epoch %d: train loss %f 27 0.45110630616545677\n",
      "Epoch 27: val loss 0.530441\n",
      "\n",
      "Epoch %d: train loss %f 28 0.41742944344878197\n",
      "Epoch 28: val loss 0.505812\n",
      "\n",
      "Epoch %d: train loss %f 29 0.40814095363020897\n",
      "Epoch 29: val loss 0.497288\n",
      "\n",
      "Epoch %d: train loss %f 30 0.4162193927913904\n",
      "Epoch 30: val loss 0.512903\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4087274745106697\n",
      "Epoch 31: val loss 0.505048\n",
      "\n",
      "Epoch %d: train loss %f 32 0.42108749225735664\n",
      "Epoch 32: val loss 0.511649\n",
      "\n",
      "Epoch %d: train loss %f 33 0.40217307955026627\n",
      "Epoch 33: val loss 0.506759\n",
      "\n",
      "Epoch %d: train loss %f 34 0.40931717306375504\n",
      "Epoch 34: val loss 0.509340\n",
      "\n",
      "Epoch %d: train loss %f 35 0.38128335401415825\n",
      "Epoch 35: val loss 0.513148\n",
      "\n",
      "Epoch %d: train loss %f 36 0.38915421441197395\n",
      "Epoch 36: val loss 0.509575\n",
      "\n",
      "Epoch %d: train loss %f 37 0.3843253683298826\n",
      "Epoch 37: val loss 0.518815\n",
      "\n",
      "Epoch %d: train loss %f 38 0.4174160249531269\n",
      "Epoch 38: val loss 0.517343\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4220489002764225\n",
      "Epoch 39: val loss 0.512736\n",
      "\n",
      "Epoch %d: train loss %f 40 0.3980838656425476\n",
      "Epoch 40: val loss 0.517522\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4096612446010113\n",
      "Epoch 41: val loss 0.510621\n",
      "\n",
      "Epoch %d: train loss %f 42 0.38516981340944767\n",
      "Epoch 42: val loss 0.505558\n",
      "\n",
      "Epoch %d: train loss %f 43 0.3992008436471224\n",
      "Epoch 43: val loss 0.515648\n",
      "\n",
      "Epoch %d: train loss %f 44 0.3932559974491596\n",
      "Epoch 44: val loss 0.508134\n",
      "\n",
      "Epoch %d: train loss %f 45 0.3716720752418041\n",
      "Epoch 45: val loss 0.516250\n",
      "\n",
      "Epoch %d: train loss %f 46 0.3810281828045845\n",
      "Epoch 46: val loss 0.518152\n",
      "\n",
      "Epoch %d: train loss %f 47 0.37321382761001587\n",
      "Epoch 47: val loss 0.525466\n",
      "\n",
      "Epoch %d: train loss %f 48 0.3653675466775894\n",
      "Epoch 48: val loss 0.520765\n",
      "\n",
      "Epoch %d: train loss %f 49 0.3790002092719078\n",
      "Epoch 49: val loss 0.516925\n",
      "\n",
      "Epoch %d: train loss %f 50 0.3829811178147793\n",
      "Epoch 50: val loss 0.518724\n",
      "\n",
      "Epoch %d: train loss %f 51 0.3773966357111931\n",
      "Epoch 51: val loss 0.519286\n",
      "\n",
      "Epoch %d: train loss %f 52 0.3935075141489506\n",
      "Epoch 52: val loss 0.526549\n",
      "\n",
      "Epoch %d: train loss %f 53 0.35789073817431927\n",
      "Epoch 53: val loss 0.534400\n",
      "\n",
      "Epoch %d: train loss %f 54 0.37672531604766846\n",
      "Epoch 54: val loss 0.531545\n",
      "\n",
      "Epoch %d: train loss %f 55 0.34580861032009125\n",
      "Epoch 55: val loss 0.543783\n",
      "\n",
      "Epoch %d: train loss %f 56 0.36861302703619003\n",
      "Epoch 56: val loss 0.527351\n",
      "\n",
      "Epoch %d: train loss %f 57 0.34380152076482773\n",
      "Epoch 57: val loss 0.534183\n",
      "\n",
      "Epoch %d: train loss %f 58 0.3312302529811859\n",
      "Epoch 58: val loss 0.530099\n",
      "\n",
      "Epoch %d: train loss %f 59 0.3677334152162075\n",
      "Epoch 59: val loss 0.544061\n",
      "\n",
      "Epoch %d: train loss %f 60 0.32537681236863136\n",
      "Epoch 60: val loss 0.548866\n",
      "\n",
      "Epoch %d: train loss %f 61 0.37547275610268116\n",
      "Epoch 61: val loss 0.556729\n",
      "\n",
      "Epoch %d: train loss %f 62 0.34305894188582897\n",
      "Epoch 62: val loss 0.556036\n",
      "\n",
      "Epoch %d: train loss %f 63 0.3394956439733505\n",
      "Epoch 63: val loss 0.551094\n",
      "\n",
      "Epoch %d: train loss %f 64 0.3363860212266445\n",
      "Epoch 64: val loss 0.557953\n",
      "\n",
      "Epoch %d: train loss %f 65 0.34922967851161957\n",
      "Epoch 65: val loss 0.546127\n",
      "\n",
      "Epoch %d: train loss %f 66 0.33079403452575207\n",
      "Epoch 66: val loss 0.551837\n",
      "\n",
      "Epoch %d: train loss %f 67 0.34732811711728573\n",
      "Epoch 67: val loss 0.540960\n",
      "\n",
      "Epoch %d: train loss %f 68 0.3445731084793806\n",
      "Epoch 68: val loss 0.559527\n",
      "\n",
      "Epoch %d: train loss %f 69 0.34541610814630985\n",
      "Epoch 69: val loss 0.549480\n",
      "\n",
      "Epoch %d: train loss %f 70 0.35879450663924217\n",
      "Epoch 70: val loss 0.561076\n",
      "\n",
      "Epoch %d: train loss %f 71 0.31562747806310654\n",
      "Epoch 71: val loss 0.574740\n",
      "\n",
      "Epoch %d: train loss %f 72 0.3383076097816229\n",
      "Epoch 72: val loss 0.571471\n",
      "\n",
      "Epoch %d: train loss %f 73 0.31598322838544846\n",
      "Epoch 73: val loss 0.564318\n",
      "\n",
      "Epoch %d: train loss %f 74 0.3226033393293619\n",
      "Epoch 74: val loss 0.564845\n",
      "\n",
      "Epoch %d: train loss %f 75 0.3076648525893688\n",
      "Epoch 75: val loss 0.571757\n",
      "\n",
      "Epoch %d: train loss %f 76 0.31025346368551254\n",
      "Epoch 76: val loss 0.589081\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3551270216703415\n",
      "Epoch 77: val loss 0.586799\n",
      "\n",
      "Epoch %d: train loss %f 78 0.3115407172590494\n",
      "Epoch 78: val loss 0.560928\n",
      "\n",
      "Epoch %d: train loss %f 79 0.3218278046697378\n",
      "Epoch 79: val loss 0.564886\n",
      "\n",
      "Epoch %d: train loss %f 80 0.34297119453549385\n",
      "Epoch 80: val loss 0.590878\n",
      "\n",
      "Epoch %d: train loss %f 81 0.3245881777256727\n",
      "Epoch 81: val loss 0.592483\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3221573084592819\n",
      "Epoch 82: val loss 0.588943\n",
      "\n",
      "Epoch %d: train loss %f 83 0.33978732489049435\n",
      "Epoch 83: val loss 0.605742\n",
      "\n",
      "Epoch %d: train loss %f 84 0.33778434805572033\n",
      "Epoch 84: val loss 0.558720\n",
      "\n",
      "Epoch %d: train loss %f 85 0.34879510290920734\n",
      "Epoch 85: val loss 0.576589\n",
      "\n",
      "Epoch %d: train loss %f 86 0.3258539643138647\n",
      "Epoch 86: val loss 0.564430\n",
      "\n",
      "Epoch %d: train loss %f 87 0.34932165779173374\n",
      "Epoch 87: val loss 0.552781\n",
      "\n",
      "Epoch %d: train loss %f 88 0.3242958262562752\n",
      "Epoch 88: val loss 0.588176\n",
      "\n",
      "Epoch %d: train loss %f 89 0.29896741546690464\n",
      "Epoch 89: val loss 0.626617\n",
      "\n",
      "Epoch %d: train loss %f 90 0.3179035447537899\n",
      "Epoch 90: val loss 0.598483\n",
      "\n",
      "Epoch %d: train loss %f 91 0.29890002124011517\n",
      "Epoch 91: val loss 0.584761\n",
      "\n",
      "Epoch %d: train loss %f 92 0.3569749966263771\n",
      "Epoch 92: val loss 0.576073\n",
      "\n",
      "Epoch %d: train loss %f 93 0.34401784278452396\n",
      "Epoch 93: val loss 0.616210\n",
      "\n",
      "Epoch %d: train loss %f 94 0.3444871474057436\n",
      "Epoch 94: val loss 0.580028\n",
      "\n",
      "Epoch %d: train loss %f 95 0.3002369534224272\n",
      "Epoch 95: val loss 0.593994\n",
      "\n",
      "Epoch %d: train loss %f 96 0.32116012647747993\n",
      "Epoch 96: val loss 0.608283\n",
      "\n",
      "Epoch %d: train loss %f 97 0.31550975702703\n",
      "Epoch 97: val loss 0.594294\n",
      "\n",
      "Epoch %d: train loss %f 98 0.34454905427992344\n",
      "Epoch 98: val loss 0.580830\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3040348384529352\n",
      "Epoch 99: val loss 0.620910\n",
      "\n",
      "Epoch %d: train loss %f 100 0.31637832894921303\n",
      "Epoch 100: val loss 0.571501\n",
      "\n",
      "Epoch %d: train loss %f 101 0.31950072571635246\n",
      "Epoch 101: val loss 0.579110\n",
      "\n",
      "Epoch %d: train loss %f 102 0.32541341707110405\n",
      "Epoch 102: val loss 0.599491\n",
      "\n",
      "Epoch %d: train loss %f 103 0.29647633619606495\n",
      "Epoch 103: val loss 0.592755\n",
      "\n",
      "Epoch %d: train loss %f 104 0.29637826792895794\n",
      "Epoch 104: val loss 0.591722\n",
      "\n",
      "Epoch %d: train loss %f 105 0.32543209474533796\n",
      "Epoch 105: val loss 0.645357\n",
      "\n",
      "Epoch %d: train loss %f 106 0.3456264063715935\n",
      "Epoch 106: val loss 0.602042\n",
      "\n",
      "Epoch %d: train loss %f 107 0.30719323456287384\n",
      "Epoch 107: val loss 0.629064\n",
      "\n",
      "Epoch %d: train loss %f 108 0.29633294977247715\n",
      "Epoch 108: val loss 0.614748\n",
      "\n",
      "Epoch %d: train loss %f 109 0.3113341275602579\n",
      "Epoch 109: val loss 0.646605\n",
      "\n",
      "Epoch %d: train loss %f 110 0.30033063143491745\n",
      "Epoch 110: val loss 0.609157\n",
      "\n",
      "Epoch %d: train loss %f 111 0.28632332757115364\n",
      "Epoch 111: val loss 0.610800\n",
      "\n",
      "Epoch %d: train loss %f 112 0.27928207255899906\n",
      "Epoch 112: val loss 0.622607\n",
      "\n",
      "Epoch %d: train loss %f 113 0.29263686668127775\n",
      "Epoch 113: val loss 0.633201\n",
      "\n",
      "Epoch %d: train loss %f 114 0.30403619073331356\n",
      "Epoch 114: val loss 0.642678\n",
      "\n",
      "Epoch %d: train loss %f 115 0.3198724202811718\n",
      "Epoch 115: val loss 0.642202\n",
      "\n",
      "Epoch %d: train loss %f 116 0.30546582117676735\n",
      "Epoch 116: val loss 0.637906\n",
      "\n",
      "Epoch %d: train loss %f 117 0.2947321906685829\n",
      "Epoch 117: val loss 0.638764\n",
      "\n",
      "Epoch %d: train loss %f 118 0.28699030354619026\n",
      "Epoch 118: val loss 0.613575\n",
      "\n",
      "Epoch %d: train loss %f 119 0.26396393217146397\n",
      "Epoch 119: val loss 0.625801\n",
      "\n",
      "Epoch %d: train loss %f 120 0.32222048565745354\n",
      "Epoch 120: val loss 0.644400\n",
      "\n",
      "Epoch %d: train loss %f 121 0.2941184937953949\n",
      "Epoch 121: val loss 0.631923\n",
      "\n",
      "Epoch %d: train loss %f 122 0.27247145026922226\n",
      "Epoch 122: val loss 0.634505\n",
      "\n",
      "Epoch %d: train loss %f 123 0.2757075373083353\n",
      "Epoch 123: val loss 0.640882\n",
      "\n",
      "Epoch %d: train loss %f 124 0.2792798615992069\n",
      "Epoch 124: val loss 0.645997\n",
      "\n",
      "Epoch %d: train loss %f 125 0.2602576520293951\n",
      "Epoch 125: val loss 0.647345\n",
      "\n",
      "Epoch %d: train loss %f 126 0.26345421746373177\n",
      "Epoch 126: val loss 0.658376\n",
      "\n",
      "Epoch %d: train loss %f 127 0.2816013377159834\n",
      "Epoch 127: val loss 0.628837\n",
      "\n",
      "Epoch %d: train loss %f 128 0.2776738964021206\n",
      "Epoch 128: val loss 0.652739\n",
      "\n",
      "Epoch %d: train loss %f 129 0.2884580008685589\n",
      "Epoch 129: val loss 0.646338\n",
      "\n",
      "Epoch %d: train loss %f 130 0.2547807898372412\n",
      "Epoch 130: val loss 0.653397\n",
      "\n",
      "Epoch %d: train loss %f 131 0.29488653875887394\n",
      "Epoch 131: val loss 0.646908\n",
      "\n",
      "Epoch %d: train loss %f 132 0.28776070661842823\n",
      "Epoch 132: val loss 0.637516\n",
      "\n",
      "Epoch %d: train loss %f 133 0.2412082441151142\n",
      "Epoch 133: val loss 0.638561\n",
      "\n",
      "Epoch %d: train loss %f 134 0.27677147649228573\n",
      "Epoch 134: val loss 0.644633\n",
      "\n",
      "Epoch %d: train loss %f 135 0.2752372119575739\n",
      "Epoch 135: val loss 0.635287\n",
      "\n",
      "Epoch %d: train loss %f 136 0.26851719431579113\n",
      "Epoch 136: val loss 0.655224\n",
      "\n",
      "Epoch %d: train loss %f 137 0.2966962456703186\n",
      "Epoch 137: val loss 0.640293\n",
      "\n",
      "Epoch %d: train loss %f 138 0.27517740428447723\n",
      "Epoch 138: val loss 0.601407\n",
      "\n",
      "Epoch %d: train loss %f 139 0.27388871088624\n",
      "Epoch 139: val loss 0.631843\n",
      "\n",
      "Epoch %d: train loss %f 140 0.25578379817306995\n",
      "Epoch 140: val loss 0.643993\n",
      "\n",
      "Epoch %d: train loss %f 141 0.25617641396820545\n",
      "Epoch 141: val loss 0.653200\n",
      "\n",
      "Epoch %d: train loss %f 142 0.23318536393344402\n",
      "Epoch 142: val loss 0.686647\n",
      "\n",
      "Epoch %d: train loss %f 143 0.2823615651577711\n",
      "Epoch 143: val loss 0.666201\n",
      "\n",
      "Epoch %d: train loss %f 144 0.2873721681535244\n",
      "Epoch 144: val loss 0.652595\n",
      "\n",
      "Epoch %d: train loss %f 145 0.258209727704525\n",
      "Epoch 145: val loss 0.626798\n",
      "\n",
      "Epoch %d: train loss %f 146 0.23662153631448746\n",
      "Epoch 146: val loss 0.651690\n",
      "\n",
      "Epoch %d: train loss %f 147 0.31169547140598297\n",
      "Epoch 147: val loss 0.622697\n",
      "\n",
      "Epoch %d: train loss %f 148 0.24793601408600807\n",
      "Epoch 148: val loss 0.651829\n",
      "\n",
      "Epoch %d: train loss %f 149 0.2579923290759325\n",
      "Epoch 149: val loss 0.647546\n",
      "\n",
      "Epoch %d: train loss %f 150 0.3021226115524769\n",
      "Epoch 150: val loss 0.629242\n",
      "\n",
      "Epoch %d: train loss %f 151 0.24964048899710178\n",
      "Epoch 151: val loss 0.600276\n",
      "\n",
      "Epoch %d: train loss %f 152 0.25462732277810574\n",
      "Epoch 152: val loss 0.617388\n",
      "\n",
      "Epoch %d: train loss %f 153 0.2735777385532856\n",
      "Epoch 153: val loss 0.615863\n",
      "\n",
      "Epoch %d: train loss %f 154 0.25865085422992706\n",
      "Epoch 154: val loss 0.674416\n",
      "\n",
      "Epoch %d: train loss %f 155 0.2503026998601854\n",
      "Epoch 155: val loss 0.668985\n",
      "\n",
      "Epoch %d: train loss %f 156 0.24042485654354095\n",
      "Epoch 156: val loss 0.687808\n",
      "\n",
      "Epoch %d: train loss %f 157 0.25622257124632597\n",
      "Epoch 157: val loss 0.685262\n",
      "\n",
      "Epoch %d: train loss %f 158 0.31019224040210247\n",
      "Epoch 158: val loss 0.684201\n",
      "\n",
      "Epoch %d: train loss %f 159 0.21861296892166138\n",
      "Epoch 159: val loss 0.676821\n",
      "\n",
      "Epoch %d: train loss %f 160 0.19766408577561378\n",
      "Epoch 160: val loss 0.671995\n",
      "\n",
      "Epoch %d: train loss %f 161 0.26408117450773716\n",
      "Epoch 161: val loss 0.700045\n",
      "\n",
      "Epoch %d: train loss %f 162 0.2087075961753726\n",
      "Epoch 162: val loss 0.677647\n",
      "\n",
      "Epoch %d: train loss %f 163 0.2586122788488865\n",
      "Epoch 163: val loss 0.696947\n",
      "\n",
      "Epoch %d: train loss %f 164 0.28360177390277386\n",
      "Epoch 164: val loss 0.704432\n",
      "\n",
      "Epoch %d: train loss %f 165 0.23884625732898712\n",
      "Epoch 165: val loss 0.680342\n",
      "\n",
      "Epoch %d: train loss %f 166 0.26322588697075844\n",
      "Epoch 166: val loss 0.696950\n",
      "\n",
      "Epoch %d: train loss %f 167 0.2491769650951028\n",
      "Epoch 167: val loss 0.698012\n",
      "\n",
      "Epoch %d: train loss %f 168 0.24464094266295433\n",
      "Epoch 168: val loss 0.710733\n",
      "\n",
      "Epoch %d: train loss %f 169 0.25086808018386364\n",
      "Epoch 169: val loss 0.686734\n",
      "\n",
      "Epoch %d: train loss %f 170 0.2333222571760416\n",
      "Epoch 170: val loss 0.679546\n",
      "\n",
      "Epoch %d: train loss %f 171 0.2552709123119712\n",
      "Epoch 171: val loss 0.678568\n",
      "\n",
      "Epoch %d: train loss %f 172 0.2120989728718996\n",
      "Epoch 172: val loss 0.664578\n",
      "\n",
      "Epoch %d: train loss %f 173 0.22531462740153074\n",
      "Epoch 173: val loss 0.655115\n",
      "\n",
      "Epoch %d: train loss %f 174 0.2558191418647766\n",
      "Epoch 174: val loss 0.650101\n",
      "\n",
      "Epoch %d: train loss %f 175 0.23196073435246944\n",
      "Epoch 175: val loss 0.646416\n",
      "\n",
      "Epoch %d: train loss %f 176 0.2696436047554016\n",
      "Epoch 176: val loss 0.666906\n",
      "\n",
      "Epoch %d: train loss %f 177 0.20178308710455894\n",
      "Epoch 177: val loss 0.672655\n",
      "\n",
      "Epoch %d: train loss %f 178 0.22645247261971235\n",
      "Epoch 178: val loss 0.696127\n",
      "\n",
      "Epoch %d: train loss %f 179 0.215984009206295\n",
      "Epoch 179: val loss 0.700572\n",
      "\n",
      "Epoch %d: train loss %f 180 0.230962248519063\n",
      "Epoch 180: val loss 0.702219\n",
      "\n",
      "Epoch %d: train loss %f 181 0.22503916919231415\n",
      "Epoch 181: val loss 0.727617\n",
      "\n",
      "Epoch %d: train loss %f 182 0.2902455357834697\n",
      "Epoch 182: val loss 0.710725\n",
      "\n",
      "Epoch %d: train loss %f 183 0.23972441628575325\n",
      "Epoch 183: val loss 0.727418\n",
      "\n",
      "Epoch %d: train loss %f 184 0.26460007205605507\n",
      "Epoch 184: val loss 0.686445\n",
      "\n",
      "Epoch %d: train loss %f 185 0.20674855448305607\n",
      "Epoch 185: val loss 0.641521\n",
      "\n",
      "Epoch %d: train loss %f 186 0.2906567845493555\n",
      "Epoch 186: val loss 0.653154\n",
      "\n",
      "Epoch %d: train loss %f 187 0.21691188029944897\n",
      "Epoch 187: val loss 0.679153\n",
      "\n",
      "Epoch %d: train loss %f 188 0.1927162753418088\n",
      "Epoch 188: val loss 0.702038\n",
      "\n",
      "Epoch %d: train loss %f 189 0.2643014919012785\n",
      "Epoch 189: val loss 0.712707\n",
      "\n",
      "Epoch %d: train loss %f 190 0.2429860271513462\n",
      "Epoch 190: val loss 0.704160\n",
      "\n",
      "Epoch %d: train loss %f 191 0.19874844513833523\n",
      "Epoch 191: val loss 0.654213\n",
      "\n",
      "Epoch %d: train loss %f 192 0.2405410371720791\n",
      "Epoch 192: val loss 0.673350\n",
      "\n",
      "Epoch %d: train loss %f 193 0.2193301310762763\n",
      "Epoch 193: val loss 0.691074\n",
      "\n",
      "Epoch %d: train loss %f 194 0.2431544428691268\n",
      "Epoch 194: val loss 0.725248\n",
      "\n",
      "Epoch %d: train loss %f 195 0.2082640491425991\n",
      "Epoch 195: val loss 0.716409\n",
      "\n",
      "Epoch %d: train loss %f 196 0.20146599039435387\n",
      "Epoch 196: val loss 0.712221\n",
      "\n",
      "Epoch %d: train loss %f 197 0.21287230122834444\n",
      "Epoch 197: val loss 0.727711\n",
      "\n",
      "Epoch %d: train loss %f 198 0.22111632954329252\n",
      "Epoch 198: val loss 0.705998\n",
      "\n",
      "Epoch %d: train loss %f 199 0.20403707679361105\n",
      "Epoch 199: val loss 0.691094\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6875513941049576\n",
      "Epoch 0: val loss 0.686951\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6801629140973091\n",
      "Epoch 1: val loss 0.683094\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6728290393948555\n",
      "Epoch 2: val loss 0.677829\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6627674624323845\n",
      "Epoch 3: val loss 0.669489\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6501983627676964\n",
      "Epoch 4: val loss 0.656263\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6297978162765503\n",
      "Epoch 5: val loss 0.636630\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6079868525266647\n",
      "Epoch 6: val loss 0.606437\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5725730210542679\n",
      "Epoch 7: val loss 0.572034\n",
      "\n",
      "Epoch %d: train loss %f 8 0.548181377351284\n",
      "Epoch 8: val loss 0.539147\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5066175274550915\n",
      "Epoch 9: val loss 0.517206\n",
      "\n",
      "Epoch %d: train loss %f 10 0.49795109778642654\n",
      "Epoch 10: val loss 0.509344\n",
      "\n",
      "Epoch %d: train loss %f 11 0.4889807514846325\n",
      "Epoch 11: val loss 0.511506\n",
      "\n",
      "Epoch %d: train loss %f 12 0.4824846237897873\n",
      "Epoch 12: val loss 0.515530\n",
      "\n",
      "Epoch %d: train loss %f 13 0.4610382691025734\n",
      "Epoch 13: val loss 0.517196\n",
      "\n",
      "Epoch %d: train loss %f 14 0.47463110089302063\n",
      "Epoch 14: val loss 0.518309\n",
      "\n",
      "Epoch %d: train loss %f 15 0.4710262194275856\n",
      "Epoch 15: val loss 0.518373\n",
      "\n",
      "Epoch %d: train loss %f 16 0.45140906795859337\n",
      "Epoch 16: val loss 0.519582\n",
      "\n",
      "Epoch %d: train loss %f 17 0.45416469126939774\n",
      "Epoch 17: val loss 0.519490\n",
      "\n",
      "Epoch %d: train loss %f 18 0.44127144664525986\n",
      "Epoch 18: val loss 0.524320\n",
      "\n",
      "Epoch %d: train loss %f 19 0.4531540013849735\n",
      "Epoch 19: val loss 0.522527\n",
      "\n",
      "Epoch %d: train loss %f 20 0.45154401287436485\n",
      "Epoch 20: val loss 0.524684\n",
      "\n",
      "Epoch %d: train loss %f 21 0.44129323586821556\n",
      "Epoch 21: val loss 0.512337\n",
      "\n",
      "Epoch %d: train loss %f 22 0.4235728532075882\n",
      "Epoch 22: val loss 0.521986\n",
      "\n",
      "Epoch %d: train loss %f 23 0.4110543243587017\n",
      "Epoch 23: val loss 0.520924\n",
      "\n",
      "Epoch %d: train loss %f 24 0.43240271136164665\n",
      "Epoch 24: val loss 0.535189\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4189855195581913\n",
      "Epoch 25: val loss 0.528609\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4084387719631195\n",
      "Epoch 26: val loss 0.525018\n",
      "\n",
      "Epoch %d: train loss %f 27 0.4318503253161907\n",
      "Epoch 27: val loss 0.539361\n",
      "\n",
      "Epoch %d: train loss %f 28 0.41116050258278847\n",
      "Epoch 28: val loss 0.538801\n",
      "\n",
      "Epoch %d: train loss %f 29 0.41716983541846275\n",
      "Epoch 29: val loss 0.529571\n",
      "\n",
      "Epoch %d: train loss %f 30 0.3876471035182476\n",
      "Epoch 30: val loss 0.540878\n",
      "\n",
      "Epoch %d: train loss %f 31 0.40730680897831917\n",
      "Epoch 31: val loss 0.545035\n",
      "\n",
      "Epoch %d: train loss %f 32 0.40685904771089554\n",
      "Epoch 32: val loss 0.554837\n",
      "\n",
      "Epoch %d: train loss %f 33 0.38322900980710983\n",
      "Epoch 33: val loss 0.542434\n",
      "\n",
      "Epoch %d: train loss %f 34 0.38087355718016624\n",
      "Epoch 34: val loss 0.552518\n",
      "\n",
      "Epoch %d: train loss %f 35 0.3940601870417595\n",
      "Epoch 35: val loss 0.545556\n",
      "\n",
      "Epoch %d: train loss %f 36 0.40552080050110817\n",
      "Epoch 36: val loss 0.550713\n",
      "\n",
      "Epoch %d: train loss %f 37 0.39513910189270973\n",
      "Epoch 37: val loss 0.563400\n",
      "\n",
      "Epoch %d: train loss %f 38 0.40763466246426105\n",
      "Epoch 38: val loss 0.539786\n",
      "\n",
      "Epoch %d: train loss %f 39 0.40346162021160126\n",
      "Epoch 39: val loss 0.553394\n",
      "\n",
      "Epoch %d: train loss %f 40 0.3627099357545376\n",
      "Epoch 40: val loss 0.547782\n",
      "\n",
      "Epoch %d: train loss %f 41 0.38747390918433666\n",
      "Epoch 41: val loss 0.540353\n",
      "\n",
      "Epoch %d: train loss %f 42 0.3849044628441334\n",
      "Epoch 42: val loss 0.548696\n",
      "\n",
      "Epoch %d: train loss %f 43 0.3752388469874859\n",
      "Epoch 43: val loss 0.551098\n",
      "\n",
      "Epoch %d: train loss %f 44 0.3896276168525219\n",
      "Epoch 44: val loss 0.548082\n",
      "\n",
      "Epoch %d: train loss %f 45 0.35678560100495815\n",
      "Epoch 45: val loss 0.556032\n",
      "\n",
      "Epoch %d: train loss %f 46 0.3801344595849514\n",
      "Epoch 46: val loss 0.557709\n",
      "\n",
      "Epoch %d: train loss %f 47 0.3608634024858475\n",
      "Epoch 47: val loss 0.564022\n",
      "\n",
      "Epoch %d: train loss %f 48 0.36267150565981865\n",
      "Epoch 48: val loss 0.575690\n",
      "\n",
      "Epoch %d: train loss %f 49 0.38085038028657436\n",
      "Epoch 49: val loss 0.595490\n",
      "\n",
      "Epoch %d: train loss %f 50 0.351797292008996\n",
      "Epoch 50: val loss 0.586319\n",
      "\n",
      "Epoch %d: train loss %f 51 0.3457412160933018\n",
      "Epoch 51: val loss 0.587945\n",
      "\n",
      "Epoch %d: train loss %f 52 0.3525969833135605\n",
      "Epoch 52: val loss 0.595357\n",
      "\n",
      "Epoch %d: train loss %f 53 0.3495032414793968\n",
      "Epoch 53: val loss 0.593295\n",
      "\n",
      "Epoch %d: train loss %f 54 0.3486965373158455\n",
      "Epoch 54: val loss 0.594897\n",
      "\n",
      "Epoch %d: train loss %f 55 0.3491430450230837\n",
      "Epoch 55: val loss 0.602440\n",
      "\n",
      "Epoch %d: train loss %f 56 0.3602936528623104\n",
      "Epoch 56: val loss 0.625678\n",
      "\n",
      "Epoch %d: train loss %f 57 0.34204985946416855\n",
      "Epoch 57: val loss 0.628955\n",
      "\n",
      "Epoch %d: train loss %f 58 0.3426318597048521\n",
      "Epoch 58: val loss 0.649459\n",
      "\n",
      "Epoch %d: train loss %f 59 0.3198244795203209\n",
      "Epoch 59: val loss 0.642699\n",
      "\n",
      "Epoch %d: train loss %f 60 0.3317914828658104\n",
      "Epoch 60: val loss 0.635827\n",
      "\n",
      "Epoch %d: train loss %f 61 0.31795219145715237\n",
      "Epoch 61: val loss 0.632314\n",
      "\n",
      "Epoch %d: train loss %f 62 0.3354901559650898\n",
      "Epoch 62: val loss 0.644000\n",
      "\n",
      "Epoch %d: train loss %f 63 0.3458741046488285\n",
      "Epoch 63: val loss 0.635345\n",
      "\n",
      "Epoch %d: train loss %f 64 0.3542478308081627\n",
      "Epoch 64: val loss 0.643860\n",
      "\n",
      "Epoch %d: train loss %f 65 0.3207577168941498\n",
      "Epoch 65: val loss 0.644908\n",
      "\n",
      "Epoch %d: train loss %f 66 0.32073547318577766\n",
      "Epoch 66: val loss 0.652739\n",
      "\n",
      "Epoch %d: train loss %f 67 0.330122297629714\n",
      "Epoch 67: val loss 0.642172\n",
      "\n",
      "Epoch %d: train loss %f 68 0.359699010848999\n",
      "Epoch 68: val loss 0.672370\n",
      "\n",
      "Epoch %d: train loss %f 69 0.3309763465076685\n",
      "Epoch 69: val loss 0.672355\n",
      "\n",
      "Epoch %d: train loss %f 70 0.34318591468036175\n",
      "Epoch 70: val loss 0.668626\n",
      "\n",
      "Epoch %d: train loss %f 71 0.32236379012465477\n",
      "Epoch 71: val loss 0.678469\n",
      "\n",
      "Epoch %d: train loss %f 72 0.3190726935863495\n",
      "Epoch 72: val loss 0.683861\n",
      "\n",
      "Epoch %d: train loss %f 73 0.33579685911536217\n",
      "Epoch 73: val loss 0.674659\n",
      "\n",
      "Epoch %d: train loss %f 74 0.32509165816009045\n",
      "Epoch 74: val loss 0.648695\n",
      "\n",
      "Epoch %d: train loss %f 75 0.2873958982527256\n",
      "Epoch 75: val loss 0.660217\n",
      "\n",
      "Epoch %d: train loss %f 76 0.3196798153221607\n",
      "Epoch 76: val loss 0.655774\n",
      "\n",
      "Epoch %d: train loss %f 77 0.348684661090374\n",
      "Epoch 77: val loss 0.686938\n",
      "\n",
      "Epoch %d: train loss %f 78 0.3497032932937145\n",
      "Epoch 78: val loss 0.674124\n",
      "\n",
      "Epoch %d: train loss %f 79 0.31188750080764294\n",
      "Epoch 79: val loss 0.669228\n",
      "\n",
      "Epoch %d: train loss %f 80 0.3037281930446625\n",
      "Epoch 80: val loss 0.684335\n",
      "\n",
      "Epoch %d: train loss %f 81 0.2982786763459444\n",
      "Epoch 81: val loss 0.685676\n",
      "\n",
      "Epoch %d: train loss %f 82 0.29883684776723385\n",
      "Epoch 82: val loss 0.685163\n",
      "\n",
      "Epoch %d: train loss %f 83 0.33355628326535225\n",
      "Epoch 83: val loss 0.672260\n",
      "\n",
      "Epoch %d: train loss %f 84 0.3103558886796236\n",
      "Epoch 84: val loss 0.675994\n",
      "\n",
      "Epoch %d: train loss %f 85 0.30370620638132095\n",
      "Epoch 85: val loss 0.668566\n",
      "\n",
      "Epoch %d: train loss %f 86 0.3092157728970051\n",
      "Epoch 86: val loss 0.698174\n",
      "\n",
      "Epoch %d: train loss %f 87 0.2774574998766184\n",
      "Epoch 87: val loss 0.675937\n",
      "\n",
      "Epoch %d: train loss %f 88 0.2721167206764221\n",
      "Epoch 88: val loss 0.675638\n",
      "\n",
      "Epoch %d: train loss %f 89 0.31346908770501614\n",
      "Epoch 89: val loss 0.691709\n",
      "\n",
      "Epoch %d: train loss %f 90 0.3054763451218605\n",
      "Epoch 90: val loss 0.715459\n",
      "\n",
      "Epoch %d: train loss %f 91 0.3099848795682192\n",
      "Epoch 91: val loss 0.732706\n",
      "\n",
      "Epoch %d: train loss %f 92 0.30772883258759975\n",
      "Epoch 92: val loss 0.747722\n",
      "\n",
      "Epoch %d: train loss %f 93 0.2985652405768633\n",
      "Epoch 93: val loss 0.753487\n",
      "\n",
      "Epoch %d: train loss %f 94 0.2672380842268467\n",
      "Epoch 94: val loss 0.748766\n",
      "\n",
      "Epoch %d: train loss %f 95 0.2687630131840706\n",
      "Epoch 95: val loss 0.729916\n",
      "\n",
      "Epoch %d: train loss %f 96 0.3081430308520794\n",
      "Epoch 96: val loss 0.724537\n",
      "\n",
      "Epoch %d: train loss %f 97 0.3094143643975258\n",
      "Epoch 97: val loss 0.747006\n",
      "\n",
      "Epoch %d: train loss %f 98 0.33031863905489445\n",
      "Epoch 98: val loss 0.771040\n",
      "\n",
      "Epoch %d: train loss %f 99 0.28468671441078186\n",
      "Epoch 99: val loss 0.718233\n",
      "\n",
      "Epoch %d: train loss %f 100 0.3192084077745676\n",
      "Epoch 100: val loss 0.711286\n",
      "\n",
      "Epoch %d: train loss %f 101 0.31281618028879166\n",
      "Epoch 101: val loss 0.700252\n",
      "\n",
      "Epoch %d: train loss %f 102 0.2740874383598566\n",
      "Epoch 102: val loss 0.693903\n",
      "\n",
      "Epoch %d: train loss %f 103 0.23991948552429676\n",
      "Epoch 103: val loss 0.702957\n",
      "\n",
      "Epoch %d: train loss %f 104 0.2827468831092119\n",
      "Epoch 104: val loss 0.704512\n",
      "\n",
      "Epoch %d: train loss %f 105 0.32638002932071686\n",
      "Epoch 105: val loss 0.716944\n",
      "\n",
      "Epoch %d: train loss %f 106 0.2766126627102494\n",
      "Epoch 106: val loss 0.757772\n",
      "\n",
      "Epoch %d: train loss %f 107 0.32969151623547077\n",
      "Epoch 107: val loss 0.721192\n",
      "\n",
      "Epoch %d: train loss %f 108 0.24954340234398842\n",
      "Epoch 108: val loss 0.722749\n",
      "\n",
      "Epoch %d: train loss %f 109 0.27025145292282104\n",
      "Epoch 109: val loss 0.719027\n",
      "\n",
      "Epoch %d: train loss %f 110 0.27097238786518574\n",
      "Epoch 110: val loss 0.736001\n",
      "\n",
      "Epoch %d: train loss %f 111 0.2528503444045782\n",
      "Epoch 111: val loss 0.774640\n",
      "\n",
      "Epoch %d: train loss %f 112 0.29923633206635714\n",
      "Epoch 112: val loss 0.797814\n",
      "\n",
      "Epoch %d: train loss %f 113 0.2632606215775013\n",
      "Epoch 113: val loss 0.789694\n",
      "\n",
      "Epoch %d: train loss %f 114 0.2712985407561064\n",
      "Epoch 114: val loss 0.789900\n",
      "\n",
      "Epoch %d: train loss %f 115 0.2717675995081663\n",
      "Epoch 115: val loss 0.779853\n",
      "\n",
      "Epoch %d: train loss %f 116 0.2707956377416849\n",
      "Epoch 116: val loss 0.746345\n",
      "\n",
      "Epoch %d: train loss %f 117 0.2754808384925127\n",
      "Epoch 117: val loss 0.755196\n",
      "\n",
      "Epoch %d: train loss %f 118 0.2656052391976118\n",
      "Epoch 118: val loss 0.767469\n",
      "\n",
      "Epoch %d: train loss %f 119 0.24652943294495344\n",
      "Epoch 119: val loss 0.768535\n",
      "\n",
      "Epoch %d: train loss %f 120 0.28334392979741096\n",
      "Epoch 120: val loss 0.772515\n",
      "\n",
      "Epoch %d: train loss %f 121 0.25741284154355526\n",
      "Epoch 121: val loss 0.774522\n",
      "\n",
      "Epoch %d: train loss %f 122 0.2918734271079302\n",
      "Epoch 122: val loss 0.765126\n",
      "\n",
      "Epoch %d: train loss %f 123 0.26859728060662746\n",
      "Epoch 123: val loss 0.762147\n",
      "\n",
      "Epoch %d: train loss %f 124 0.27674541994929314\n",
      "Epoch 124: val loss 0.795606\n",
      "\n",
      "Epoch %d: train loss %f 125 0.2664544377475977\n",
      "Epoch 125: val loss 0.744085\n",
      "\n",
      "Epoch %d: train loss %f 126 0.27374412678182125\n",
      "Epoch 126: val loss 0.736187\n",
      "\n",
      "Epoch %d: train loss %f 127 0.3076903261244297\n",
      "Epoch 127: val loss 0.739988\n",
      "\n",
      "Epoch %d: train loss %f 128 0.28469221107661724\n",
      "Epoch 128: val loss 0.768632\n",
      "\n",
      "Epoch %d: train loss %f 129 0.2824612073600292\n",
      "Epoch 129: val loss 0.740189\n",
      "\n",
      "Epoch %d: train loss %f 130 0.2911683488637209\n",
      "Epoch 130: val loss 0.750185\n",
      "\n",
      "Epoch %d: train loss %f 131 0.2845199555158615\n",
      "Epoch 131: val loss 0.764771\n",
      "\n",
      "Epoch %d: train loss %f 132 0.2520928215235472\n",
      "Epoch 132: val loss 0.779140\n",
      "\n",
      "Epoch %d: train loss %f 133 0.24132508039474487\n",
      "Epoch 133: val loss 0.801700\n",
      "\n",
      "Epoch %d: train loss %f 134 0.2499354388564825\n",
      "Epoch 134: val loss 0.811563\n",
      "\n",
      "Epoch %d: train loss %f 135 0.2565103117376566\n",
      "Epoch 135: val loss 0.838935\n",
      "\n",
      "Epoch %d: train loss %f 136 0.24709821864962578\n",
      "Epoch 136: val loss 0.827093\n",
      "\n",
      "Epoch %d: train loss %f 137 0.25555599853396416\n",
      "Epoch 137: val loss 0.807401\n",
      "\n",
      "Epoch %d: train loss %f 138 0.2684905035421252\n",
      "Epoch 138: val loss 0.799177\n",
      "\n",
      "Epoch %d: train loss %f 139 0.2451348602771759\n",
      "Epoch 139: val loss 0.803502\n",
      "\n",
      "Epoch %d: train loss %f 140 0.25942888483405113\n",
      "Epoch 140: val loss 0.773372\n",
      "\n",
      "Epoch %d: train loss %f 141 0.2273175362497568\n",
      "Epoch 141: val loss 0.778310\n",
      "\n",
      "Epoch %d: train loss %f 142 0.2478623278439045\n",
      "Epoch 142: val loss 0.779523\n",
      "\n",
      "Epoch %d: train loss %f 143 0.23167351819574833\n",
      "Epoch 143: val loss 0.790877\n",
      "\n",
      "Epoch %d: train loss %f 144 0.267965417355299\n",
      "Epoch 144: val loss 0.820514\n",
      "\n",
      "Epoch %d: train loss %f 145 0.25327196530997753\n",
      "Epoch 145: val loss 0.798977\n",
      "\n",
      "Epoch %d: train loss %f 146 0.25413320399820805\n",
      "Epoch 146: val loss 0.839040\n",
      "\n",
      "Epoch %d: train loss %f 147 0.3115858696401119\n",
      "Epoch 147: val loss 0.792583\n",
      "\n",
      "Epoch %d: train loss %f 148 0.2489672675728798\n",
      "Epoch 148: val loss 0.752690\n",
      "\n",
      "Epoch %d: train loss %f 149 0.28588175773620605\n",
      "Epoch 149: val loss 0.754936\n",
      "\n",
      "Epoch %d: train loss %f 150 0.22539410553872585\n",
      "Epoch 150: val loss 0.783550\n",
      "\n",
      "Epoch %d: train loss %f 151 0.2576961275190115\n",
      "Epoch 151: val loss 0.818718\n",
      "\n",
      "Epoch %d: train loss %f 152 0.23962665908038616\n",
      "Epoch 152: val loss 0.791848\n",
      "\n",
      "Epoch %d: train loss %f 153 0.22984484024345875\n",
      "Epoch 153: val loss 0.786066\n",
      "\n",
      "Epoch %d: train loss %f 154 0.21605844236910343\n",
      "Epoch 154: val loss 0.808483\n",
      "\n",
      "Epoch %d: train loss %f 155 0.2515480685979128\n",
      "Epoch 155: val loss 0.802601\n",
      "\n",
      "Epoch %d: train loss %f 156 0.23801933880895376\n",
      "Epoch 156: val loss 0.802235\n",
      "\n",
      "Epoch %d: train loss %f 157 0.23952106572687626\n",
      "Epoch 157: val loss 0.810933\n",
      "\n",
      "Epoch %d: train loss %f 158 0.28699706122279167\n",
      "Epoch 158: val loss 0.828034\n",
      "\n",
      "Epoch %d: train loss %f 159 0.23191700503230095\n",
      "Epoch 159: val loss 0.802425\n",
      "\n",
      "Epoch %d: train loss %f 160 0.23589329794049263\n",
      "Epoch 160: val loss 0.813937\n",
      "\n",
      "Epoch %d: train loss %f 161 0.24305187352001667\n",
      "Epoch 161: val loss 0.787643\n",
      "\n",
      "Epoch %d: train loss %f 162 0.22182434797286987\n",
      "Epoch 162: val loss 0.777594\n",
      "\n",
      "Epoch %d: train loss %f 163 0.24363709054887295\n",
      "Epoch 163: val loss 0.781423\n",
      "\n",
      "Epoch %d: train loss %f 164 0.23508870042860508\n",
      "Epoch 164: val loss 0.776331\n",
      "\n",
      "Epoch %d: train loss %f 165 0.23384528048336506\n",
      "Epoch 165: val loss 0.768202\n",
      "\n",
      "Epoch %d: train loss %f 166 0.20679827127605677\n",
      "Epoch 166: val loss 0.779121\n",
      "\n",
      "Epoch %d: train loss %f 167 0.24772781878709793\n",
      "Epoch 167: val loss 0.780184\n",
      "\n",
      "Epoch %d: train loss %f 168 0.24250957928597927\n",
      "Epoch 168: val loss 0.826255\n",
      "\n",
      "Epoch %d: train loss %f 169 0.20691622328013182\n",
      "Epoch 169: val loss 0.808536\n",
      "\n",
      "Epoch %d: train loss %f 170 0.19421583134680986\n",
      "Epoch 170: val loss 0.778347\n",
      "\n",
      "Epoch %d: train loss %f 171 0.19069621060043573\n",
      "Epoch 171: val loss 0.801750\n",
      "\n",
      "Epoch %d: train loss %f 172 0.21177714318037033\n",
      "Epoch 172: val loss 0.842862\n",
      "\n",
      "Epoch %d: train loss %f 173 0.23290409334003925\n",
      "Epoch 173: val loss 0.840115\n",
      "\n",
      "Epoch %d: train loss %f 174 0.21371665224432945\n",
      "Epoch 174: val loss 0.896896\n",
      "\n",
      "Epoch %d: train loss %f 175 0.1997022731229663\n",
      "Epoch 175: val loss 0.904504\n",
      "\n",
      "Epoch %d: train loss %f 176 0.2271125903353095\n",
      "Epoch 176: val loss 0.897430\n",
      "\n",
      "Epoch %d: train loss %f 177 0.2310349214822054\n",
      "Epoch 177: val loss 0.841782\n",
      "\n",
      "Epoch %d: train loss %f 178 0.2164408452808857\n",
      "Epoch 178: val loss 0.816400\n",
      "\n",
      "Epoch %d: train loss %f 179 0.2244602832943201\n",
      "Epoch 179: val loss 0.857143\n",
      "\n",
      "Epoch %d: train loss %f 180 0.19656921923160553\n",
      "Epoch 180: val loss 0.868280\n",
      "\n",
      "Epoch %d: train loss %f 181 0.23291068989783525\n",
      "Epoch 181: val loss 0.863739\n",
      "\n",
      "Epoch %d: train loss %f 182 0.23169149924069643\n",
      "Epoch 182: val loss 0.864671\n",
      "\n",
      "Epoch %d: train loss %f 183 0.2886611744761467\n",
      "Epoch 183: val loss 0.887994\n",
      "\n",
      "Epoch %d: train loss %f 184 0.26810023933649063\n",
      "Epoch 184: val loss 0.847622\n",
      "\n",
      "Epoch %d: train loss %f 185 0.2006429387256503\n",
      "Epoch 185: val loss 0.849121\n",
      "\n",
      "Epoch %d: train loss %f 186 0.23905727826058865\n",
      "Epoch 186: val loss 0.851905\n",
      "\n",
      "Epoch %d: train loss %f 187 0.20263990107923746\n",
      "Epoch 187: val loss 0.880674\n",
      "\n",
      "Epoch %d: train loss %f 188 0.19535430241376162\n",
      "Epoch 188: val loss 0.862615\n",
      "\n",
      "Epoch %d: train loss %f 189 0.19559320248663425\n",
      "Epoch 189: val loss 0.852183\n",
      "\n",
      "Epoch %d: train loss %f 190 0.23450404033064842\n",
      "Epoch 190: val loss 0.859196\n",
      "\n",
      "Epoch %d: train loss %f 191 0.23149553686380386\n",
      "Epoch 191: val loss 0.845964\n",
      "\n",
      "Epoch %d: train loss %f 192 0.21636235434561968\n",
      "Epoch 192: val loss 0.848428\n",
      "\n",
      "Epoch %d: train loss %f 193 0.19199844636023045\n",
      "Epoch 193: val loss 0.884784\n",
      "\n",
      "Epoch %d: train loss %f 194 0.2286149924620986\n",
      "Epoch 194: val loss 0.877200\n",
      "\n",
      "Epoch %d: train loss %f 195 0.2116448599845171\n",
      "Epoch 195: val loss 0.846049\n",
      "\n",
      "Epoch %d: train loss %f 196 0.2449600901454687\n",
      "Epoch 196: val loss 0.847291\n",
      "\n",
      "Epoch %d: train loss %f 197 0.19595480617135763\n",
      "Epoch 197: val loss 0.870745\n",
      "\n",
      "Epoch %d: train loss %f 198 0.2423619907349348\n",
      "Epoch 198: val loss 0.888008\n",
      "\n",
      "Epoch %d: train loss %f 199 0.18689955538138747\n",
      "Epoch 199: val loss 0.883326\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6945386528968811\n",
      "Epoch 0: val loss 0.694019\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6879601702094078\n",
      "Epoch 1: val loss 0.689894\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6776299700140953\n",
      "Epoch 2: val loss 0.684339\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6638450026512146\n",
      "Epoch 3: val loss 0.675060\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6458704099059105\n",
      "Epoch 4: val loss 0.659325\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6245394721627235\n",
      "Epoch 5: val loss 0.631582\n",
      "\n",
      "Epoch %d: train loss %f 6 0.5928639471530914\n",
      "Epoch 6: val loss 0.597189\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5603880807757378\n",
      "Epoch 7: val loss 0.548957\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5275030173361301\n",
      "Epoch 8: val loss 0.535873\n",
      "\n",
      "Epoch %d: train loss %f 9 0.48501623049378395\n",
      "Epoch 9: val loss 0.517991\n",
      "\n",
      "Epoch %d: train loss %f 10 0.4936337135732174\n",
      "Epoch 10: val loss 0.505984\n",
      "\n",
      "Epoch %d: train loss %f 11 0.48830458149313927\n",
      "Epoch 11: val loss 0.511176\n",
      "\n",
      "Epoch %d: train loss %f 12 0.4689119830727577\n",
      "Epoch 12: val loss 0.514138\n",
      "\n",
      "Epoch %d: train loss %f 13 0.48498719185590744\n",
      "Epoch 13: val loss 0.506966\n",
      "\n",
      "Epoch %d: train loss %f 14 0.4660314731299877\n",
      "Epoch 14: val loss 0.513815\n",
      "\n",
      "Epoch %d: train loss %f 15 0.4493271969258785\n",
      "Epoch 15: val loss 0.510212\n",
      "\n",
      "Epoch %d: train loss %f 16 0.44039586931467056\n",
      "Epoch 16: val loss 0.512367\n",
      "\n",
      "Epoch %d: train loss %f 17 0.45664289966225624\n",
      "Epoch 17: val loss 0.524517\n",
      "\n",
      "Epoch %d: train loss %f 18 0.44757136330008507\n",
      "Epoch 18: val loss 0.523768\n",
      "\n",
      "Epoch %d: train loss %f 19 0.4427100457251072\n",
      "Epoch 19: val loss 0.521408\n",
      "\n",
      "Epoch %d: train loss %f 20 0.44298550114035606\n",
      "Epoch 20: val loss 0.519466\n",
      "\n",
      "Epoch %d: train loss %f 21 0.42494332790374756\n",
      "Epoch 21: val loss 0.524592\n",
      "\n",
      "Epoch %d: train loss %f 22 0.43565427139401436\n",
      "Epoch 22: val loss 0.534792\n",
      "\n",
      "Epoch %d: train loss %f 23 0.44188184663653374\n",
      "Epoch 23: val loss 0.530391\n",
      "\n",
      "Epoch %d: train loss %f 24 0.4416794367134571\n",
      "Epoch 24: val loss 0.519099\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4343773014843464\n",
      "Epoch 25: val loss 0.529607\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4389847218990326\n",
      "Epoch 26: val loss 0.541700\n",
      "\n",
      "Epoch %d: train loss %f 27 0.40695853531360626\n",
      "Epoch 27: val loss 0.534429\n",
      "\n",
      "Epoch %d: train loss %f 28 0.39853212982416153\n",
      "Epoch 28: val loss 0.543122\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4101167917251587\n",
      "Epoch 29: val loss 0.530251\n",
      "\n",
      "Epoch %d: train loss %f 30 0.42816948145627975\n",
      "Epoch 30: val loss 0.555033\n",
      "\n",
      "Epoch %d: train loss %f 31 0.41001045517623425\n",
      "Epoch 31: val loss 0.549955\n",
      "\n",
      "Epoch %d: train loss %f 32 0.43423550948500633\n",
      "Epoch 32: val loss 0.540306\n",
      "\n",
      "Epoch %d: train loss %f 33 0.4062724709510803\n",
      "Epoch 33: val loss 0.550975\n",
      "\n",
      "Epoch %d: train loss %f 34 0.40318047255277634\n",
      "Epoch 34: val loss 0.565010\n",
      "\n",
      "Epoch %d: train loss %f 35 0.41173162683844566\n",
      "Epoch 35: val loss 0.574611\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4037964195013046\n",
      "Epoch 36: val loss 0.560082\n",
      "\n",
      "Epoch %d: train loss %f 37 0.3973138965666294\n",
      "Epoch 37: val loss 0.551027\n",
      "\n",
      "Epoch %d: train loss %f 38 0.38274820521473885\n",
      "Epoch 38: val loss 0.560545\n",
      "\n",
      "Epoch %d: train loss %f 39 0.389770682901144\n",
      "Epoch 39: val loss 0.566689\n",
      "\n",
      "Epoch %d: train loss %f 40 0.39215694926679134\n",
      "Epoch 40: val loss 0.555593\n",
      "\n",
      "Epoch %d: train loss %f 41 0.3754033297300339\n",
      "Epoch 41: val loss 0.579200\n",
      "\n",
      "Epoch %d: train loss %f 42 0.3960758298635483\n",
      "Epoch 42: val loss 0.588144\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4112074337899685\n",
      "Epoch 43: val loss 0.560307\n",
      "\n",
      "Epoch %d: train loss %f 44 0.40059177204966545\n",
      "Epoch 44: val loss 0.589905\n",
      "\n",
      "Epoch %d: train loss %f 45 0.39856499060988426\n",
      "Epoch 45: val loss 0.562266\n",
      "\n",
      "Epoch %d: train loss %f 46 0.38062426447868347\n",
      "Epoch 46: val loss 0.552670\n",
      "\n",
      "Epoch %d: train loss %f 47 0.4031345397233963\n",
      "Epoch 47: val loss 0.594053\n",
      "\n",
      "Epoch %d: train loss %f 48 0.3858112022280693\n",
      "Epoch 48: val loss 0.610284\n",
      "\n",
      "Epoch %d: train loss %f 49 0.37385198287665844\n",
      "Epoch 49: val loss 0.591890\n",
      "\n",
      "Epoch %d: train loss %f 50 0.37463185004889965\n",
      "Epoch 50: val loss 0.587141\n",
      "\n",
      "Epoch %d: train loss %f 51 0.35756082832813263\n",
      "Epoch 51: val loss 0.599588\n",
      "\n",
      "Epoch %d: train loss %f 52 0.3854982703924179\n",
      "Epoch 52: val loss 0.629491\n",
      "\n",
      "Epoch %d: train loss %f 53 0.3585272002965212\n",
      "Epoch 53: val loss 0.605313\n",
      "\n",
      "Epoch %d: train loss %f 54 0.36108117550611496\n",
      "Epoch 54: val loss 0.620954\n",
      "\n",
      "Epoch %d: train loss %f 55 0.38431643694639206\n",
      "Epoch 55: val loss 0.648437\n",
      "\n",
      "Epoch %d: train loss %f 56 0.34923262521624565\n",
      "Epoch 56: val loss 0.614587\n",
      "\n",
      "Epoch %d: train loss %f 57 0.36065924912691116\n",
      "Epoch 57: val loss 0.626608\n",
      "\n",
      "Epoch %d: train loss %f 58 0.3770001083612442\n",
      "Epoch 58: val loss 0.615313\n",
      "\n",
      "Epoch %d: train loss %f 59 0.37238461151719093\n",
      "Epoch 59: val loss 0.649625\n",
      "\n",
      "Epoch %d: train loss %f 60 0.3834260441362858\n",
      "Epoch 60: val loss 0.620669\n",
      "\n",
      "Epoch %d: train loss %f 61 0.38981783390045166\n",
      "Epoch 61: val loss 0.632668\n",
      "\n",
      "Epoch %d: train loss %f 62 0.3663524352014065\n",
      "Epoch 62: val loss 0.607017\n",
      "\n",
      "Epoch %d: train loss %f 63 0.3515185844153166\n",
      "Epoch 63: val loss 0.583408\n",
      "\n",
      "Epoch %d: train loss %f 64 0.35985231027007103\n",
      "Epoch 64: val loss 0.588955\n",
      "\n",
      "Epoch %d: train loss %f 65 0.3705385457724333\n",
      "Epoch 65: val loss 0.601972\n",
      "\n",
      "Epoch %d: train loss %f 66 0.35544089041650295\n",
      "Epoch 66: val loss 0.590479\n",
      "\n",
      "Epoch %d: train loss %f 67 0.3777883443981409\n",
      "Epoch 67: val loss 0.605620\n",
      "\n",
      "Epoch %d: train loss %f 68 0.3560378812253475\n",
      "Epoch 68: val loss 0.589958\n",
      "\n",
      "Epoch %d: train loss %f 69 0.3657519333064556\n",
      "Epoch 69: val loss 0.608160\n",
      "\n",
      "Epoch %d: train loss %f 70 0.37691619992256165\n",
      "Epoch 70: val loss 0.604030\n",
      "\n",
      "Epoch %d: train loss %f 71 0.3537578694522381\n",
      "Epoch 71: val loss 0.585835\n",
      "\n",
      "Epoch %d: train loss %f 72 0.33939697220921516\n",
      "Epoch 72: val loss 0.600157\n",
      "\n",
      "Epoch %d: train loss %f 73 0.34990907460451126\n",
      "Epoch 73: val loss 0.651267\n",
      "\n",
      "Epoch %d: train loss %f 74 0.3502301871776581\n",
      "Epoch 74: val loss 0.631721\n",
      "\n",
      "Epoch %d: train loss %f 75 0.3407815173268318\n",
      "Epoch 75: val loss 0.616060\n",
      "\n",
      "Epoch %d: train loss %f 76 0.38061015121638775\n",
      "Epoch 76: val loss 0.628597\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3166439160704613\n",
      "Epoch 77: val loss 0.621310\n",
      "\n",
      "Epoch %d: train loss %f 78 0.32407266087830067\n",
      "Epoch 78: val loss 0.616020\n",
      "\n",
      "Epoch %d: train loss %f 79 0.3367996998131275\n",
      "Epoch 79: val loss 0.612745\n",
      "\n",
      "Epoch %d: train loss %f 80 0.34930724650621414\n",
      "Epoch 80: val loss 0.610106\n",
      "\n",
      "Epoch %d: train loss %f 81 0.33401944302022457\n",
      "Epoch 81: val loss 0.649853\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3390409108251333\n",
      "Epoch 82: val loss 0.641172\n",
      "\n",
      "Epoch %d: train loss %f 83 0.33068995736539364\n",
      "Epoch 83: val loss 0.623994\n",
      "\n",
      "Epoch %d: train loss %f 84 0.3232989087700844\n",
      "Epoch 84: val loss 0.628542\n",
      "\n",
      "Epoch %d: train loss %f 85 0.3475891202688217\n",
      "Epoch 85: val loss 0.624717\n",
      "\n",
      "Epoch %d: train loss %f 86 0.3314807955175638\n",
      "Epoch 86: val loss 0.630444\n",
      "\n",
      "Epoch %d: train loss %f 87 0.34335874393582344\n",
      "Epoch 87: val loss 0.675086\n",
      "\n",
      "Epoch %d: train loss %f 88 0.31965383887290955\n",
      "Epoch 88: val loss 0.665907\n",
      "\n",
      "Epoch %d: train loss %f 89 0.32842159643769264\n",
      "Epoch 89: val loss 0.638611\n",
      "\n",
      "Epoch %d: train loss %f 90 0.33839579299092293\n",
      "Epoch 90: val loss 0.630195\n",
      "\n",
      "Epoch %d: train loss %f 91 0.319581450894475\n",
      "Epoch 91: val loss 0.657647\n",
      "\n",
      "Epoch %d: train loss %f 92 0.2946644779294729\n",
      "Epoch 92: val loss 0.637341\n",
      "\n",
      "Epoch %d: train loss %f 93 0.295552471652627\n",
      "Epoch 93: val loss 0.644433\n",
      "\n",
      "Epoch %d: train loss %f 94 0.29017670452594757\n",
      "Epoch 94: val loss 0.650955\n",
      "\n",
      "Epoch %d: train loss %f 95 0.29340338334441185\n",
      "Epoch 95: val loss 0.686136\n",
      "\n",
      "Epoch %d: train loss %f 96 0.32251841202378273\n",
      "Epoch 96: val loss 0.663068\n",
      "\n",
      "Epoch %d: train loss %f 97 0.3160798791795969\n",
      "Epoch 97: val loss 0.642259\n",
      "\n",
      "Epoch %d: train loss %f 98 0.2742520198225975\n",
      "Epoch 98: val loss 0.649249\n",
      "\n",
      "Epoch %d: train loss %f 99 0.28198266588151455\n",
      "Epoch 99: val loss 0.665397\n",
      "\n",
      "Epoch %d: train loss %f 100 0.32135789282619953\n",
      "Epoch 100: val loss 0.704713\n",
      "\n",
      "Epoch %d: train loss %f 101 0.3061134498566389\n",
      "Epoch 101: val loss 0.688578\n",
      "\n",
      "Epoch %d: train loss %f 102 0.29146550688892603\n",
      "Epoch 102: val loss 0.659544\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3138508815318346\n",
      "Epoch 103: val loss 0.671998\n",
      "\n",
      "Epoch %d: train loss %f 104 0.28623778373003006\n",
      "Epoch 104: val loss 0.695603\n",
      "\n",
      "Epoch %d: train loss %f 105 0.2527543120086193\n",
      "Epoch 105: val loss 0.695626\n",
      "\n",
      "Epoch %d: train loss %f 106 0.2857179846614599\n",
      "Epoch 106: val loss 0.693723\n",
      "\n",
      "Epoch %d: train loss %f 107 0.28229965455830097\n",
      "Epoch 107: val loss 0.711792\n",
      "\n",
      "Epoch %d: train loss %f 108 0.2922389656305313\n",
      "Epoch 108: val loss 0.684947\n",
      "\n",
      "Epoch %d: train loss %f 109 0.2671825736761093\n",
      "Epoch 109: val loss 0.697892\n",
      "\n",
      "Epoch %d: train loss %f 110 0.2492046169936657\n",
      "Epoch 110: val loss 0.723408\n",
      "\n",
      "Epoch %d: train loss %f 111 0.26338791381567717\n",
      "Epoch 111: val loss 0.727920\n",
      "\n",
      "Epoch %d: train loss %f 112 0.30820254050195217\n",
      "Epoch 112: val loss 0.778049\n",
      "\n",
      "Epoch %d: train loss %f 113 0.26251760870218277\n",
      "Epoch 113: val loss 0.695636\n",
      "\n",
      "Epoch %d: train loss %f 114 0.25601291563361883\n",
      "Epoch 114: val loss 0.736840\n",
      "\n",
      "Epoch %d: train loss %f 115 0.29837989434599876\n",
      "Epoch 115: val loss 0.744751\n",
      "\n",
      "Epoch %d: train loss %f 116 0.2527803312987089\n",
      "Epoch 116: val loss 0.688526\n",
      "\n",
      "Epoch %d: train loss %f 117 0.2655897531658411\n",
      "Epoch 117: val loss 0.726621\n",
      "\n",
      "Epoch %d: train loss %f 118 0.3014013208448887\n",
      "Epoch 118: val loss 0.703529\n",
      "\n",
      "Epoch %d: train loss %f 119 0.2357067819684744\n",
      "Epoch 119: val loss 0.729005\n",
      "\n",
      "Epoch %d: train loss %f 120 0.2958072256296873\n",
      "Epoch 120: val loss 0.771278\n",
      "\n",
      "Epoch %d: train loss %f 121 0.2627832181751728\n",
      "Epoch 121: val loss 0.843663\n",
      "\n",
      "Epoch %d: train loss %f 122 0.26158082112669945\n",
      "Epoch 122: val loss 0.761317\n",
      "\n",
      "Epoch %d: train loss %f 123 0.2539400104433298\n",
      "Epoch 123: val loss 0.750189\n",
      "\n",
      "Epoch %d: train loss %f 124 0.24890976212918758\n",
      "Epoch 124: val loss 0.761095\n",
      "\n",
      "Epoch %d: train loss %f 125 0.2834422569721937\n",
      "Epoch 125: val loss 0.751855\n",
      "\n",
      "Epoch %d: train loss %f 126 0.2753335740417242\n",
      "Epoch 126: val loss 0.800738\n",
      "\n",
      "Epoch %d: train loss %f 127 0.24881936982274055\n",
      "Epoch 127: val loss 0.874060\n",
      "\n",
      "Epoch %d: train loss %f 128 0.25539496541023254\n",
      "Epoch 128: val loss 0.734691\n",
      "\n",
      "Epoch %d: train loss %f 129 0.306999322026968\n",
      "Epoch 129: val loss 0.729527\n",
      "\n",
      "Epoch %d: train loss %f 130 0.2224910445511341\n",
      "Epoch 130: val loss 0.749085\n",
      "\n",
      "Epoch %d: train loss %f 131 0.22789455018937588\n",
      "Epoch 131: val loss 0.726463\n",
      "\n",
      "Epoch %d: train loss %f 132 0.23039098177105188\n",
      "Epoch 132: val loss 0.741142\n",
      "\n",
      "Epoch %d: train loss %f 133 0.2725123669952154\n",
      "Epoch 133: val loss 0.789596\n",
      "\n",
      "Epoch %d: train loss %f 134 0.24520014226436615\n",
      "Epoch 134: val loss 0.812700\n",
      "\n",
      "Epoch %d: train loss %f 135 0.23307382501661777\n",
      "Epoch 135: val loss 0.755973\n",
      "\n",
      "Epoch %d: train loss %f 136 0.2659428007900715\n",
      "Epoch 136: val loss 0.751415\n",
      "\n",
      "Epoch %d: train loss %f 137 0.19008858688175678\n",
      "Epoch 137: val loss 0.788460\n",
      "\n",
      "Epoch %d: train loss %f 138 0.2326511312276125\n",
      "Epoch 138: val loss 0.803175\n",
      "\n",
      "Epoch %d: train loss %f 139 0.22368434723466635\n",
      "Epoch 139: val loss 0.813690\n",
      "\n",
      "Epoch %d: train loss %f 140 0.2230579163879156\n",
      "Epoch 140: val loss 0.789348\n",
      "\n",
      "Epoch %d: train loss %f 141 0.21003505308181047\n",
      "Epoch 141: val loss 0.771884\n",
      "\n",
      "Epoch %d: train loss %f 142 0.24785882513970137\n",
      "Epoch 142: val loss 0.894986\n",
      "\n",
      "Epoch %d: train loss %f 143 0.26844511087983847\n",
      "Epoch 143: val loss 0.787656\n",
      "\n",
      "Epoch %d: train loss %f 144 0.2287081154063344\n",
      "Epoch 144: val loss 0.794993\n",
      "\n",
      "Epoch %d: train loss %f 145 0.22282504476606846\n",
      "Epoch 145: val loss 0.755094\n",
      "\n",
      "Epoch %d: train loss %f 146 0.20499402657151222\n",
      "Epoch 146: val loss 0.762667\n",
      "\n",
      "Epoch %d: train loss %f 147 0.21664611715823412\n",
      "Epoch 147: val loss 0.814672\n",
      "\n",
      "Epoch %d: train loss %f 148 0.21438949927687645\n",
      "Epoch 148: val loss 0.814231\n",
      "\n",
      "Epoch %d: train loss %f 149 0.18280740920454264\n",
      "Epoch 149: val loss 0.818104\n",
      "\n",
      "Epoch %d: train loss %f 150 0.21203065663576126\n",
      "Epoch 150: val loss 0.880223\n",
      "\n",
      "Epoch %d: train loss %f 151 0.2094085682183504\n",
      "Epoch 151: val loss 0.815075\n",
      "\n",
      "Epoch %d: train loss %f 152 0.21233423613011837\n",
      "Epoch 152: val loss 0.802943\n",
      "\n",
      "Epoch %d: train loss %f 153 0.21490466874092817\n",
      "Epoch 153: val loss 0.849478\n",
      "\n",
      "Epoch %d: train loss %f 154 0.23789043724536896\n",
      "Epoch 154: val loss 0.851311\n",
      "\n",
      "Epoch %d: train loss %f 155 0.20211514458060265\n",
      "Epoch 155: val loss 0.841223\n",
      "\n",
      "Epoch %d: train loss %f 156 0.1921313889324665\n",
      "Epoch 156: val loss 0.836232\n",
      "\n",
      "Epoch %d: train loss %f 157 0.21395611111074686\n",
      "Epoch 157: val loss 0.864904\n",
      "\n",
      "Epoch %d: train loss %f 158 0.21600125078111887\n",
      "Epoch 158: val loss 0.853470\n",
      "\n",
      "Epoch %d: train loss %f 159 0.22623400297015905\n",
      "Epoch 159: val loss 0.866609\n",
      "\n",
      "Epoch %d: train loss %f 160 0.20669314358383417\n",
      "Epoch 160: val loss 0.939786\n",
      "\n",
      "Epoch %d: train loss %f 161 0.17874807864427567\n",
      "Epoch 161: val loss 0.954105\n",
      "\n",
      "Epoch %d: train loss %f 162 0.16531990841031075\n",
      "Epoch 162: val loss 0.895519\n",
      "\n",
      "Epoch %d: train loss %f 163 0.24420502595603466\n",
      "Epoch 163: val loss 0.962348\n",
      "\n",
      "Epoch %d: train loss %f 164 0.24240956641733646\n",
      "Epoch 164: val loss 0.885254\n",
      "\n",
      "Epoch %d: train loss %f 165 0.21934322454035282\n",
      "Epoch 165: val loss 0.922057\n",
      "\n",
      "Epoch %d: train loss %f 166 0.2002436015754938\n",
      "Epoch 166: val loss 0.884461\n",
      "\n",
      "Epoch %d: train loss %f 167 0.1822005044668913\n",
      "Epoch 167: val loss 0.932550\n",
      "\n",
      "Epoch %d: train loss %f 168 0.21853441186249256\n",
      "Epoch 168: val loss 0.954755\n",
      "\n",
      "Epoch %d: train loss %f 169 0.22623819950968027\n",
      "Epoch 169: val loss 0.867191\n",
      "\n",
      "Epoch %d: train loss %f 170 0.21134438272565603\n",
      "Epoch 170: val loss 0.917696\n",
      "\n",
      "Epoch %d: train loss %f 171 0.17628864059224725\n",
      "Epoch 171: val loss 0.879178\n",
      "\n",
      "Epoch %d: train loss %f 172 0.17750397976487875\n",
      "Epoch 172: val loss 0.899491\n",
      "\n",
      "Epoch %d: train loss %f 173 0.20200454257428646\n",
      "Epoch 173: val loss 0.949626\n",
      "\n",
      "Epoch %d: train loss %f 174 0.22225572168827057\n",
      "Epoch 174: val loss 0.922570\n",
      "\n",
      "Epoch %d: train loss %f 175 0.2231081910431385\n",
      "Epoch 175: val loss 0.928714\n",
      "\n",
      "Epoch %d: train loss %f 176 0.22716423124074936\n",
      "Epoch 176: val loss 0.922113\n",
      "\n",
      "Epoch %d: train loss %f 177 0.18354465905576944\n",
      "Epoch 177: val loss 1.013023\n",
      "\n",
      "Epoch %d: train loss %f 178 0.23523288499563932\n",
      "Epoch 178: val loss 0.937339\n",
      "\n",
      "Epoch %d: train loss %f 179 0.20644807629287243\n",
      "Epoch 179: val loss 0.977685\n",
      "\n",
      "Epoch %d: train loss %f 180 0.24771420005708933\n",
      "Epoch 180: val loss 0.867365\n",
      "\n",
      "Epoch %d: train loss %f 181 0.183870080858469\n",
      "Epoch 181: val loss 0.926022\n",
      "\n",
      "Epoch %d: train loss %f 182 0.210341508500278\n",
      "Epoch 182: val loss 0.882187\n",
      "\n",
      "Epoch %d: train loss %f 183 0.19773590564727783\n",
      "Epoch 183: val loss 0.901576\n",
      "\n",
      "Epoch %d: train loss %f 184 0.20587548334151506\n",
      "Epoch 184: val loss 0.911395\n",
      "\n",
      "Epoch %d: train loss %f 185 0.19806032348424196\n",
      "Epoch 185: val loss 0.883337\n",
      "\n",
      "Epoch %d: train loss %f 186 0.1753162732347846\n",
      "Epoch 186: val loss 0.883154\n",
      "\n",
      "Epoch %d: train loss %f 187 0.1751252105459571\n",
      "Epoch 187: val loss 0.949273\n",
      "\n",
      "Epoch %d: train loss %f 188 0.19392867479473352\n",
      "Epoch 188: val loss 0.968679\n",
      "\n",
      "Epoch %d: train loss %f 189 0.20339537877589464\n",
      "Epoch 189: val loss 0.968357\n",
      "\n",
      "Epoch %d: train loss %f 190 0.14529934898018837\n",
      "Epoch 190: val loss 0.930272\n",
      "\n",
      "Epoch %d: train loss %f 191 0.20606521237641573\n",
      "Epoch 191: val loss 0.927712\n",
      "\n",
      "Epoch %d: train loss %f 192 0.20691697672009468\n",
      "Epoch 192: val loss 0.980748\n",
      "\n",
      "Epoch %d: train loss %f 193 0.1303347907960415\n",
      "Epoch 193: val loss 0.907380\n",
      "\n",
      "Epoch %d: train loss %f 194 0.1956057120114565\n",
      "Epoch 194: val loss 0.931623\n",
      "\n",
      "Epoch %d: train loss %f 195 0.17873299773782492\n",
      "Epoch 195: val loss 0.923635\n",
      "\n",
      "Epoch %d: train loss %f 196 0.20619399100542068\n",
      "Epoch 196: val loss 1.031676\n",
      "\n",
      "Epoch %d: train loss %f 197 0.14702343102544546\n",
      "Epoch 197: val loss 1.047523\n",
      "\n",
      "Epoch %d: train loss %f 198 0.21767967380583286\n",
      "Epoch 198: val loss 0.927846\n",
      "\n",
      "Epoch %d: train loss %f 199 0.20168631710112095\n",
      "Epoch 199: val loss 0.907265\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6761694008653815\n",
      "Epoch 0: val loss 0.676657\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6647810935974121\n",
      "Epoch 1: val loss 0.671093\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6511646725914695\n",
      "Epoch 2: val loss 0.660922\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6212942816994407\n",
      "Epoch 3: val loss 0.641957\n",
      "\n",
      "Epoch %d: train loss %f 4 0.5798817439512773\n",
      "Epoch 4: val loss 0.605964\n",
      "\n",
      "Epoch %d: train loss %f 5 0.5321748581799594\n",
      "Epoch 5: val loss 0.557134\n",
      "\n",
      "Epoch %d: train loss %f 6 0.49902782385999506\n",
      "Epoch 6: val loss 0.527017\n",
      "\n",
      "Epoch %d: train loss %f 7 0.48580132831226697\n",
      "Epoch 7: val loss 0.535686\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5056324845010584\n",
      "Epoch 8: val loss 0.511405\n",
      "\n",
      "Epoch %d: train loss %f 9 0.48132813247767364\n",
      "Epoch 9: val loss 0.547298\n",
      "\n",
      "Epoch %d: train loss %f 10 0.45588118650696496\n",
      "Epoch 10: val loss 0.522410\n",
      "\n",
      "Epoch %d: train loss %f 11 0.45692492344162683\n",
      "Epoch 11: val loss 0.529218\n",
      "\n",
      "Epoch %d: train loss %f 12 0.4608503553000363\n",
      "Epoch 12: val loss 0.539904\n",
      "\n",
      "Epoch %d: train loss %f 13 0.48417178609154443\n",
      "Epoch 13: val loss 0.548880\n",
      "\n",
      "Epoch %d: train loss %f 14 0.47639806975017895\n",
      "Epoch 14: val loss 0.542977\n",
      "\n",
      "Epoch %d: train loss %f 15 0.4567261880094355\n",
      "Epoch 15: val loss 0.558284\n",
      "\n",
      "Epoch %d: train loss %f 16 0.44194511527364905\n",
      "Epoch 16: val loss 0.533656\n",
      "\n",
      "Epoch %d: train loss %f 17 0.45912824977527966\n",
      "Epoch 17: val loss 0.537095\n",
      "\n",
      "Epoch %d: train loss %f 18 0.44977033680135553\n",
      "Epoch 18: val loss 0.537147\n",
      "\n",
      "Epoch %d: train loss %f 19 0.4574889215556058\n",
      "Epoch 19: val loss 0.545577\n",
      "\n",
      "Epoch %d: train loss %f 20 0.4167331673882224\n",
      "Epoch 20: val loss 0.558667\n",
      "\n",
      "Epoch %d: train loss %f 21 0.459987765008753\n",
      "Epoch 21: val loss 0.551589\n",
      "\n",
      "Epoch %d: train loss %f 22 0.44034448807889764\n",
      "Epoch 22: val loss 0.556129\n",
      "\n",
      "Epoch %d: train loss %f 23 0.4432586417956786\n",
      "Epoch 23: val loss 0.552548\n",
      "\n",
      "Epoch %d: train loss %f 24 0.42517726800658484\n",
      "Epoch 24: val loss 0.557459\n",
      "\n",
      "Epoch %d: train loss %f 25 0.43598638610406354\n",
      "Epoch 25: val loss 0.562635\n",
      "\n",
      "Epoch %d: train loss %f 26 0.449528924443505\n",
      "Epoch 26: val loss 0.589696\n",
      "\n",
      "Epoch %d: train loss %f 27 0.4278452396392822\n",
      "Epoch 27: val loss 0.571981\n",
      "\n",
      "Epoch %d: train loss %f 28 0.44322896003723145\n",
      "Epoch 28: val loss 0.578690\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4372800209305503\n",
      "Epoch 29: val loss 0.575696\n",
      "\n",
      "Epoch %d: train loss %f 30 0.42669778249480506\n",
      "Epoch 30: val loss 0.551720\n",
      "\n",
      "Epoch %d: train loss %f 31 0.41652802445671777\n",
      "Epoch 31: val loss 0.576910\n",
      "\n",
      "Epoch %d: train loss %f 32 0.46424495902928437\n",
      "Epoch 32: val loss 0.557134\n",
      "\n",
      "Epoch %d: train loss %f 33 0.4023562127893621\n",
      "Epoch 33: val loss 0.564263\n",
      "\n",
      "Epoch %d: train loss %f 34 0.41572206670587714\n",
      "Epoch 34: val loss 0.548344\n",
      "\n",
      "Epoch %d: train loss %f 35 0.41440869732336566\n",
      "Epoch 35: val loss 0.550764\n",
      "\n",
      "Epoch %d: train loss %f 36 0.408882813020186\n",
      "Epoch 36: val loss 0.551206\n",
      "\n",
      "Epoch %d: train loss %f 37 0.398854905908758\n",
      "Epoch 37: val loss 0.570450\n",
      "\n",
      "Epoch %d: train loss %f 38 0.4038018963553689\n",
      "Epoch 38: val loss 0.573762\n",
      "\n",
      "Epoch %d: train loss %f 39 0.41081632673740387\n",
      "Epoch 39: val loss 0.614110\n",
      "\n",
      "Epoch %d: train loss %f 40 0.4002802155234597\n",
      "Epoch 40: val loss 0.589608\n",
      "\n",
      "Epoch %d: train loss %f 41 0.41016253558072174\n",
      "Epoch 41: val loss 0.558470\n",
      "\n",
      "Epoch %d: train loss %f 42 0.3951003741134297\n",
      "Epoch 42: val loss 0.555393\n",
      "\n",
      "Epoch %d: train loss %f 43 0.41477440703998913\n",
      "Epoch 43: val loss 0.574692\n",
      "\n",
      "Epoch %d: train loss %f 44 0.37151047722859815\n",
      "Epoch 44: val loss 0.572082\n",
      "\n",
      "Epoch %d: train loss %f 45 0.3787972249767997\n",
      "Epoch 45: val loss 0.574717\n",
      "\n",
      "Epoch %d: train loss %f 46 0.37676039609042083\n",
      "Epoch 46: val loss 0.591466\n",
      "\n",
      "Epoch %d: train loss %f 47 0.3679064471613277\n",
      "Epoch 47: val loss 0.569257\n",
      "\n",
      "Epoch %d: train loss %f 48 0.37697796388105914\n",
      "Epoch 48: val loss 0.581921\n",
      "\n",
      "Epoch %d: train loss %f 49 0.40191159736026416\n",
      "Epoch 49: val loss 0.585785\n",
      "\n",
      "Epoch %d: train loss %f 50 0.3807909136468714\n",
      "Epoch 50: val loss 0.589874\n",
      "\n",
      "Epoch %d: train loss %f 51 0.3590965284542604\n",
      "Epoch 51: val loss 0.597435\n",
      "\n",
      "Epoch %d: train loss %f 52 0.3693056919357993\n",
      "Epoch 52: val loss 0.586906\n",
      "\n",
      "Epoch %d: train loss %f 53 0.37947186692194507\n",
      "Epoch 53: val loss 0.569557\n",
      "\n",
      "Epoch %d: train loss %f 54 0.37165160883556714\n",
      "Epoch 54: val loss 0.605057\n",
      "\n",
      "Epoch %d: train loss %f 55 0.37924101813273\n",
      "Epoch 55: val loss 0.599004\n",
      "\n",
      "Epoch %d: train loss %f 56 0.3488140526142987\n",
      "Epoch 56: val loss 0.625802\n",
      "\n",
      "Epoch %d: train loss %f 57 0.34498870643702423\n",
      "Epoch 57: val loss 0.622060\n",
      "\n",
      "Epoch %d: train loss %f 58 0.37437993017109955\n",
      "Epoch 58: val loss 0.641121\n",
      "\n",
      "Epoch %d: train loss %f 59 0.3368736668066545\n",
      "Epoch 59: val loss 0.617210\n",
      "\n",
      "Epoch %d: train loss %f 60 0.367287584326484\n",
      "Epoch 60: val loss 0.626382\n",
      "\n",
      "Epoch %d: train loss %f 61 0.35195632685314526\n",
      "Epoch 61: val loss 0.667789\n",
      "\n",
      "Epoch %d: train loss %f 62 0.3709818327968771\n",
      "Epoch 62: val loss 0.644586\n",
      "\n",
      "Epoch %d: train loss %f 63 0.34400369091467425\n",
      "Epoch 63: val loss 0.629940\n",
      "\n",
      "Epoch %d: train loss %f 64 0.37000111829150806\n",
      "Epoch 64: val loss 0.630999\n",
      "\n",
      "Epoch %d: train loss %f 65 0.357748417691751\n",
      "Epoch 65: val loss 0.649649\n",
      "\n",
      "Epoch %d: train loss %f 66 0.346145979382775\n",
      "Epoch 66: val loss 0.611660\n",
      "\n",
      "Epoch %d: train loss %f 67 0.36326647888530383\n",
      "Epoch 67: val loss 0.632522\n",
      "\n",
      "Epoch %d: train loss %f 68 0.34785937585613946\n",
      "Epoch 68: val loss 0.636549\n",
      "\n",
      "Epoch %d: train loss %f 69 0.362497478723526\n",
      "Epoch 69: val loss 0.661445\n",
      "\n",
      "Epoch %d: train loss %f 70 0.3306929401376031\n",
      "Epoch 70: val loss 0.626012\n",
      "\n",
      "Epoch %d: train loss %f 71 0.34527272663333197\n",
      "Epoch 71: val loss 0.604090\n",
      "\n",
      "Epoch %d: train loss %f 72 0.3505580194971778\n",
      "Epoch 72: val loss 0.639264\n",
      "\n",
      "Epoch %d: train loss %f 73 0.3549553237178109\n",
      "Epoch 73: val loss 0.611113\n",
      "\n",
      "Epoch %d: train loss %f 74 0.32367211241613736\n",
      "Epoch 74: val loss 0.659245\n",
      "\n",
      "Epoch %d: train loss %f 75 0.29451205784624274\n",
      "Epoch 75: val loss 0.653939\n",
      "\n",
      "Epoch %d: train loss %f 76 0.34260175038467755\n",
      "Epoch 76: val loss 0.634238\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3434410880912434\n",
      "Epoch 77: val loss 0.620824\n",
      "\n",
      "Epoch %d: train loss %f 78 0.3604207919402556\n",
      "Epoch 78: val loss 0.638113\n",
      "\n",
      "Epoch %d: train loss %f 79 0.31680868430571124\n",
      "Epoch 79: val loss 0.634632\n",
      "\n",
      "Epoch %d: train loss %f 80 0.34972312369129877\n",
      "Epoch 80: val loss 0.658891\n",
      "\n",
      "Epoch %d: train loss %f 81 0.3375954655083743\n",
      "Epoch 81: val loss 0.664308\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3294810083779422\n",
      "Epoch 82: val loss 0.645065\n",
      "\n",
      "Epoch %d: train loss %f 83 0.29779443551193585\n",
      "Epoch 83: val loss 0.656234\n",
      "\n",
      "Epoch %d: train loss %f 84 0.35251715779304504\n",
      "Epoch 84: val loss 0.670482\n",
      "\n",
      "Epoch %d: train loss %f 85 0.3165621574629437\n",
      "Epoch 85: val loss 0.626520\n",
      "\n",
      "Epoch %d: train loss %f 86 0.33853296122767706\n",
      "Epoch 86: val loss 0.627073\n",
      "\n",
      "Epoch %d: train loss %f 87 0.31564214012839575\n",
      "Epoch 87: val loss 0.652244\n",
      "\n",
      "Epoch %d: train loss %f 88 0.3275964233008298\n",
      "Epoch 88: val loss 0.660418\n",
      "\n",
      "Epoch %d: train loss %f 89 0.34063236821781506\n",
      "Epoch 89: val loss 0.667879\n",
      "\n",
      "Epoch %d: train loss %f 90 0.34949189153584564\n",
      "Epoch 90: val loss 0.663208\n",
      "\n",
      "Epoch %d: train loss %f 91 0.29727120765230874\n",
      "Epoch 91: val loss 0.614993\n",
      "\n",
      "Epoch %d: train loss %f 92 0.326625567945567\n",
      "Epoch 92: val loss 0.653193\n",
      "\n",
      "Epoch %d: train loss %f 93 0.33487638018348\n",
      "Epoch 93: val loss 0.634325\n",
      "\n",
      "Epoch %d: train loss %f 94 0.3530478545210578\n",
      "Epoch 94: val loss 0.659913\n",
      "\n",
      "Epoch %d: train loss %f 95 0.30005721070549707\n",
      "Epoch 95: val loss 0.644443\n",
      "\n",
      "Epoch %d: train loss %f 96 0.30996147339994257\n",
      "Epoch 96: val loss 0.659733\n",
      "\n",
      "Epoch %d: train loss %f 97 0.3660175827416507\n",
      "Epoch 97: val loss 0.647337\n",
      "\n",
      "Epoch %d: train loss %f 98 0.32298309288241644\n",
      "Epoch 98: val loss 0.657732\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3411421789364381\n",
      "Epoch 99: val loss 0.675052\n",
      "\n",
      "Epoch %d: train loss %f 100 0.3297971338033676\n",
      "Epoch 100: val loss 0.647288\n",
      "\n",
      "Epoch %d: train loss %f 101 0.3227845322002064\n",
      "Epoch 101: val loss 0.664593\n",
      "\n",
      "Epoch %d: train loss %f 102 0.32902409678155725\n",
      "Epoch 102: val loss 0.627878\n",
      "\n",
      "Epoch %d: train loss %f 103 0.34921595318750903\n",
      "Epoch 103: val loss 0.613936\n",
      "\n",
      "Epoch %d: train loss %f 104 0.29664543541994964\n",
      "Epoch 104: val loss 0.635648\n",
      "\n",
      "Epoch %d: train loss %f 105 0.3359440185806968\n",
      "Epoch 105: val loss 0.632759\n",
      "\n",
      "Epoch %d: train loss %f 106 0.35593558576974\n",
      "Epoch 106: val loss 0.640433\n",
      "\n",
      "Epoch %d: train loss %f 107 0.3393675264987079\n",
      "Epoch 107: val loss 0.688090\n",
      "\n",
      "Epoch %d: train loss %f 108 0.3119687383825129\n",
      "Epoch 108: val loss 0.626459\n",
      "\n",
      "Epoch %d: train loss %f 109 0.336997005072507\n",
      "Epoch 109: val loss 0.666982\n",
      "\n",
      "Epoch %d: train loss %f 110 0.31496360356157477\n",
      "Epoch 110: val loss 0.667669\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3373327851295471\n",
      "Epoch 111: val loss 0.642941\n",
      "\n",
      "Epoch %d: train loss %f 112 0.32701899246736005\n",
      "Epoch 112: val loss 0.610142\n",
      "\n",
      "Epoch %d: train loss %f 113 0.2990572140975432\n",
      "Epoch 113: val loss 0.649841\n",
      "\n",
      "Epoch %d: train loss %f 114 0.35385682637041266\n",
      "Epoch 114: val loss 0.675881\n",
      "\n",
      "Epoch %d: train loss %f 115 0.34411114860664715\n",
      "Epoch 115: val loss 0.672142\n",
      "\n",
      "Epoch %d: train loss %f 116 0.3215855630961331\n",
      "Epoch 116: val loss 0.620476\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3363247202201323\n",
      "Epoch 117: val loss 0.652059\n",
      "\n",
      "Epoch %d: train loss %f 118 0.30059809576381336\n",
      "Epoch 118: val loss 0.612373\n",
      "\n",
      "Epoch %d: train loss %f 119 0.33137943663380365\n",
      "Epoch 119: val loss 0.619289\n",
      "\n",
      "Epoch %d: train loss %f 120 0.31632327220656653\n",
      "Epoch 120: val loss 0.647655\n",
      "\n",
      "Epoch %d: train loss %f 121 0.30623567781665106\n",
      "Epoch 121: val loss 0.671030\n",
      "\n",
      "Epoch %d: train loss %f 122 0.2999042814428156\n",
      "Epoch 122: val loss 0.670699\n",
      "\n",
      "Epoch %d: train loss %f 123 0.28943356465209613\n",
      "Epoch 123: val loss 0.676217\n",
      "\n",
      "Epoch %d: train loss %f 124 0.32395101406357507\n",
      "Epoch 124: val loss 0.644716\n",
      "\n",
      "Epoch %d: train loss %f 125 0.30894323641603644\n",
      "Epoch 125: val loss 0.676844\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3156452747908505\n",
      "Epoch 126: val loss 0.687190\n",
      "\n",
      "Epoch %d: train loss %f 127 0.3243875205516815\n",
      "Epoch 127: val loss 0.663258\n",
      "\n",
      "Epoch %d: train loss %f 128 0.31719588014212524\n",
      "Epoch 128: val loss 0.672778\n",
      "\n",
      "Epoch %d: train loss %f 129 0.2955342707308856\n",
      "Epoch 129: val loss 0.655299\n",
      "\n",
      "Epoch %d: train loss %f 130 0.2880486087365584\n",
      "Epoch 130: val loss 0.691222\n",
      "\n",
      "Epoch %d: train loss %f 131 0.3091068755496632\n",
      "Epoch 131: val loss 0.646901\n",
      "\n",
      "Epoch %d: train loss %f 132 0.30811249667947943\n",
      "Epoch 132: val loss 0.649292\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3083287165923552\n",
      "Epoch 133: val loss 0.667601\n",
      "\n",
      "Epoch %d: train loss %f 134 0.2928315739740025\n",
      "Epoch 134: val loss 0.635617\n",
      "\n",
      "Epoch %d: train loss %f 135 0.31307725608348846\n",
      "Epoch 135: val loss 0.666522\n",
      "\n",
      "Epoch %d: train loss %f 136 0.30082307755947113\n",
      "Epoch 136: val loss 0.653021\n",
      "\n",
      "Epoch %d: train loss %f 137 0.286768300966783\n",
      "Epoch 137: val loss 0.651374\n",
      "\n",
      "Epoch %d: train loss %f 138 0.29476947676051746\n",
      "Epoch 138: val loss 0.706142\n",
      "\n",
      "Epoch %d: train loss %f 139 0.30918183651837433\n",
      "Epoch 139: val loss 0.672535\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6923913836479187\n",
      "Epoch 0: val loss 0.693678\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6792866289615631\n",
      "Epoch 1: val loss 0.686017\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6585862398147583\n",
      "Epoch 2: val loss 0.668254\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6220912039279938\n",
      "Epoch 3: val loss 0.633823\n",
      "\n",
      "Epoch %d: train loss %f 4 0.5854757815599442\n",
      "Epoch 4: val loss 0.590977\n",
      "\n",
      "Epoch %d: train loss %f 5 0.5661734968423844\n",
      "Epoch 5: val loss 0.562889\n",
      "\n",
      "Epoch %d: train loss %f 6 0.5495726704597473\n",
      "Epoch 6: val loss 0.544125\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5220842570066452\n",
      "Epoch 7: val loss 0.532011\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5175766795873642\n",
      "Epoch 8: val loss 0.527950\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5208306550979614\n",
      "Epoch 9: val loss 0.526144\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5058345109224319\n",
      "Epoch 10: val loss 0.525215\n",
      "\n",
      "Epoch %d: train loss %f 11 0.536407420039177\n",
      "Epoch 11: val loss 0.522803\n",
      "\n",
      "Epoch %d: train loss %f 12 0.49888443052768705\n",
      "Epoch 12: val loss 0.526060\n",
      "\n",
      "Epoch %d: train loss %f 13 0.48701362013816835\n",
      "Epoch 13: val loss 0.519119\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5287494242191315\n",
      "Epoch 14: val loss 0.518083\n",
      "\n",
      "Epoch %d: train loss %f 15 0.49074522256851194\n",
      "Epoch 15: val loss 0.517113\n",
      "\n",
      "Epoch %d: train loss %f 16 0.4849777966737747\n",
      "Epoch 16: val loss 0.517771\n",
      "\n",
      "Epoch %d: train loss %f 17 0.4776055455207825\n",
      "Epoch 17: val loss 0.527120\n",
      "\n",
      "Epoch %d: train loss %f 18 0.4812936305999756\n",
      "Epoch 18: val loss 0.521555\n",
      "\n",
      "Epoch %d: train loss %f 19 0.48407965898513794\n",
      "Epoch 19: val loss 0.517225\n",
      "\n",
      "Epoch %d: train loss %f 20 0.489230740070343\n",
      "Epoch 20: val loss 0.535449\n",
      "\n",
      "Epoch %d: train loss %f 21 0.4808896780014038\n",
      "Epoch 21: val loss 0.519221\n",
      "\n",
      "Epoch %d: train loss %f 22 0.4671541333198547\n",
      "Epoch 22: val loss 0.528959\n",
      "\n",
      "Epoch %d: train loss %f 23 0.4726279050111771\n",
      "Epoch 23: val loss 0.516972\n",
      "\n",
      "Epoch %d: train loss %f 24 0.46343581676483153\n",
      "Epoch 24: val loss 0.526725\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4580710232257843\n",
      "Epoch 25: val loss 0.524231\n",
      "\n",
      "Epoch %d: train loss %f 26 0.464973458647728\n",
      "Epoch 26: val loss 0.521223\n",
      "\n",
      "Epoch %d: train loss %f 27 0.45072944462299347\n",
      "Epoch 27: val loss 0.528158\n",
      "\n",
      "Epoch %d: train loss %f 28 0.45290533602237704\n",
      "Epoch 28: val loss 0.514789\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4315495491027832\n",
      "Epoch 29: val loss 0.519593\n",
      "\n",
      "Epoch %d: train loss %f 30 0.43833484351634977\n",
      "Epoch 30: val loss 0.527966\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4626224279403687\n",
      "Epoch 31: val loss 0.537050\n",
      "\n",
      "Epoch %d: train loss %f 32 0.46169348657131193\n",
      "Epoch 32: val loss 0.526454\n",
      "\n",
      "Epoch %d: train loss %f 33 0.47828249335289\n",
      "Epoch 33: val loss 0.527355\n",
      "\n",
      "Epoch %d: train loss %f 34 0.4614434689283371\n",
      "Epoch 34: val loss 0.538399\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4431867301464081\n",
      "Epoch 35: val loss 0.527597\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4466096371412277\n",
      "Epoch 36: val loss 0.529421\n",
      "\n",
      "Epoch %d: train loss %f 37 0.45747215747833253\n",
      "Epoch 37: val loss 0.533031\n",
      "\n",
      "Epoch %d: train loss %f 38 0.44514444172382356\n",
      "Epoch 38: val loss 0.530846\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4464990824460983\n",
      "Epoch 39: val loss 0.527988\n",
      "\n",
      "Epoch %d: train loss %f 40 0.4564490795135498\n",
      "Epoch 40: val loss 0.526166\n",
      "\n",
      "Epoch %d: train loss %f 41 0.434452548623085\n",
      "Epoch 41: val loss 0.529864\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4132590427994728\n",
      "Epoch 42: val loss 0.525558\n",
      "\n",
      "Epoch %d: train loss %f 43 0.46215389370918275\n",
      "Epoch 43: val loss 0.534630\n",
      "\n",
      "Epoch %d: train loss %f 44 0.43169275522232053\n",
      "Epoch 44: val loss 0.527773\n",
      "\n",
      "Epoch %d: train loss %f 45 0.4470222324132919\n",
      "Epoch 45: val loss 0.537393\n",
      "\n",
      "Epoch %d: train loss %f 46 0.43138488531112673\n",
      "Epoch 46: val loss 0.525862\n",
      "\n",
      "Epoch %d: train loss %f 47 0.41148604452610016\n",
      "Epoch 47: val loss 0.534052\n",
      "\n",
      "Epoch %d: train loss %f 48 0.4430211216211319\n",
      "Epoch 48: val loss 0.557584\n",
      "\n",
      "Epoch %d: train loss %f 49 0.44400947391986845\n",
      "Epoch 49: val loss 0.540832\n",
      "\n",
      "Epoch %d: train loss %f 50 0.43993438482284547\n",
      "Epoch 50: val loss 0.532986\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4252917513251305\n",
      "Epoch 51: val loss 0.531873\n",
      "\n",
      "Epoch %d: train loss %f 52 0.42126658856868743\n",
      "Epoch 52: val loss 0.538436\n",
      "\n",
      "Epoch %d: train loss %f 53 0.4107092976570129\n",
      "Epoch 53: val loss 0.534952\n",
      "\n",
      "Epoch %d: train loss %f 54 0.38995817601680755\n",
      "Epoch 54: val loss 0.544504\n",
      "\n",
      "Epoch %d: train loss %f 55 0.4011795610189438\n",
      "Epoch 55: val loss 0.531920\n",
      "\n",
      "Epoch %d: train loss %f 56 0.4410253494977951\n",
      "Epoch 56: val loss 0.551122\n",
      "\n",
      "Epoch %d: train loss %f 57 0.41248393058776855\n",
      "Epoch 57: val loss 0.553065\n",
      "\n",
      "Epoch %d: train loss %f 58 0.4302917689085007\n",
      "Epoch 58: val loss 0.535230\n",
      "\n",
      "Epoch %d: train loss %f 59 0.39905265271663665\n",
      "Epoch 59: val loss 0.535369\n",
      "\n",
      "Epoch %d: train loss %f 60 0.42573114633560183\n",
      "Epoch 60: val loss 0.530488\n",
      "\n",
      "Epoch %d: train loss %f 61 0.4364379942417145\n",
      "Epoch 61: val loss 0.524651\n",
      "\n",
      "Epoch %d: train loss %f 62 0.4396176695823669\n",
      "Epoch 62: val loss 0.530535\n",
      "\n",
      "Epoch %d: train loss %f 63 0.4153490215539932\n",
      "Epoch 63: val loss 0.554313\n",
      "\n",
      "Epoch %d: train loss %f 64 0.42006585001945496\n",
      "Epoch 64: val loss 0.549660\n",
      "\n",
      "Epoch %d: train loss %f 65 0.39473617672920225\n",
      "Epoch 65: val loss 0.542017\n",
      "\n",
      "Epoch %d: train loss %f 66 0.40256164520978927\n",
      "Epoch 66: val loss 0.542372\n",
      "\n",
      "Epoch %d: train loss %f 67 0.4039867252111435\n",
      "Epoch 67: val loss 0.543170\n",
      "\n",
      "Epoch %d: train loss %f 68 0.4026684433221817\n",
      "Epoch 68: val loss 0.541177\n",
      "\n",
      "Epoch %d: train loss %f 69 0.40113143175840377\n",
      "Epoch 69: val loss 0.557478\n",
      "\n",
      "Epoch %d: train loss %f 70 0.391959086060524\n",
      "Epoch 70: val loss 0.551915\n",
      "\n",
      "Epoch %d: train loss %f 71 0.3878308475017548\n",
      "Epoch 71: val loss 0.538347\n",
      "\n",
      "Epoch %d: train loss %f 72 0.4187379479408264\n",
      "Epoch 72: val loss 0.539817\n",
      "\n",
      "Epoch %d: train loss %f 73 0.4063453793525696\n",
      "Epoch 73: val loss 0.555098\n",
      "\n",
      "Epoch %d: train loss %f 74 0.4258688062429428\n",
      "Epoch 74: val loss 0.532200\n",
      "\n",
      "Epoch %d: train loss %f 75 0.4077874481678009\n",
      "Epoch 75: val loss 0.558397\n",
      "\n",
      "Epoch %d: train loss %f 76 0.4481648176908493\n",
      "Epoch 76: val loss 0.537837\n",
      "\n",
      "Epoch %d: train loss %f 77 0.37674298584461213\n",
      "Epoch 77: val loss 0.550214\n",
      "\n",
      "Epoch %d: train loss %f 78 0.3864739179611206\n",
      "Epoch 78: val loss 0.543400\n",
      "\n",
      "Epoch %d: train loss %f 79 0.3754937618970871\n",
      "Epoch 79: val loss 0.576673\n",
      "\n",
      "Epoch %d: train loss %f 80 0.3896542489528656\n",
      "Epoch 80: val loss 0.554511\n",
      "\n",
      "Epoch %d: train loss %f 81 0.4047107815742493\n",
      "Epoch 81: val loss 0.534860\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3959051638841629\n",
      "Epoch 82: val loss 0.555497\n",
      "\n",
      "Epoch %d: train loss %f 83 0.39263801723718644\n",
      "Epoch 83: val loss 0.543065\n",
      "\n",
      "Epoch %d: train loss %f 84 0.39019877910614015\n",
      "Epoch 84: val loss 0.571998\n",
      "\n",
      "Epoch %d: train loss %f 85 0.37081551402807233\n",
      "Epoch 85: val loss 0.543286\n",
      "\n",
      "Epoch %d: train loss %f 86 0.3740913733839989\n",
      "Epoch 86: val loss 0.558257\n",
      "\n",
      "Epoch %d: train loss %f 87 0.38883885741233826\n",
      "Epoch 87: val loss 0.577143\n",
      "\n",
      "Epoch %d: train loss %f 88 0.4094630151987076\n",
      "Epoch 88: val loss 0.551759\n",
      "\n",
      "Epoch %d: train loss %f 89 0.38007579296827315\n",
      "Epoch 89: val loss 0.557182\n",
      "\n",
      "Epoch %d: train loss %f 90 0.38396180272102354\n",
      "Epoch 90: val loss 0.567339\n",
      "\n",
      "Epoch %d: train loss %f 91 0.41696794927120207\n",
      "Epoch 91: val loss 0.544495\n",
      "\n",
      "Epoch %d: train loss %f 92 0.39729235172271726\n",
      "Epoch 92: val loss 0.551714\n",
      "\n",
      "Epoch %d: train loss %f 93 0.3630190670490265\n",
      "Epoch 93: val loss 0.542164\n",
      "\n",
      "Epoch %d: train loss %f 94 0.4129001259803772\n",
      "Epoch 94: val loss 0.566654\n",
      "\n",
      "Epoch %d: train loss %f 95 0.4219463974237442\n",
      "Epoch 95: val loss 0.538689\n",
      "\n",
      "Epoch %d: train loss %f 96 0.3803193151950836\n",
      "Epoch 96: val loss 0.529576\n",
      "\n",
      "Epoch %d: train loss %f 97 0.37980604767799375\n",
      "Epoch 97: val loss 0.543896\n",
      "\n",
      "Epoch %d: train loss %f 98 0.40541352778673173\n",
      "Epoch 98: val loss 0.561373\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3626677647233009\n",
      "Epoch 99: val loss 0.564682\n",
      "\n",
      "Epoch %d: train loss %f 100 0.36850483566522596\n",
      "Epoch 100: val loss 0.554281\n",
      "\n",
      "Epoch %d: train loss %f 101 0.3751858800649643\n",
      "Epoch 101: val loss 0.554182\n",
      "\n",
      "Epoch %d: train loss %f 102 0.3895724669098854\n",
      "Epoch 102: val loss 0.573178\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3753909274935722\n",
      "Epoch 103: val loss 0.546464\n",
      "\n",
      "Epoch %d: train loss %f 104 0.36300062835216523\n",
      "Epoch 104: val loss 0.563047\n",
      "\n",
      "Epoch %d: train loss %f 105 0.33839566707611085\n",
      "Epoch 105: val loss 0.586608\n",
      "\n",
      "Epoch %d: train loss %f 106 0.36951829195022584\n",
      "Epoch 106: val loss 0.580900\n",
      "\n",
      "Epoch %d: train loss %f 107 0.3682879626750946\n",
      "Epoch 107: val loss 0.568217\n",
      "\n",
      "Epoch %d: train loss %f 108 0.3438243180513382\n",
      "Epoch 108: val loss 0.588086\n",
      "\n",
      "Epoch %d: train loss %f 109 0.37866443693637847\n",
      "Epoch 109: val loss 0.560048\n",
      "\n",
      "Epoch %d: train loss %f 110 0.34447515308856963\n",
      "Epoch 110: val loss 0.564058\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3708886444568634\n",
      "Epoch 111: val loss 0.609098\n",
      "\n",
      "Epoch %d: train loss %f 112 0.3866813749074936\n",
      "Epoch 112: val loss 0.604002\n",
      "\n",
      "Epoch %d: train loss %f 113 0.3467573642730713\n",
      "Epoch 113: val loss 0.588400\n",
      "\n",
      "Epoch %d: train loss %f 114 0.342061722278595\n",
      "Epoch 114: val loss 0.568085\n",
      "\n",
      "Epoch %d: train loss %f 115 0.36581486016511916\n",
      "Epoch 115: val loss 0.596137\n",
      "\n",
      "Epoch %d: train loss %f 116 0.3175104856491089\n",
      "Epoch 116: val loss 0.606886\n",
      "\n",
      "Epoch %d: train loss %f 117 0.35424927771091463\n",
      "Epoch 117: val loss 0.584942\n",
      "\n",
      "Epoch %d: train loss %f 118 0.36462674885988233\n",
      "Epoch 118: val loss 0.578757\n",
      "\n",
      "Epoch %d: train loss %f 119 0.34630212783813474\n",
      "Epoch 119: val loss 0.596664\n",
      "\n",
      "Epoch %d: train loss %f 120 0.33254419565200805\n",
      "Epoch 120: val loss 0.602025\n",
      "\n",
      "Epoch %d: train loss %f 121 0.3315508455038071\n",
      "Epoch 121: val loss 0.602193\n",
      "\n",
      "Epoch %d: train loss %f 122 0.36500050127506256\n",
      "Epoch 122: val loss 0.585381\n",
      "\n",
      "Epoch %d: train loss %f 123 0.3555305987596512\n",
      "Epoch 123: val loss 0.579517\n",
      "\n",
      "Epoch %d: train loss %f 124 0.34620802402496337\n",
      "Epoch 124: val loss 0.609189\n",
      "\n",
      "Epoch %d: train loss %f 125 0.3744866758584976\n",
      "Epoch 125: val loss 0.613816\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3116996377706528\n",
      "Epoch 126: val loss 0.585783\n",
      "\n",
      "Epoch %d: train loss %f 127 0.3665498659014702\n",
      "Epoch 127: val loss 0.578956\n",
      "\n",
      "Epoch %d: train loss %f 128 0.36824012398719785\n",
      "Epoch 128: val loss 0.597004\n",
      "\n",
      "Epoch %d: train loss %f 129 0.3175049602985382\n",
      "Epoch 129: val loss 0.622157\n",
      "\n",
      "Epoch %d: train loss %f 130 0.3340774968266487\n",
      "Epoch 130: val loss 0.606102\n",
      "\n",
      "Epoch %d: train loss %f 131 0.33916083425283433\n",
      "Epoch 131: val loss 0.598471\n",
      "\n",
      "Epoch %d: train loss %f 132 0.31132626086473464\n",
      "Epoch 132: val loss 0.626471\n",
      "\n",
      "Epoch %d: train loss %f 133 0.33772272169589995\n",
      "Epoch 133: val loss 0.633662\n",
      "\n",
      "Epoch %d: train loss %f 134 0.3406407371163368\n",
      "Epoch 134: val loss 0.578316\n",
      "\n",
      "Epoch %d: train loss %f 135 0.35538976192474364\n",
      "Epoch 135: val loss 0.594808\n",
      "\n",
      "Epoch %d: train loss %f 136 0.36555935591459277\n",
      "Epoch 136: val loss 0.605829\n",
      "\n",
      "Epoch %d: train loss %f 137 0.3464105173945427\n",
      "Epoch 137: val loss 0.616094\n",
      "\n",
      "Epoch %d: train loss %f 138 0.29060278087854385\n",
      "Epoch 138: val loss 0.593539\n",
      "\n",
      "Epoch %d: train loss %f 139 0.3470968246459961\n",
      "Epoch 139: val loss 0.612550\n",
      "\n",
      "Epoch %d: train loss %f 140 0.341117325425148\n",
      "Epoch 140: val loss 0.601224\n",
      "\n",
      "Epoch %d: train loss %f 141 0.34324919283390043\n",
      "Epoch 141: val loss 0.624031\n",
      "\n",
      "Epoch %d: train loss %f 142 0.29181329607963563\n",
      "Epoch 142: val loss 0.613036\n",
      "\n",
      "Epoch %d: train loss %f 143 0.32471352368593215\n",
      "Epoch 143: val loss 0.652279\n",
      "\n",
      "Epoch %d: train loss %f 144 0.3410118162631989\n",
      "Epoch 144: val loss 0.619044\n",
      "\n",
      "Epoch %d: train loss %f 145 0.34694554954767226\n",
      "Epoch 145: val loss 0.654464\n",
      "\n",
      "Epoch %d: train loss %f 146 0.3179103657603264\n",
      "Epoch 146: val loss 0.627618\n",
      "\n",
      "Epoch %d: train loss %f 147 0.33812808245420456\n",
      "Epoch 147: val loss 0.666148\n",
      "\n",
      "Epoch %d: train loss %f 148 0.32008944600820544\n",
      "Epoch 148: val loss 0.651436\n",
      "\n",
      "Epoch %d: train loss %f 149 0.36061377376317977\n",
      "Epoch 149: val loss 0.680059\n",
      "\n",
      "Epoch %d: train loss %f 150 0.3276234447956085\n",
      "Epoch 150: val loss 0.644186\n",
      "\n",
      "Epoch %d: train loss %f 151 0.3231861338019371\n",
      "Epoch 151: val loss 0.638762\n",
      "\n",
      "Epoch %d: train loss %f 152 0.31719946712255476\n",
      "Epoch 152: val loss 0.634621\n",
      "\n",
      "Epoch %d: train loss %f 153 0.2744981437921524\n",
      "Epoch 153: val loss 0.679582\n",
      "\n",
      "Epoch %d: train loss %f 154 0.3022809445858002\n",
      "Epoch 154: val loss 0.634069\n",
      "\n",
      "Epoch %d: train loss %f 155 0.31769636571407317\n",
      "Epoch 155: val loss 0.650912\n",
      "\n",
      "Epoch %d: train loss %f 156 0.3059589430689812\n",
      "Epoch 156: val loss 0.660494\n",
      "\n",
      "Epoch %d: train loss %f 157 0.29915173202753065\n",
      "Epoch 157: val loss 0.650206\n",
      "\n",
      "Epoch %d: train loss %f 158 0.34584623128175734\n",
      "Epoch 158: val loss 0.642811\n",
      "\n",
      "Epoch %d: train loss %f 159 0.30307257771492\n",
      "Epoch 159: val loss 0.672545\n",
      "\n",
      "Epoch %d: train loss %f 160 0.31588922888040544\n",
      "Epoch 160: val loss 0.645790\n",
      "\n",
      "Epoch %d: train loss %f 161 0.3121670514345169\n",
      "Epoch 161: val loss 0.640890\n",
      "\n",
      "Epoch %d: train loss %f 162 0.3098578616976738\n",
      "Epoch 162: val loss 0.689664\n",
      "\n",
      "Epoch %d: train loss %f 163 0.2974637269973755\n",
      "Epoch 163: val loss 0.681448\n",
      "\n",
      "Epoch %d: train loss %f 164 0.346510449051857\n",
      "Epoch 164: val loss 0.704744\n",
      "\n",
      "Epoch %d: train loss %f 165 0.2997653305530548\n",
      "Epoch 165: val loss 0.665440\n",
      "\n",
      "Epoch %d: train loss %f 166 0.33120676428079604\n",
      "Epoch 166: val loss 0.643948\n",
      "\n",
      "Epoch %d: train loss %f 167 0.318201807141304\n",
      "Epoch 167: val loss 0.661966\n",
      "\n",
      "Epoch %d: train loss %f 168 0.34600901752710345\n",
      "Epoch 168: val loss 0.669705\n",
      "\n",
      "Epoch %d: train loss %f 169 0.307424983382225\n",
      "Epoch 169: val loss 0.646956\n",
      "\n",
      "Epoch %d: train loss %f 170 0.3195295438170433\n",
      "Epoch 170: val loss 0.647129\n",
      "\n",
      "Epoch %d: train loss %f 171 0.2973377540707588\n",
      "Epoch 171: val loss 0.678333\n",
      "\n",
      "Epoch %d: train loss %f 172 0.2769590333104134\n",
      "Epoch 172: val loss 0.666228\n",
      "\n",
      "Epoch %d: train loss %f 173 0.301191046833992\n",
      "Epoch 173: val loss 0.688364\n",
      "\n",
      "Epoch %d: train loss %f 174 0.329794080555439\n",
      "Epoch 174: val loss 0.673351\n",
      "\n",
      "Epoch %d: train loss %f 175 0.33430264741182325\n",
      "Epoch 175: val loss 0.717941\n",
      "\n",
      "Epoch %d: train loss %f 176 0.35084218978881837\n",
      "Epoch 176: val loss 0.694989\n",
      "\n",
      "Epoch %d: train loss %f 177 0.3417453095316887\n",
      "Epoch 177: val loss 0.672458\n",
      "\n",
      "Epoch %d: train loss %f 178 0.26715474426746366\n",
      "Epoch 178: val loss 0.685676\n",
      "\n",
      "Epoch %d: train loss %f 179 0.26633708626031877\n",
      "Epoch 179: val loss 0.703797\n",
      "\n",
      "Epoch %d: train loss %f 180 0.2799155876040459\n",
      "Epoch 180: val loss 0.681921\n",
      "\n",
      "Epoch %d: train loss %f 181 0.3092415854334831\n",
      "Epoch 181: val loss 0.700424\n",
      "\n",
      "Epoch %d: train loss %f 182 0.2672395557165146\n",
      "Epoch 182: val loss 0.734138\n",
      "\n",
      "Epoch %d: train loss %f 183 0.2730483368039131\n",
      "Epoch 183: val loss 0.713785\n",
      "\n",
      "Epoch %d: train loss %f 184 0.2721630081534386\n",
      "Epoch 184: val loss 0.727661\n",
      "\n",
      "Epoch %d: train loss %f 185 0.31915726512670517\n",
      "Epoch 185: val loss 0.749318\n",
      "\n",
      "Epoch %d: train loss %f 186 0.3093071788549423\n",
      "Epoch 186: val loss 0.696405\n",
      "\n",
      "Epoch %d: train loss %f 187 0.28399756401777265\n",
      "Epoch 187: val loss 0.721562\n",
      "\n",
      "Epoch %d: train loss %f 188 0.3044082835316658\n",
      "Epoch 188: val loss 0.724283\n",
      "\n",
      "Epoch %d: train loss %f 189 0.2819455198943615\n",
      "Epoch 189: val loss 0.764822\n",
      "\n",
      "Epoch %d: train loss %f 190 0.2770614117383957\n",
      "Epoch 190: val loss 0.710395\n",
      "\n",
      "Epoch %d: train loss %f 191 0.24131790399551392\n",
      "Epoch 191: val loss 0.720850\n",
      "\n",
      "Epoch %d: train loss %f 192 0.3283605664968491\n",
      "Epoch 192: val loss 0.746933\n",
      "\n",
      "Epoch %d: train loss %f 193 0.3094267934560776\n",
      "Epoch 193: val loss 0.762797\n",
      "\n",
      "Epoch %d: train loss %f 194 0.2974285170435905\n",
      "Epoch 194: val loss 0.746098\n",
      "\n",
      "Epoch %d: train loss %f 195 0.2641621842980385\n",
      "Epoch 195: val loss 0.729302\n",
      "\n",
      "Epoch %d: train loss %f 196 0.3021784469485283\n",
      "Epoch 196: val loss 0.708779\n",
      "\n",
      "Epoch %d: train loss %f 197 0.3333726435899734\n",
      "Epoch 197: val loss 0.717636\n",
      "\n",
      "Epoch %d: train loss %f 198 0.30120488405227663\n",
      "Epoch 198: val loss 0.696993\n",
      "\n",
      "Epoch %d: train loss %f 199 0.27729782164096833\n",
      "Epoch 199: val loss 0.708737\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6767061412334442\n",
      "Epoch 0: val loss 0.680640\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6601261138916016\n",
      "Epoch 1: val loss 0.675964\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6457715034484863\n",
      "Epoch 2: val loss 0.665828\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6068394482135773\n",
      "Epoch 3: val loss 0.648101\n",
      "\n",
      "Epoch %d: train loss %f 4 0.5789378166198731\n",
      "Epoch 4: val loss 0.624706\n",
      "\n",
      "Epoch %d: train loss %f 5 0.5428628772497177\n",
      "Epoch 5: val loss 0.608549\n",
      "\n",
      "Epoch %d: train loss %f 6 0.5198098719120026\n",
      "Epoch 6: val loss 0.602077\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5257543683052063\n",
      "Epoch 7: val loss 0.596330\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5250603437423706\n",
      "Epoch 8: val loss 0.594636\n",
      "\n",
      "Epoch %d: train loss %f 9 0.513838741183281\n",
      "Epoch 9: val loss 0.601308\n",
      "\n",
      "Epoch %d: train loss %f 10 0.49342499673366547\n",
      "Epoch 10: val loss 0.600374\n",
      "\n",
      "Epoch %d: train loss %f 11 0.49391408264636993\n",
      "Epoch 11: val loss 0.601857\n",
      "\n",
      "Epoch %d: train loss %f 12 0.48631480932235716\n",
      "Epoch 12: val loss 0.596334\n",
      "\n",
      "Epoch %d: train loss %f 13 0.48801159858703613\n",
      "Epoch 13: val loss 0.591960\n",
      "\n",
      "Epoch %d: train loss %f 14 0.48805451989173887\n",
      "Epoch 14: val loss 0.599096\n",
      "\n",
      "Epoch %d: train loss %f 15 0.4885576993227005\n",
      "Epoch 15: val loss 0.607709\n",
      "\n",
      "Epoch %d: train loss %f 16 0.49018227458000185\n",
      "Epoch 16: val loss 0.588744\n",
      "\n",
      "Epoch %d: train loss %f 17 0.4693871706724167\n",
      "Epoch 17: val loss 0.601376\n",
      "\n",
      "Epoch %d: train loss %f 18 0.4889084011316299\n",
      "Epoch 18: val loss 0.587895\n",
      "\n",
      "Epoch %d: train loss %f 19 0.4945949465036392\n",
      "Epoch 19: val loss 0.581908\n",
      "\n",
      "Epoch %d: train loss %f 20 0.44715179204940797\n",
      "Epoch 20: val loss 0.581531\n",
      "\n",
      "Epoch %d: train loss %f 21 0.47658721506595614\n",
      "Epoch 21: val loss 0.576225\n",
      "\n",
      "Epoch %d: train loss %f 22 0.43435940742492674\n",
      "Epoch 22: val loss 0.606108\n",
      "\n",
      "Epoch %d: train loss %f 23 0.45846825540065766\n",
      "Epoch 23: val loss 0.583842\n",
      "\n",
      "Epoch %d: train loss %f 24 0.4561959505081177\n",
      "Epoch 24: val loss 0.581271\n",
      "\n",
      "Epoch %d: train loss %f 25 0.45921250283718107\n",
      "Epoch 25: val loss 0.578333\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4476660072803497\n",
      "Epoch 26: val loss 0.560955\n",
      "\n",
      "Epoch %d: train loss %f 27 0.4562841594219208\n",
      "Epoch 27: val loss 0.574383\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4528676956892014\n",
      "Epoch 28: val loss 0.609789\n",
      "\n",
      "Epoch %d: train loss %f 29 0.45647539794445036\n",
      "Epoch 29: val loss 0.608137\n",
      "\n",
      "Epoch %d: train loss %f 30 0.45040434300899507\n",
      "Epoch 30: val loss 0.574351\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4263099193572998\n",
      "Epoch 31: val loss 0.613154\n",
      "\n",
      "Epoch %d: train loss %f 32 0.41275453865528106\n",
      "Epoch 32: val loss 0.576417\n",
      "\n",
      "Epoch %d: train loss %f 33 0.3979282557964325\n",
      "Epoch 33: val loss 0.610182\n",
      "\n",
      "Epoch %d: train loss %f 34 0.441817107796669\n",
      "Epoch 34: val loss 0.584278\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4109218567609787\n",
      "Epoch 35: val loss 0.591395\n",
      "\n",
      "Epoch %d: train loss %f 36 0.3987251460552216\n",
      "Epoch 36: val loss 0.623581\n",
      "\n",
      "Epoch %d: train loss %f 37 0.40707783997058866\n",
      "Epoch 37: val loss 0.601103\n",
      "\n",
      "Epoch %d: train loss %f 38 0.4033507540822029\n",
      "Epoch 38: val loss 0.629867\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4106937527656555\n",
      "Epoch 39: val loss 0.623921\n",
      "\n",
      "Epoch %d: train loss %f 40 0.374213832616806\n",
      "Epoch 40: val loss 0.604029\n",
      "\n",
      "Epoch %d: train loss %f 41 0.42269608974456785\n",
      "Epoch 41: val loss 0.653475\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4226381480693817\n",
      "Epoch 42: val loss 0.684537\n",
      "\n",
      "Epoch %d: train loss %f 43 0.3905594706535339\n",
      "Epoch 43: val loss 0.611531\n",
      "\n",
      "Epoch %d: train loss %f 44 0.4000033766031265\n",
      "Epoch 44: val loss 0.638731\n",
      "\n",
      "Epoch %d: train loss %f 45 0.37902249991893766\n",
      "Epoch 45: val loss 0.646473\n",
      "\n",
      "Epoch %d: train loss %f 46 0.377428212761879\n",
      "Epoch 46: val loss 0.658532\n",
      "\n",
      "Epoch %d: train loss %f 47 0.43048508614301684\n",
      "Epoch 47: val loss 0.626267\n",
      "\n",
      "Epoch %d: train loss %f 48 0.36026066839694976\n",
      "Epoch 48: val loss 0.690515\n",
      "\n",
      "Epoch %d: train loss %f 49 0.3987100005149841\n",
      "Epoch 49: val loss 0.653904\n",
      "\n",
      "Epoch %d: train loss %f 50 0.39715232104063036\n",
      "Epoch 50: val loss 0.672260\n",
      "\n",
      "Epoch %d: train loss %f 51 0.38896919041872025\n",
      "Epoch 51: val loss 0.670957\n",
      "\n",
      "Epoch %d: train loss %f 52 0.3546323597431183\n",
      "Epoch 52: val loss 0.682362\n",
      "\n",
      "Epoch %d: train loss %f 53 0.31824238151311873\n",
      "Epoch 53: val loss 0.707286\n",
      "\n",
      "Epoch %d: train loss %f 54 0.3498542755842209\n",
      "Epoch 54: val loss 0.692244\n",
      "\n",
      "Epoch %d: train loss %f 55 0.362964329123497\n",
      "Epoch 55: val loss 0.699855\n",
      "\n",
      "Epoch %d: train loss %f 56 0.37871685028076174\n",
      "Epoch 56: val loss 0.745710\n",
      "\n",
      "Epoch %d: train loss %f 57 0.3646356001496315\n",
      "Epoch 57: val loss 0.744573\n",
      "\n",
      "Epoch %d: train loss %f 58 0.35553606450557707\n",
      "Epoch 58: val loss 0.678681\n",
      "\n",
      "Epoch %d: train loss %f 59 0.3550002157688141\n",
      "Epoch 59: val loss 0.754862\n",
      "\n",
      "Epoch %d: train loss %f 60 0.3434921205043793\n",
      "Epoch 60: val loss 0.722415\n",
      "\n",
      "Epoch %d: train loss %f 61 0.3390138059854507\n",
      "Epoch 61: val loss 0.699090\n",
      "\n",
      "Epoch %d: train loss %f 62 0.33467754870653155\n",
      "Epoch 62: val loss 0.740035\n",
      "\n",
      "Epoch %d: train loss %f 63 0.36618811786174776\n",
      "Epoch 63: val loss 0.694488\n",
      "\n",
      "Epoch %d: train loss %f 64 0.35211102962493895\n",
      "Epoch 64: val loss 0.729765\n",
      "\n",
      "Epoch %d: train loss %f 65 0.34115194231271745\n",
      "Epoch 65: val loss 0.733270\n",
      "\n",
      "Epoch %d: train loss %f 66 0.34200750589370726\n",
      "Epoch 66: val loss 0.776060\n",
      "\n",
      "Epoch %d: train loss %f 67 0.32616398483514786\n",
      "Epoch 67: val loss 0.748067\n",
      "\n",
      "Epoch %d: train loss %f 68 0.3468879535794258\n",
      "Epoch 68: val loss 0.742256\n",
      "\n",
      "Epoch %d: train loss %f 69 0.3107855021953583\n",
      "Epoch 69: val loss 0.780431\n",
      "\n",
      "Epoch %d: train loss %f 70 0.338298599421978\n",
      "Epoch 70: val loss 0.765220\n",
      "\n",
      "Epoch %d: train loss %f 71 0.33227520883083345\n",
      "Epoch 71: val loss 0.756585\n",
      "\n",
      "Epoch %d: train loss %f 72 0.29679699838161466\n",
      "Epoch 72: val loss 0.816755\n",
      "\n",
      "Epoch %d: train loss %f 73 0.31624774634838104\n",
      "Epoch 73: val loss 0.807103\n",
      "\n",
      "Epoch %d: train loss %f 74 0.3398331686854362\n",
      "Epoch 74: val loss 0.815545\n",
      "\n",
      "Epoch %d: train loss %f 75 0.3165459454059601\n",
      "Epoch 75: val loss 0.760579\n",
      "\n",
      "Epoch %d: train loss %f 76 0.3511276826262474\n",
      "Epoch 76: val loss 0.794349\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3174812138080597\n",
      "Epoch 77: val loss 0.818001\n",
      "\n",
      "Epoch %d: train loss %f 78 0.2982840955257416\n",
      "Epoch 78: val loss 0.769873\n",
      "\n",
      "Epoch %d: train loss %f 79 0.3062912106513977\n",
      "Epoch 79: val loss 0.849877\n",
      "\n",
      "Epoch %d: train loss %f 80 0.3479763329029083\n",
      "Epoch 80: val loss 0.820647\n",
      "\n",
      "Epoch %d: train loss %f 81 0.32333405762910844\n",
      "Epoch 81: val loss 0.823474\n",
      "\n",
      "Epoch %d: train loss %f 82 0.31753685176372526\n",
      "Epoch 82: val loss 0.838853\n",
      "\n",
      "Epoch %d: train loss %f 83 0.29080313742160796\n",
      "Epoch 83: val loss 0.833456\n",
      "\n",
      "Epoch %d: train loss %f 84 0.34115873128175733\n",
      "Epoch 84: val loss 0.838709\n",
      "\n",
      "Epoch %d: train loss %f 85 0.31539242565631864\n",
      "Epoch 85: val loss 0.831897\n",
      "\n",
      "Epoch %d: train loss %f 86 0.30692043751478193\n",
      "Epoch 86: val loss 0.799496\n",
      "\n",
      "Epoch %d: train loss %f 87 0.28467279076576235\n",
      "Epoch 87: val loss 0.813892\n",
      "\n",
      "Epoch %d: train loss %f 88 0.29543157666921616\n",
      "Epoch 88: val loss 0.835944\n",
      "\n",
      "Epoch %d: train loss %f 89 0.3081960305571556\n",
      "Epoch 89: val loss 0.848866\n",
      "\n",
      "Epoch %d: train loss %f 90 0.2774344548583031\n",
      "Epoch 90: val loss 0.901757\n",
      "\n",
      "Epoch %d: train loss %f 91 0.2667411655187607\n",
      "Epoch 91: val loss 0.863781\n",
      "\n",
      "Epoch %d: train loss %f 92 0.27093329206109046\n",
      "Epoch 92: val loss 0.885480\n",
      "\n",
      "Epoch %d: train loss %f 93 0.2899098455905914\n",
      "Epoch 93: val loss 0.892363\n",
      "\n",
      "Epoch %d: train loss %f 94 0.31327118873596194\n",
      "Epoch 94: val loss 0.887042\n",
      "\n",
      "Epoch %d: train loss %f 95 0.28809095472097396\n",
      "Epoch 95: val loss 0.867930\n",
      "\n",
      "Epoch %d: train loss %f 96 0.2816912800073624\n",
      "Epoch 96: val loss 0.861638\n",
      "\n",
      "Epoch %d: train loss %f 97 0.288377883285284\n",
      "Epoch 97: val loss 0.884984\n",
      "\n",
      "Epoch %d: train loss %f 98 0.32469521909952165\n",
      "Epoch 98: val loss 0.945390\n",
      "\n",
      "Epoch %d: train loss %f 99 0.28405442237854006\n",
      "Epoch 99: val loss 0.925640\n",
      "\n",
      "Epoch %d: train loss %f 100 0.30401487052440646\n",
      "Epoch 100: val loss 0.865976\n",
      "\n",
      "Epoch %d: train loss %f 101 0.29324577301740645\n",
      "Epoch 101: val loss 0.923673\n",
      "\n",
      "Epoch %d: train loss %f 102 0.27345955222845075\n",
      "Epoch 102: val loss 0.948822\n",
      "\n",
      "Epoch %d: train loss %f 103 0.2735817976295948\n",
      "Epoch 103: val loss 0.862578\n",
      "\n",
      "Epoch %d: train loss %f 104 0.2428361475467682\n",
      "Epoch 104: val loss 0.895860\n",
      "\n",
      "Epoch %d: train loss %f 105 0.30853108912706373\n",
      "Epoch 105: val loss 0.936902\n",
      "\n",
      "Epoch %d: train loss %f 106 0.25961650609970094\n",
      "Epoch 106: val loss 0.976716\n",
      "\n",
      "Epoch %d: train loss %f 107 0.2887865424156189\n",
      "Epoch 107: val loss 0.916602\n",
      "\n",
      "Epoch %d: train loss %f 108 0.2655429497361183\n",
      "Epoch 108: val loss 1.000969\n",
      "\n",
      "Epoch %d: train loss %f 109 0.2427899144589901\n",
      "Epoch 109: val loss 1.021091\n",
      "\n",
      "Epoch %d: train loss %f 110 0.27378383874893186\n",
      "Epoch 110: val loss 0.997973\n",
      "\n",
      "Epoch %d: train loss %f 111 0.26303364485502245\n",
      "Epoch 111: val loss 0.953512\n",
      "\n",
      "Epoch %d: train loss %f 112 0.2583462581038475\n",
      "Epoch 112: val loss 1.016323\n",
      "\n",
      "Epoch %d: train loss %f 113 0.27226950973272324\n",
      "Epoch 113: val loss 1.036809\n",
      "\n",
      "Epoch %d: train loss %f 114 0.29337447732686994\n",
      "Epoch 114: val loss 0.940656\n",
      "\n",
      "Epoch %d: train loss %f 115 0.2791193962097168\n",
      "Epoch 115: val loss 0.911003\n",
      "\n",
      "Epoch %d: train loss %f 116 0.27831327021121977\n",
      "Epoch 116: val loss 0.919306\n",
      "\n",
      "Epoch %d: train loss %f 117 0.2725238770246506\n",
      "Epoch 117: val loss 0.941891\n",
      "\n",
      "Epoch %d: train loss %f 118 0.29546119421720507\n",
      "Epoch 118: val loss 1.009803\n",
      "\n",
      "Epoch %d: train loss %f 119 0.23038249015808104\n",
      "Epoch 119: val loss 0.958134\n",
      "\n",
      "Epoch %d: train loss %f 120 0.27228138148784636\n",
      "Epoch 120: val loss 1.009965\n",
      "\n",
      "Epoch %d: train loss %f 121 0.2731219053268433\n",
      "Epoch 121: val loss 1.000158\n",
      "\n",
      "Epoch %d: train loss %f 122 0.2324783831834793\n",
      "Epoch 122: val loss 1.033436\n",
      "\n",
      "Epoch %d: train loss %f 123 0.31040582060813904\n",
      "Epoch 123: val loss 1.034569\n",
      "\n",
      "Epoch %d: train loss %f 124 0.2682679682970047\n",
      "Epoch 124: val loss 1.040130\n",
      "\n",
      "Epoch %d: train loss %f 125 0.24659072384238243\n",
      "Epoch 125: val loss 0.994119\n",
      "\n",
      "Epoch %d: train loss %f 126 0.26410537511110305\n",
      "Epoch 126: val loss 0.960107\n",
      "\n",
      "Epoch %d: train loss %f 127 0.25733743906021117\n",
      "Epoch 127: val loss 0.978564\n",
      "\n",
      "Epoch %d: train loss %f 128 0.2683385327458382\n",
      "Epoch 128: val loss 1.007805\n",
      "\n",
      "Epoch %d: train loss %f 129 0.25344770699739455\n",
      "Epoch 129: val loss 1.012326\n",
      "\n",
      "Epoch %d: train loss %f 130 0.28611475229263306\n",
      "Epoch 130: val loss 0.978115\n",
      "\n",
      "Epoch %d: train loss %f 131 0.26599601954221724\n",
      "Epoch 131: val loss 0.990720\n",
      "\n",
      "Epoch %d: train loss %f 132 0.28969746083021164\n",
      "Epoch 132: val loss 1.026078\n",
      "\n",
      "Epoch %d: train loss %f 133 0.26247917115688324\n",
      "Epoch 133: val loss 1.030709\n",
      "\n",
      "Epoch %d: train loss %f 134 0.2898763656616211\n",
      "Epoch 134: val loss 0.992056\n",
      "\n",
      "Epoch %d: train loss %f 135 0.22072802856564522\n",
      "Epoch 135: val loss 0.975351\n",
      "\n",
      "Epoch %d: train loss %f 136 0.27679402083158494\n",
      "Epoch 136: val loss 0.939256\n",
      "\n",
      "Epoch %d: train loss %f 137 0.2517497278749943\n",
      "Epoch 137: val loss 1.075101\n",
      "\n",
      "Epoch %d: train loss %f 138 0.2707338809967041\n",
      "Epoch 138: val loss 1.084046\n",
      "\n",
      "Epoch %d: train loss %f 139 0.23526232466101646\n",
      "Epoch 139: val loss 1.013754\n",
      "\n",
      "Epoch %d: train loss %f 140 0.2616288959980011\n",
      "Epoch 140: val loss 0.997996\n",
      "\n",
      "Epoch %d: train loss %f 141 0.260019363462925\n",
      "Epoch 141: val loss 0.985037\n",
      "\n",
      "Epoch %d: train loss %f 142 0.2574548631906509\n",
      "Epoch 142: val loss 1.038753\n",
      "\n",
      "Epoch %d: train loss %f 143 0.24547886848449707\n",
      "Epoch 143: val loss 1.103920\n",
      "\n",
      "Epoch %d: train loss %f 144 0.27753050401806834\n",
      "Epoch 144: val loss 1.018711\n",
      "\n",
      "Epoch %d: train loss %f 145 0.22378416508436202\n",
      "Epoch 145: val loss 0.981453\n",
      "\n",
      "Epoch %d: train loss %f 146 0.2971518009901047\n",
      "Epoch 146: val loss 1.029335\n",
      "\n",
      "Epoch %d: train loss %f 147 0.2563575476408005\n",
      "Epoch 147: val loss 1.057657\n",
      "\n",
      "Epoch %d: train loss %f 148 0.2664993032813072\n",
      "Epoch 148: val loss 1.080433\n",
      "\n",
      "Epoch %d: train loss %f 149 0.25669076591730117\n",
      "Epoch 149: val loss 1.131046\n",
      "\n",
      "Epoch %d: train loss %f 150 0.2191666044294834\n",
      "Epoch 150: val loss 1.086983\n",
      "\n",
      "Epoch %d: train loss %f 151 0.21068924069404601\n",
      "Epoch 151: val loss 1.070860\n",
      "\n",
      "Epoch %d: train loss %f 152 0.2191058076918125\n",
      "Epoch 152: val loss 1.022303\n",
      "\n",
      "Epoch %d: train loss %f 153 0.2711938485503197\n",
      "Epoch 153: val loss 1.045310\n",
      "\n",
      "Epoch %d: train loss %f 154 0.24163951873779296\n",
      "Epoch 154: val loss 1.080604\n",
      "\n",
      "Epoch %d: train loss %f 155 0.2197807475924492\n",
      "Epoch 155: val loss 1.053673\n",
      "\n",
      "Epoch %d: train loss %f 156 0.22918523475527763\n",
      "Epoch 156: val loss 1.076959\n",
      "\n",
      "Epoch %d: train loss %f 157 0.2511637985706329\n",
      "Epoch 157: val loss 1.111967\n",
      "\n",
      "Epoch %d: train loss %f 158 0.25420782119035723\n",
      "Epoch 158: val loss 1.149122\n",
      "\n",
      "Epoch %d: train loss %f 159 0.24764225929975509\n",
      "Epoch 159: val loss 1.126291\n",
      "\n",
      "Epoch %d: train loss %f 160 0.21642366349697112\n",
      "Epoch 160: val loss 1.040133\n",
      "\n",
      "Epoch %d: train loss %f 161 0.25614295452833175\n",
      "Epoch 161: val loss 1.037599\n",
      "\n",
      "Epoch %d: train loss %f 162 0.2710553802549839\n",
      "Epoch 162: val loss 1.068659\n",
      "\n",
      "Epoch %d: train loss %f 163 0.24702927395701407\n",
      "Epoch 163: val loss 1.103891\n",
      "\n",
      "Epoch %d: train loss %f 164 0.23168070539832114\n",
      "Epoch 164: val loss 1.190671\n",
      "\n",
      "Epoch %d: train loss %f 165 0.24532893896102906\n",
      "Epoch 165: val loss 1.099348\n",
      "\n",
      "Epoch %d: train loss %f 166 0.21719639003276825\n",
      "Epoch 166: val loss 1.188660\n",
      "\n",
      "Epoch %d: train loss %f 167 0.26503376066684725\n",
      "Epoch 167: val loss 1.140799\n",
      "\n",
      "Epoch %d: train loss %f 168 0.26825738698244095\n",
      "Epoch 168: val loss 1.075138\n",
      "\n",
      "Epoch %d: train loss %f 169 0.24561979621648788\n",
      "Epoch 169: val loss 1.133242\n",
      "\n",
      "Epoch %d: train loss %f 170 0.24249657914042472\n",
      "Epoch 170: val loss 1.052910\n",
      "\n",
      "Epoch %d: train loss %f 171 0.22463130205869675\n",
      "Epoch 171: val loss 1.052203\n",
      "\n",
      "Epoch %d: train loss %f 172 0.2521224096417427\n",
      "Epoch 172: val loss 1.111639\n",
      "\n",
      "Epoch %d: train loss %f 173 0.26477919071912764\n",
      "Epoch 173: val loss 1.174574\n",
      "\n",
      "Epoch %d: train loss %f 174 0.2751833528280258\n",
      "Epoch 174: val loss 1.146726\n",
      "\n",
      "Epoch %d: train loss %f 175 0.24745972603559493\n",
      "Epoch 175: val loss 1.079101\n",
      "\n",
      "Epoch %d: train loss %f 176 0.2836755633354187\n",
      "Epoch 176: val loss 1.166445\n",
      "\n",
      "Epoch %d: train loss %f 177 0.22095846831798555\n",
      "Epoch 177: val loss 1.185733\n",
      "\n",
      "Epoch %d: train loss %f 178 0.2730266720056534\n",
      "Epoch 178: val loss 1.153614\n",
      "\n",
      "Epoch %d: train loss %f 179 0.2775756672024727\n",
      "Epoch 179: val loss 1.058894\n",
      "\n",
      "Epoch %d: train loss %f 180 0.2405495062470436\n",
      "Epoch 180: val loss 1.088624\n",
      "\n",
      "Epoch %d: train loss %f 181 0.22722506746649743\n",
      "Epoch 181: val loss 1.169322\n",
      "\n",
      "Epoch %d: train loss %f 182 0.29253694862127305\n",
      "Epoch 182: val loss 1.090451\n",
      "\n",
      "Epoch %d: train loss %f 183 0.2471482440829277\n",
      "Epoch 183: val loss 1.123248\n",
      "\n",
      "Epoch %d: train loss %f 184 0.2185549817979336\n",
      "Epoch 184: val loss 1.073543\n",
      "\n",
      "Epoch %d: train loss %f 185 0.2421335496008396\n",
      "Epoch 185: val loss 1.002524\n",
      "\n",
      "Epoch %d: train loss %f 186 0.24014133363962173\n",
      "Epoch 186: val loss 1.110787\n",
      "\n",
      "Epoch %d: train loss %f 187 0.22808756828308105\n",
      "Epoch 187: val loss 1.118538\n",
      "\n",
      "Epoch %d: train loss %f 188 0.23408213555812835\n",
      "Epoch 188: val loss 1.103837\n",
      "\n",
      "Epoch %d: train loss %f 189 0.21351434476673603\n",
      "Epoch 189: val loss 1.143769\n",
      "\n",
      "Epoch %d: train loss %f 190 0.228813024610281\n",
      "Epoch 190: val loss 1.083984\n",
      "\n",
      "Epoch %d: train loss %f 191 0.23842052668333052\n",
      "Epoch 191: val loss 1.030520\n",
      "\n",
      "Epoch %d: train loss %f 192 0.21661556661128997\n",
      "Epoch 192: val loss 1.189029\n",
      "\n",
      "Epoch %d: train loss %f 193 0.2425653949379921\n",
      "Epoch 193: val loss 1.210037\n",
      "\n",
      "Epoch %d: train loss %f 194 0.21013454496860504\n",
      "Epoch 194: val loss 1.179740\n",
      "\n",
      "Epoch %d: train loss %f 195 0.2145529121160507\n",
      "Epoch 195: val loss 1.104402\n",
      "\n",
      "Epoch %d: train loss %f 196 0.19471570067107677\n",
      "Epoch 196: val loss 1.061387\n",
      "\n",
      "Epoch %d: train loss %f 197 0.18446500413119793\n",
      "Epoch 197: val loss 1.125423\n",
      "\n",
      "Epoch %d: train loss %f 198 0.20567402392625808\n",
      "Epoch 198: val loss 1.204430\n",
      "\n",
      "Epoch %d: train loss %f 199 0.22022624760866166\n",
      "Epoch 199: val loss 1.118514\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6906593859195709\n",
      "Epoch 0: val loss 0.689815\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6839907109737396\n",
      "Epoch 1: val loss 0.685148\n",
      "\n",
      "Epoch %d: train loss %f 2 0.674017870426178\n",
      "Epoch 2: val loss 0.677125\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6628952503204346\n",
      "Epoch 3: val loss 0.659976\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6394058287143707\n",
      "Epoch 4: val loss 0.628084\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6098000943660736\n",
      "Epoch 5: val loss 0.580314\n",
      "\n",
      "Epoch %d: train loss %f 6 0.5786867618560791\n",
      "Epoch 6: val loss 0.523334\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5649665981531143\n",
      "Epoch 7: val loss 0.494965\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5571734726428985\n",
      "Epoch 8: val loss 0.485230\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5309761822223663\n",
      "Epoch 9: val loss 0.478090\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5212567865848541\n",
      "Epoch 10: val loss 0.485667\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5327516317367553\n",
      "Epoch 11: val loss 0.489104\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5338176995515823\n",
      "Epoch 12: val loss 0.481769\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5358576118946076\n",
      "Epoch 13: val loss 0.478128\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5175781965255737\n",
      "Epoch 14: val loss 0.481397\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5258556842803955\n",
      "Epoch 15: val loss 0.466832\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5311578989028931\n",
      "Epoch 16: val loss 0.475398\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5179180949926376\n",
      "Epoch 17: val loss 0.498821\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5012078404426574\n",
      "Epoch 18: val loss 0.471194\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5275679498910903\n",
      "Epoch 19: val loss 0.469959\n",
      "\n",
      "Epoch %d: train loss %f 20 0.4882160246372223\n",
      "Epoch 20: val loss 0.466629\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5160501360893249\n",
      "Epoch 21: val loss 0.466927\n",
      "\n",
      "Epoch %d: train loss %f 22 0.48936379253864287\n",
      "Epoch 22: val loss 0.477387\n",
      "\n",
      "Epoch %d: train loss %f 23 0.4940548688173294\n",
      "Epoch 23: val loss 0.464850\n",
      "\n",
      "Epoch %d: train loss %f 24 0.5144357085227966\n",
      "Epoch 24: val loss 0.473992\n",
      "\n",
      "Epoch %d: train loss %f 25 0.503512442111969\n",
      "Epoch 25: val loss 0.462689\n",
      "\n",
      "Epoch %d: train loss %f 26 0.5021666258573532\n",
      "Epoch 26: val loss 0.466277\n",
      "\n",
      "Epoch %d: train loss %f 27 0.4880886673927307\n",
      "Epoch 27: val loss 0.468463\n",
      "\n",
      "Epoch %d: train loss %f 28 0.48159527480602266\n",
      "Epoch 28: val loss 0.464385\n",
      "\n",
      "Epoch %d: train loss %f 29 0.46591399013996126\n",
      "Epoch 29: val loss 0.448518\n",
      "\n",
      "Epoch %d: train loss %f 30 0.4609537810087204\n",
      "Epoch 30: val loss 0.447710\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4901749104261398\n",
      "Epoch 31: val loss 0.448819\n",
      "\n",
      "Epoch %d: train loss %f 32 0.47142073810100554\n",
      "Epoch 32: val loss 0.451850\n",
      "\n",
      "Epoch %d: train loss %f 33 0.4678262323141098\n",
      "Epoch 33: val loss 0.445654\n",
      "\n",
      "Epoch %d: train loss %f 34 0.4718102037906647\n",
      "Epoch 34: val loss 0.451057\n",
      "\n",
      "Epoch %d: train loss %f 35 0.47044428884983064\n",
      "Epoch 35: val loss 0.449652\n",
      "\n",
      "Epoch %d: train loss %f 36 0.43767014145851135\n",
      "Epoch 36: val loss 0.453381\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4698381036520004\n",
      "Epoch 37: val loss 0.441301\n",
      "\n",
      "Epoch %d: train loss %f 38 0.4525748282670975\n",
      "Epoch 38: val loss 0.443235\n",
      "\n",
      "Epoch %d: train loss %f 39 0.441882386803627\n",
      "Epoch 39: val loss 0.463941\n",
      "\n",
      "Epoch %d: train loss %f 40 0.48511061668395994\n",
      "Epoch 40: val loss 0.437286\n",
      "\n",
      "Epoch %d: train loss %f 41 0.45970902442932127\n",
      "Epoch 41: val loss 0.454174\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4620522528886795\n",
      "Epoch 42: val loss 0.448329\n",
      "\n",
      "Epoch %d: train loss %f 43 0.45718380212783816\n",
      "Epoch 43: val loss 0.454480\n",
      "\n",
      "Epoch %d: train loss %f 44 0.44848535358905794\n",
      "Epoch 44: val loss 0.456556\n",
      "\n",
      "Epoch %d: train loss %f 45 0.46717005372047427\n",
      "Epoch 45: val loss 0.454901\n",
      "\n",
      "Epoch %d: train loss %f 46 0.43975572288036346\n",
      "Epoch 46: val loss 0.444910\n",
      "\n",
      "Epoch %d: train loss %f 47 0.4351430118083954\n",
      "Epoch 47: val loss 0.448939\n",
      "\n",
      "Epoch %d: train loss %f 48 0.446471706032753\n",
      "Epoch 48: val loss 0.458870\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4310284599661827\n",
      "Epoch 49: val loss 0.451561\n",
      "\n",
      "Epoch %d: train loss %f 50 0.45807071030139923\n",
      "Epoch 50: val loss 0.451795\n",
      "\n",
      "Epoch %d: train loss %f 51 0.46749749779701233\n",
      "Epoch 51: val loss 0.477322\n",
      "\n",
      "Epoch %d: train loss %f 52 0.44492039978504183\n",
      "Epoch 52: val loss 0.436039\n",
      "\n",
      "Epoch %d: train loss %f 53 0.437325194478035\n",
      "Epoch 53: val loss 0.454605\n",
      "\n",
      "Epoch %d: train loss %f 54 0.42732736468315125\n",
      "Epoch 54: val loss 0.442550\n",
      "\n",
      "Epoch %d: train loss %f 55 0.4456464499235153\n",
      "Epoch 55: val loss 0.452572\n",
      "\n",
      "Epoch %d: train loss %f 56 0.4653642326593399\n",
      "Epoch 56: val loss 0.464782\n",
      "\n",
      "Epoch %d: train loss %f 57 0.42258902490139005\n",
      "Epoch 57: val loss 0.450743\n",
      "\n",
      "Epoch %d: train loss %f 58 0.43065973818302156\n",
      "Epoch 58: val loss 0.462440\n",
      "\n",
      "Epoch %d: train loss %f 59 0.43941223323345185\n",
      "Epoch 59: val loss 0.459307\n",
      "\n",
      "Epoch %d: train loss %f 60 0.416026371717453\n",
      "Epoch 60: val loss 0.460136\n",
      "\n",
      "Epoch %d: train loss %f 61 0.4346510797739029\n",
      "Epoch 61: val loss 0.477122\n",
      "\n",
      "Epoch %d: train loss %f 62 0.418730702996254\n",
      "Epoch 62: val loss 0.451843\n",
      "\n",
      "Epoch %d: train loss %f 63 0.4385423421859741\n",
      "Epoch 63: val loss 0.455491\n",
      "\n",
      "Epoch %d: train loss %f 64 0.4269422709941864\n",
      "Epoch 64: val loss 0.449040\n",
      "\n",
      "Epoch %d: train loss %f 65 0.42174879312515257\n",
      "Epoch 65: val loss 0.450956\n",
      "\n",
      "Epoch %d: train loss %f 66 0.41596844643354414\n",
      "Epoch 66: val loss 0.450125\n",
      "\n",
      "Epoch %d: train loss %f 67 0.39701799750328065\n",
      "Epoch 67: val loss 0.459021\n",
      "\n",
      "Epoch %d: train loss %f 68 0.4443117558956146\n",
      "Epoch 68: val loss 0.465685\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4089831650257111\n",
      "Epoch 69: val loss 0.477569\n",
      "\n",
      "Epoch %d: train loss %f 70 0.4548275887966156\n",
      "Epoch 70: val loss 0.462579\n",
      "\n",
      "Epoch %d: train loss %f 71 0.3995754823088646\n",
      "Epoch 71: val loss 0.484797\n",
      "\n",
      "Epoch %d: train loss %f 72 0.4267251998186111\n",
      "Epoch 72: val loss 0.460370\n",
      "\n",
      "Epoch %d: train loss %f 73 0.4101539522409439\n",
      "Epoch 73: val loss 0.459837\n",
      "\n",
      "Epoch %d: train loss %f 74 0.41005773544311525\n",
      "Epoch 74: val loss 0.470608\n",
      "\n",
      "Epoch %d: train loss %f 75 0.4084782421588898\n",
      "Epoch 75: val loss 0.471542\n",
      "\n",
      "Epoch %d: train loss %f 76 0.3794479876756668\n",
      "Epoch 76: val loss 0.455158\n",
      "\n",
      "Epoch %d: train loss %f 77 0.4047634661197662\n",
      "Epoch 77: val loss 0.504674\n",
      "\n",
      "Epoch %d: train loss %f 78 0.37676145434379577\n",
      "Epoch 78: val loss 0.489997\n",
      "\n",
      "Epoch %d: train loss %f 79 0.42356243133544924\n",
      "Epoch 79: val loss 0.472495\n",
      "\n",
      "Epoch %d: train loss %f 80 0.4011655405163765\n",
      "Epoch 80: val loss 0.465743\n",
      "\n",
      "Epoch %d: train loss %f 81 0.41044849157333374\n",
      "Epoch 81: val loss 0.467286\n",
      "\n",
      "Epoch %d: train loss %f 82 0.38300427198410036\n",
      "Epoch 82: val loss 0.457547\n",
      "\n",
      "Epoch %d: train loss %f 83 0.41689908504486084\n",
      "Epoch 83: val loss 0.486718\n",
      "\n",
      "Epoch %d: train loss %f 84 0.3932242214679718\n",
      "Epoch 84: val loss 0.485647\n",
      "\n",
      "Epoch %d: train loss %f 85 0.36060331761837006\n",
      "Epoch 85: val loss 0.494694\n",
      "\n",
      "Epoch %d: train loss %f 86 0.4086113214492798\n",
      "Epoch 86: val loss 0.474311\n",
      "\n",
      "Epoch %d: train loss %f 87 0.37378069162368777\n",
      "Epoch 87: val loss 0.450063\n",
      "\n",
      "Epoch %d: train loss %f 88 0.3802971184253693\n",
      "Epoch 88: val loss 0.441172\n",
      "\n",
      "Epoch %d: train loss %f 89 0.39676831364631654\n",
      "Epoch 89: val loss 0.458833\n",
      "\n",
      "Epoch %d: train loss %f 90 0.42229475677013395\n",
      "Epoch 90: val loss 0.479425\n",
      "\n",
      "Epoch %d: train loss %f 91 0.39779074490070343\n",
      "Epoch 91: val loss 0.475325\n",
      "\n",
      "Epoch %d: train loss %f 92 0.3910338133573532\n",
      "Epoch 92: val loss 0.518371\n",
      "\n",
      "Epoch %d: train loss %f 93 0.4149683028459549\n",
      "Epoch 93: val loss 0.463157\n",
      "\n",
      "Epoch %d: train loss %f 94 0.38652364909648895\n",
      "Epoch 94: val loss 0.474496\n",
      "\n",
      "Epoch %d: train loss %f 95 0.40226823687553404\n",
      "Epoch 95: val loss 0.479883\n",
      "\n",
      "Epoch %d: train loss %f 96 0.41799129694700243\n",
      "Epoch 96: val loss 0.450006\n",
      "\n",
      "Epoch %d: train loss %f 97 0.3816267877817154\n",
      "Epoch 97: val loss 0.483666\n",
      "\n",
      "Epoch %d: train loss %f 98 0.37282172590494156\n",
      "Epoch 98: val loss 0.472239\n",
      "\n",
      "Epoch %d: train loss %f 99 0.4218056172132492\n",
      "Epoch 99: val loss 0.481170\n",
      "\n",
      "Epoch %d: train loss %f 100 0.37368452548980713\n",
      "Epoch 100: val loss 0.512430\n",
      "\n",
      "Epoch %d: train loss %f 101 0.3859401658177376\n",
      "Epoch 101: val loss 0.482982\n",
      "\n",
      "Epoch %d: train loss %f 102 0.3598991587758064\n",
      "Epoch 102: val loss 0.500291\n",
      "\n",
      "Epoch %d: train loss %f 103 0.38276993930339814\n",
      "Epoch 103: val loss 0.488879\n",
      "\n",
      "Epoch %d: train loss %f 104 0.370662248134613\n",
      "Epoch 104: val loss 0.482230\n",
      "\n",
      "Epoch %d: train loss %f 105 0.387362140417099\n",
      "Epoch 105: val loss 0.494333\n",
      "\n",
      "Epoch %d: train loss %f 106 0.3618596002459526\n",
      "Epoch 106: val loss 0.492365\n",
      "\n",
      "Epoch %d: train loss %f 107 0.41107252091169355\n",
      "Epoch 107: val loss 0.461907\n",
      "\n",
      "Epoch %d: train loss %f 108 0.3633893892168999\n",
      "Epoch 108: val loss 0.475691\n",
      "\n",
      "Epoch %d: train loss %f 109 0.37984255850315096\n",
      "Epoch 109: val loss 0.481592\n",
      "\n",
      "Epoch %d: train loss %f 110 0.3428162604570389\n",
      "Epoch 110: val loss 0.476110\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3400173604488373\n",
      "Epoch 111: val loss 0.468062\n",
      "\n",
      "Epoch %d: train loss %f 112 0.33972352743148804\n",
      "Epoch 112: val loss 0.465396\n",
      "\n",
      "Epoch %d: train loss %f 113 0.3524670094251633\n",
      "Epoch 113: val loss 0.494331\n",
      "\n",
      "Epoch %d: train loss %f 114 0.3580674514174461\n",
      "Epoch 114: val loss 0.487285\n",
      "\n",
      "Epoch %d: train loss %f 115 0.38134869933128357\n",
      "Epoch 115: val loss 0.501719\n",
      "\n",
      "Epoch %d: train loss %f 116 0.3945639431476593\n",
      "Epoch 116: val loss 0.523760\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3663099706172943\n",
      "Epoch 117: val loss 0.481895\n",
      "\n",
      "Epoch %d: train loss %f 118 0.424161434173584\n",
      "Epoch 118: val loss 0.465046\n",
      "\n",
      "Epoch %d: train loss %f 119 0.40973625481128695\n",
      "Epoch 119: val loss 0.506414\n",
      "\n",
      "Epoch %d: train loss %f 120 0.3393374100327492\n",
      "Epoch 120: val loss 0.485692\n",
      "\n",
      "Epoch %d: train loss %f 121 0.36939969956874846\n",
      "Epoch 121: val loss 0.478211\n",
      "\n",
      "Epoch %d: train loss %f 122 0.38837445676326754\n",
      "Epoch 122: val loss 0.469823\n",
      "\n",
      "Epoch %d: train loss %f 123 0.3748535022139549\n",
      "Epoch 123: val loss 0.487084\n",
      "\n",
      "Epoch %d: train loss %f 124 0.36019309908151625\n",
      "Epoch 124: val loss 0.487692\n",
      "\n",
      "Epoch %d: train loss %f 125 0.3616612359881401\n",
      "Epoch 125: val loss 0.508814\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3719775348901749\n",
      "Epoch 126: val loss 0.504594\n",
      "\n",
      "Epoch %d: train loss %f 127 0.41246659904718397\n",
      "Epoch 127: val loss 0.474876\n",
      "\n",
      "Epoch %d: train loss %f 128 0.36653594970703124\n",
      "Epoch 128: val loss 0.485889\n",
      "\n",
      "Epoch %d: train loss %f 129 0.3438334360718727\n",
      "Epoch 129: val loss 0.470956\n",
      "\n",
      "Epoch %d: train loss %f 130 0.32461472898721694\n",
      "Epoch 130: val loss 0.510226\n",
      "\n",
      "Epoch %d: train loss %f 131 0.3703917503356934\n",
      "Epoch 131: val loss 0.506277\n",
      "\n",
      "Epoch %d: train loss %f 132 0.3415872424840927\n",
      "Epoch 132: val loss 0.520476\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3587660238146782\n",
      "Epoch 133: val loss 0.512318\n",
      "\n",
      "Epoch %d: train loss %f 134 0.3564337983727455\n",
      "Epoch 134: val loss 0.510974\n",
      "\n",
      "Epoch %d: train loss %f 135 0.36825053989887235\n",
      "Epoch 135: val loss 0.473762\n",
      "\n",
      "Epoch %d: train loss %f 136 0.3686863720417023\n",
      "Epoch 136: val loss 0.474105\n",
      "\n",
      "Epoch %d: train loss %f 137 0.3650543615221977\n",
      "Epoch 137: val loss 0.510217\n",
      "\n",
      "Epoch %d: train loss %f 138 0.3932616576552391\n",
      "Epoch 138: val loss 0.507959\n",
      "\n",
      "Epoch %d: train loss %f 139 0.3758072882890701\n",
      "Epoch 139: val loss 0.481952\n",
      "\n",
      "Epoch %d: train loss %f 140 0.361127445101738\n",
      "Epoch 140: val loss 0.474648\n",
      "\n",
      "Epoch %d: train loss %f 141 0.3437912046909332\n",
      "Epoch 141: val loss 0.506872\n",
      "\n",
      "Epoch %d: train loss %f 142 0.38147702217102053\n",
      "Epoch 142: val loss 0.462504\n",
      "\n",
      "Epoch %d: train loss %f 143 0.36900686621665957\n",
      "Epoch 143: val loss 0.472164\n",
      "\n",
      "Epoch %d: train loss %f 144 0.4042984530329704\n",
      "Epoch 144: val loss 0.474239\n",
      "\n",
      "Epoch %d: train loss %f 145 0.4089814558625221\n",
      "Epoch 145: val loss 0.478050\n",
      "\n",
      "Epoch %d: train loss %f 146 0.34730826020240785\n",
      "Epoch 146: val loss 0.484274\n",
      "\n",
      "Epoch %d: train loss %f 147 0.41920280158519746\n",
      "Epoch 147: val loss 0.475815\n",
      "\n",
      "Epoch %d: train loss %f 148 0.35727530121803286\n",
      "Epoch 148: val loss 0.518570\n",
      "\n",
      "Epoch %d: train loss %f 149 0.33133157044649125\n",
      "Epoch 149: val loss 0.479193\n",
      "\n",
      "Epoch %d: train loss %f 150 0.36607763171195984\n",
      "Epoch 150: val loss 0.481870\n",
      "\n",
      "Epoch %d: train loss %f 151 0.3356025397777557\n",
      "Epoch 151: val loss 0.514880\n",
      "\n",
      "Epoch %d: train loss %f 152 0.33268682211637496\n",
      "Epoch 152: val loss 0.498429\n",
      "\n",
      "Epoch %d: train loss %f 153 0.3490036308765411\n",
      "Epoch 153: val loss 0.510035\n",
      "\n",
      "Epoch %d: train loss %f 154 0.3720675647258759\n",
      "Epoch 154: val loss 0.490002\n",
      "\n",
      "Epoch %d: train loss %f 155 0.339378921687603\n",
      "Epoch 155: val loss 0.511923\n",
      "\n",
      "Epoch %d: train loss %f 156 0.3292269676923752\n",
      "Epoch 156: val loss 0.516603\n",
      "\n",
      "Epoch %d: train loss %f 157 0.36983299553394317\n",
      "Epoch 157: val loss 0.500755\n",
      "\n",
      "Epoch %d: train loss %f 158 0.3355577729642391\n",
      "Epoch 158: val loss 0.520727\n",
      "\n",
      "Epoch %d: train loss %f 159 0.3559249848127365\n",
      "Epoch 159: val loss 0.485170\n",
      "\n",
      "Epoch %d: train loss %f 160 0.3478389739990234\n",
      "Epoch 160: val loss 0.516436\n",
      "\n",
      "Epoch %d: train loss %f 161 0.3651209756731987\n",
      "Epoch 161: val loss 0.512699\n",
      "\n",
      "Epoch %d: train loss %f 162 0.33345955312252046\n",
      "Epoch 162: val loss 0.527784\n",
      "\n",
      "Epoch %d: train loss %f 163 0.3419341266155243\n",
      "Epoch 163: val loss 0.522412\n",
      "\n",
      "Epoch %d: train loss %f 164 0.35805132538080214\n",
      "Epoch 164: val loss 0.515957\n",
      "\n",
      "Epoch %d: train loss %f 165 0.3359211325645447\n",
      "Epoch 165: val loss 0.497602\n",
      "\n",
      "Epoch %d: train loss %f 166 0.34350934326648713\n",
      "Epoch 166: val loss 0.530465\n",
      "\n",
      "Epoch %d: train loss %f 167 0.3339713796973228\n",
      "Epoch 167: val loss 0.529435\n",
      "\n",
      "Epoch %d: train loss %f 168 0.37509689331054685\n",
      "Epoch 168: val loss 0.503406\n",
      "\n",
      "Epoch %d: train loss %f 169 0.3277637422084808\n",
      "Epoch 169: val loss 0.528649\n",
      "\n",
      "Epoch %d: train loss %f 170 0.34732208251953123\n",
      "Epoch 170: val loss 0.530662\n",
      "\n",
      "Epoch %d: train loss %f 171 0.3451270520687103\n",
      "Epoch 171: val loss 0.569100\n",
      "\n",
      "Epoch %d: train loss %f 172 0.34870947152376175\n",
      "Epoch 172: val loss 0.547468\n",
      "\n",
      "Epoch %d: train loss %f 173 0.3240849688649178\n",
      "Epoch 173: val loss 0.527665\n",
      "\n",
      "Epoch %d: train loss %f 174 0.3521487474441528\n",
      "Epoch 174: val loss 0.518662\n",
      "\n",
      "Epoch %d: train loss %f 175 0.34611218944191935\n",
      "Epoch 175: val loss 0.518504\n",
      "\n",
      "Epoch %d: train loss %f 176 0.3404276371002197\n",
      "Epoch 176: val loss 0.529924\n",
      "\n",
      "Epoch %d: train loss %f 177 0.32882013767957685\n",
      "Epoch 177: val loss 0.531778\n",
      "\n",
      "Epoch %d: train loss %f 178 0.31362789273262026\n",
      "Epoch 178: val loss 0.534041\n",
      "\n",
      "Epoch %d: train loss %f 179 0.3091242969036102\n",
      "Epoch 179: val loss 0.532899\n",
      "\n",
      "Epoch %d: train loss %f 180 0.362912592291832\n",
      "Epoch 180: val loss 0.524814\n",
      "\n",
      "Epoch %d: train loss %f 181 0.3337417975068092\n",
      "Epoch 181: val loss 0.545469\n",
      "\n",
      "Epoch %d: train loss %f 182 0.3269573196768761\n",
      "Epoch 182: val loss 0.503691\n",
      "\n",
      "Epoch %d: train loss %f 183 0.30975624471902846\n",
      "Epoch 183: val loss 0.504771\n",
      "\n",
      "Epoch %d: train loss %f 184 0.3412903755903244\n",
      "Epoch 184: val loss 0.536403\n",
      "\n",
      "Epoch %d: train loss %f 185 0.33563765734434126\n",
      "Epoch 185: val loss 0.561491\n",
      "\n",
      "Epoch %d: train loss %f 186 0.3436841815710068\n",
      "Epoch 186: val loss 0.545694\n",
      "\n",
      "Epoch %d: train loss %f 187 0.34971450418233874\n",
      "Epoch 187: val loss 0.507343\n",
      "\n",
      "Epoch %d: train loss %f 188 0.3323494419455528\n",
      "Epoch 188: val loss 0.514581\n",
      "\n",
      "Epoch %d: train loss %f 189 0.3435483783483505\n",
      "Epoch 189: val loss 0.525409\n",
      "\n",
      "Epoch %d: train loss %f 190 0.35030355751514436\n",
      "Epoch 190: val loss 0.531737\n",
      "\n",
      "Epoch %d: train loss %f 191 0.37479773461818694\n",
      "Epoch 191: val loss 0.517284\n",
      "\n",
      "Epoch %d: train loss %f 192 0.3601407676935196\n",
      "Epoch 192: val loss 0.550960\n",
      "\n",
      "Epoch %d: train loss %f 193 0.3351467803120613\n",
      "Epoch 193: val loss 0.519568\n",
      "\n",
      "Epoch %d: train loss %f 194 0.31063781678676605\n",
      "Epoch 194: val loss 0.555847\n",
      "\n",
      "Epoch %d: train loss %f 195 0.33410195261240005\n",
      "Epoch 195: val loss 0.533839\n",
      "\n",
      "Epoch %d: train loss %f 196 0.35023233890533445\n",
      "Epoch 196: val loss 0.537077\n",
      "\n",
      "Epoch %d: train loss %f 197 0.3301779627799988\n",
      "Epoch 197: val loss 0.545805\n",
      "\n",
      "Epoch %d: train loss %f 198 0.3575841590762138\n",
      "Epoch 198: val loss 0.536630\n",
      "\n",
      "Epoch %d: train loss %f 199 0.3656366378068924\n",
      "Epoch 199: val loss 0.518100\n",
      "\n",
      "Epoch %d: train loss %f 0 0.698770260810852\n",
      "Epoch 0: val loss 0.698281\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6832149386405945\n",
      "Epoch 1: val loss 0.690625\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6632867693901062\n",
      "Epoch 2: val loss 0.674072\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6259727478027344\n",
      "Epoch 3: val loss 0.640232\n",
      "\n",
      "Epoch %d: train loss %f 4 0.5962755143642425\n",
      "Epoch 4: val loss 0.606306\n",
      "\n",
      "Epoch %d: train loss %f 5 0.5655841737985611\n",
      "Epoch 5: val loss 0.591631\n",
      "\n",
      "Epoch %d: train loss %f 6 0.5347809344530106\n",
      "Epoch 6: val loss 0.582451\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5192027300596237\n",
      "Epoch 7: val loss 0.575745\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5280932813882828\n",
      "Epoch 8: val loss 0.574636\n",
      "\n",
      "Epoch %d: train loss %f 9 0.4972237467765808\n",
      "Epoch 9: val loss 0.576150\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5017962038516999\n",
      "Epoch 10: val loss 0.572143\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5347281873226166\n",
      "Epoch 11: val loss 0.566048\n",
      "\n",
      "Epoch %d: train loss %f 12 0.47627282738685606\n",
      "Epoch 12: val loss 0.550987\n",
      "\n",
      "Epoch %d: train loss %f 13 0.4661861568689346\n",
      "Epoch 13: val loss 0.557097\n",
      "\n",
      "Epoch %d: train loss %f 14 0.4982921242713928\n",
      "Epoch 14: val loss 0.566031\n",
      "\n",
      "Epoch %d: train loss %f 15 0.4811881721019745\n",
      "Epoch 15: val loss 0.570320\n",
      "\n",
      "Epoch %d: train loss %f 16 0.47967748939990995\n",
      "Epoch 16: val loss 0.562913\n",
      "\n",
      "Epoch %d: train loss %f 17 0.462215331196785\n",
      "Epoch 17: val loss 0.580163\n",
      "\n",
      "Epoch %d: train loss %f 18 0.4573277622461319\n",
      "Epoch 18: val loss 0.584561\n",
      "\n",
      "Epoch %d: train loss %f 19 0.4683476835489273\n",
      "Epoch 19: val loss 0.571602\n",
      "\n",
      "Epoch %d: train loss %f 20 0.4577584981918335\n",
      "Epoch 20: val loss 0.555684\n",
      "\n",
      "Epoch %d: train loss %f 21 0.44500542283058164\n",
      "Epoch 21: val loss 0.572759\n",
      "\n",
      "Epoch %d: train loss %f 22 0.451667508482933\n",
      "Epoch 22: val loss 0.580428\n",
      "\n",
      "Epoch %d: train loss %f 23 0.44786485135555265\n",
      "Epoch 23: val loss 0.578014\n",
      "\n",
      "Epoch %d: train loss %f 24 0.4452368259429932\n",
      "Epoch 24: val loss 0.589737\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4643069952726364\n",
      "Epoch 25: val loss 0.591281\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4291318029165268\n",
      "Epoch 26: val loss 0.595054\n",
      "\n",
      "Epoch %d: train loss %f 27 0.420462104678154\n",
      "Epoch 27: val loss 0.592643\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4471938908100128\n",
      "Epoch 28: val loss 0.607040\n",
      "\n",
      "Epoch %d: train loss %f 29 0.44639977514743806\n",
      "Epoch 29: val loss 0.616063\n",
      "\n",
      "Epoch %d: train loss %f 30 0.4273260414600372\n",
      "Epoch 30: val loss 0.609860\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4239273339509964\n",
      "Epoch 31: val loss 0.613376\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4233955055475235\n",
      "Epoch 32: val loss 0.609514\n",
      "\n",
      "Epoch %d: train loss %f 33 0.4094839423894882\n",
      "Epoch 33: val loss 0.617122\n",
      "\n",
      "Epoch %d: train loss %f 34 0.4217230126261711\n",
      "Epoch 34: val loss 0.609519\n",
      "\n",
      "Epoch %d: train loss %f 35 0.40395249128341676\n",
      "Epoch 35: val loss 0.604059\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4215511679649353\n",
      "Epoch 36: val loss 0.622440\n",
      "\n",
      "Epoch %d: train loss %f 37 0.44972353875637056\n",
      "Epoch 37: val loss 0.640209\n",
      "\n",
      "Epoch %d: train loss %f 38 0.3973425030708313\n",
      "Epoch 38: val loss 0.620752\n",
      "\n",
      "Epoch %d: train loss %f 39 0.38134570717811583\n",
      "Epoch 39: val loss 0.627407\n",
      "\n",
      "Epoch %d: train loss %f 40 0.3905028000473976\n",
      "Epoch 40: val loss 0.642057\n",
      "\n",
      "Epoch %d: train loss %f 41 0.3992768809199333\n",
      "Epoch 41: val loss 0.646961\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4614017248153687\n",
      "Epoch 42: val loss 0.637690\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4276465684175491\n",
      "Epoch 43: val loss 0.638316\n",
      "\n",
      "Epoch %d: train loss %f 44 0.42288312315940857\n",
      "Epoch 44: val loss 0.638176\n",
      "\n",
      "Epoch %d: train loss %f 45 0.3939208462834358\n",
      "Epoch 45: val loss 0.630523\n",
      "\n",
      "Epoch %d: train loss %f 46 0.41589510142803193\n",
      "Epoch 46: val loss 0.638281\n",
      "\n",
      "Epoch %d: train loss %f 47 0.3939663231372833\n",
      "Epoch 47: val loss 0.636933\n",
      "\n",
      "Epoch %d: train loss %f 48 0.3776260033249855\n",
      "Epoch 48: val loss 0.626006\n",
      "\n",
      "Epoch %d: train loss %f 49 0.40191211700439455\n",
      "Epoch 49: val loss 0.636417\n",
      "\n",
      "Epoch %d: train loss %f 50 0.3937649428844452\n",
      "Epoch 50: val loss 0.639113\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4039994180202484\n",
      "Epoch 51: val loss 0.652017\n",
      "\n",
      "Epoch %d: train loss %f 52 0.3696366086602211\n",
      "Epoch 52: val loss 0.651434\n",
      "\n",
      "Epoch %d: train loss %f 53 0.37588134706020354\n",
      "Epoch 53: val loss 0.685192\n",
      "\n",
      "Epoch %d: train loss %f 54 0.41460441052913666\n",
      "Epoch 54: val loss 0.650915\n",
      "\n",
      "Epoch %d: train loss %f 55 0.3662678077816963\n",
      "Epoch 55: val loss 0.644572\n",
      "\n",
      "Epoch %d: train loss %f 56 0.3843814432621002\n",
      "Epoch 56: val loss 0.641129\n",
      "\n",
      "Epoch %d: train loss %f 57 0.3996652364730835\n",
      "Epoch 57: val loss 0.669889\n",
      "\n",
      "Epoch %d: train loss %f 58 0.352963724732399\n",
      "Epoch 58: val loss 0.643015\n",
      "\n",
      "Epoch %d: train loss %f 59 0.37498548477888105\n",
      "Epoch 59: val loss 0.644269\n",
      "\n",
      "Epoch %d: train loss %f 60 0.34710704535245895\n",
      "Epoch 60: val loss 0.663500\n",
      "\n",
      "Epoch %d: train loss %f 61 0.3986138790845871\n",
      "Epoch 61: val loss 0.652122\n",
      "\n",
      "Epoch %d: train loss %f 62 0.38713852763175965\n",
      "Epoch 62: val loss 0.629144\n",
      "\n",
      "Epoch %d: train loss %f 63 0.3908692479133606\n",
      "Epoch 63: val loss 0.633678\n",
      "\n",
      "Epoch %d: train loss %f 64 0.39684823155403137\n",
      "Epoch 64: val loss 0.673134\n",
      "\n",
      "Epoch %d: train loss %f 65 0.39151344299316404\n",
      "Epoch 65: val loss 0.665333\n",
      "\n",
      "Epoch %d: train loss %f 66 0.36080098152160645\n",
      "Epoch 66: val loss 0.655460\n",
      "\n",
      "Epoch %d: train loss %f 67 0.3491199314594269\n",
      "Epoch 67: val loss 0.656200\n",
      "\n",
      "Epoch %d: train loss %f 68 0.3475102722644806\n",
      "Epoch 68: val loss 0.653347\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4162381261587143\n",
      "Epoch 69: val loss 0.642635\n",
      "\n",
      "Epoch %d: train loss %f 70 0.39877805709838865\n",
      "Epoch 70: val loss 0.647259\n",
      "\n",
      "Epoch %d: train loss %f 71 0.37504932284355164\n",
      "Epoch 71: val loss 0.691374\n",
      "\n",
      "Epoch %d: train loss %f 72 0.3848748683929443\n",
      "Epoch 72: val loss 0.676222\n",
      "\n",
      "Epoch %d: train loss %f 73 0.35215013474226\n",
      "Epoch 73: val loss 0.686583\n",
      "\n",
      "Epoch %d: train loss %f 74 0.3493928492069244\n",
      "Epoch 74: val loss 0.676367\n",
      "\n",
      "Epoch %d: train loss %f 75 0.35562537610530853\n",
      "Epoch 75: val loss 0.664408\n",
      "\n",
      "Epoch %d: train loss %f 76 0.3628721058368683\n",
      "Epoch 76: val loss 0.685115\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3702307239174843\n",
      "Epoch 77: val loss 0.689687\n",
      "\n",
      "Epoch %d: train loss %f 78 0.3597032979130745\n",
      "Epoch 78: val loss 0.683645\n",
      "\n",
      "Epoch %d: train loss %f 79 0.35558542013168337\n",
      "Epoch 79: val loss 0.673152\n",
      "\n",
      "Epoch %d: train loss %f 80 0.3795434728264809\n",
      "Epoch 80: val loss 0.693205\n",
      "\n",
      "Epoch %d: train loss %f 81 0.33628580570220945\n",
      "Epoch 81: val loss 0.691318\n",
      "\n",
      "Epoch %d: train loss %f 82 0.39158024191856383\n",
      "Epoch 82: val loss 0.697556\n",
      "\n",
      "Epoch %d: train loss %f 83 0.36979939937591555\n",
      "Epoch 83: val loss 0.682954\n",
      "\n",
      "Epoch %d: train loss %f 84 0.3401153668761253\n",
      "Epoch 84: val loss 0.669689\n",
      "\n",
      "Epoch %d: train loss %f 85 0.3527659848332405\n",
      "Epoch 85: val loss 0.702792\n",
      "\n",
      "Epoch %d: train loss %f 86 0.3407106138765812\n",
      "Epoch 86: val loss 0.733917\n",
      "\n",
      "Epoch %d: train loss %f 87 0.3606732591986656\n",
      "Epoch 87: val loss 0.725623\n",
      "\n",
      "Epoch %d: train loss %f 88 0.33401887863874435\n",
      "Epoch 88: val loss 0.709152\n",
      "\n",
      "Epoch %d: train loss %f 89 0.35432197749614713\n",
      "Epoch 89: val loss 0.703500\n",
      "\n",
      "Epoch %d: train loss %f 90 0.3159621447324753\n",
      "Epoch 90: val loss 0.690436\n",
      "\n",
      "Epoch %d: train loss %f 91 0.35105445683002473\n",
      "Epoch 91: val loss 0.729496\n",
      "\n",
      "Epoch %d: train loss %f 92 0.3448029518127441\n",
      "Epoch 92: val loss 0.723688\n",
      "\n",
      "Epoch %d: train loss %f 93 0.3659457862377167\n",
      "Epoch 93: val loss 0.708240\n",
      "\n",
      "Epoch %d: train loss %f 94 0.35310715436935425\n",
      "Epoch 94: val loss 0.691101\n",
      "\n",
      "Epoch %d: train loss %f 95 0.35686970949172975\n",
      "Epoch 95: val loss 0.677362\n",
      "\n",
      "Epoch %d: train loss %f 96 0.33530480712652205\n",
      "Epoch 96: val loss 0.676866\n",
      "\n",
      "Epoch %d: train loss %f 97 0.38059067279100417\n",
      "Epoch 97: val loss 0.677033\n",
      "\n",
      "Epoch %d: train loss %f 98 0.3421658486127853\n",
      "Epoch 98: val loss 0.684409\n",
      "\n",
      "Epoch %d: train loss %f 99 0.31093611419200895\n",
      "Epoch 99: val loss 0.701408\n",
      "\n",
      "Epoch %d: train loss %f 100 0.3685614302754402\n",
      "Epoch 100: val loss 0.694156\n",
      "\n",
      "Epoch %d: train loss %f 101 0.3650452196598053\n",
      "Epoch 101: val loss 0.692306\n",
      "\n",
      "Epoch %d: train loss %f 102 0.3268565163016319\n",
      "Epoch 102: val loss 0.691829\n",
      "\n",
      "Epoch %d: train loss %f 103 0.31648437976837157\n",
      "Epoch 103: val loss 0.700235\n",
      "\n",
      "Epoch %d: train loss %f 104 0.3346493005752563\n",
      "Epoch 104: val loss 0.697765\n",
      "\n",
      "Epoch %d: train loss %f 105 0.3200205028057098\n",
      "Epoch 105: val loss 0.685077\n",
      "\n",
      "Epoch %d: train loss %f 106 0.30561634749174116\n",
      "Epoch 106: val loss 0.718350\n",
      "\n",
      "Epoch %d: train loss %f 107 0.3258670449256897\n",
      "Epoch 107: val loss 0.704278\n",
      "\n",
      "Epoch %d: train loss %f 108 0.3264151558279991\n",
      "Epoch 108: val loss 0.686561\n",
      "\n",
      "Epoch %d: train loss %f 109 0.28981669545173644\n",
      "Epoch 109: val loss 0.704251\n",
      "\n",
      "Epoch %d: train loss %f 110 0.30297451466321945\n",
      "Epoch 110: val loss 0.689533\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3417281791567802\n",
      "Epoch 111: val loss 0.726070\n",
      "\n",
      "Epoch %d: train loss %f 112 0.32752356231212615\n",
      "Epoch 112: val loss 0.730840\n",
      "\n",
      "Epoch %d: train loss %f 113 0.2989166662096977\n",
      "Epoch 113: val loss 0.715933\n",
      "\n",
      "Epoch %d: train loss %f 114 0.328337824344635\n",
      "Epoch 114: val loss 0.713276\n",
      "\n",
      "Epoch %d: train loss %f 115 0.3302648298442364\n",
      "Epoch 115: val loss 0.707149\n",
      "\n",
      "Epoch %d: train loss %f 116 0.3220032602548599\n",
      "Epoch 116: val loss 0.711390\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3070331707596779\n",
      "Epoch 117: val loss 0.737946\n",
      "\n",
      "Epoch %d: train loss %f 118 0.2984353229403496\n",
      "Epoch 118: val loss 0.773613\n",
      "\n",
      "Epoch %d: train loss %f 119 0.321386082470417\n",
      "Epoch 119: val loss 0.755686\n",
      "\n",
      "Epoch %d: train loss %f 120 0.37716841101646426\n",
      "Epoch 120: val loss 0.735138\n",
      "\n",
      "Epoch %d: train loss %f 121 0.35185917615890505\n",
      "Epoch 121: val loss 0.723099\n",
      "\n",
      "Epoch %d: train loss %f 122 0.330753093957901\n",
      "Epoch 122: val loss 0.732255\n",
      "\n",
      "Epoch %d: train loss %f 123 0.30899677872657777\n",
      "Epoch 123: val loss 0.725818\n",
      "\n",
      "Epoch %d: train loss %f 124 0.2833437532186508\n",
      "Epoch 124: val loss 0.746525\n",
      "\n",
      "Epoch %d: train loss %f 125 0.3186302646994591\n",
      "Epoch 125: val loss 0.761337\n",
      "\n",
      "Epoch %d: train loss %f 126 0.30251074135303496\n",
      "Epoch 126: val loss 0.776762\n",
      "\n",
      "Epoch %d: train loss %f 127 0.32199364751577375\n",
      "Epoch 127: val loss 0.741588\n",
      "\n",
      "Epoch %d: train loss %f 128 0.3429279699921608\n",
      "Epoch 128: val loss 0.727647\n",
      "\n",
      "Epoch %d: train loss %f 129 0.3034055784344673\n",
      "Epoch 129: val loss 0.719642\n",
      "\n",
      "Epoch %d: train loss %f 130 0.2945302836596966\n",
      "Epoch 130: val loss 0.743842\n",
      "\n",
      "Epoch %d: train loss %f 131 0.30489362925291064\n",
      "Epoch 131: val loss 0.745563\n",
      "\n",
      "Epoch %d: train loss %f 132 0.33600868582725524\n",
      "Epoch 132: val loss 0.769328\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3271909222006798\n",
      "Epoch 133: val loss 0.776435\n",
      "\n",
      "Epoch %d: train loss %f 134 0.31588607281446457\n",
      "Epoch 134: val loss 0.779517\n",
      "\n",
      "Epoch %d: train loss %f 135 0.308710677921772\n",
      "Epoch 135: val loss 0.758872\n",
      "\n",
      "Epoch %d: train loss %f 136 0.35233159065246583\n",
      "Epoch 136: val loss 0.751498\n",
      "\n",
      "Epoch %d: train loss %f 137 0.31701513230800626\n",
      "Epoch 137: val loss 0.758569\n",
      "\n",
      "Epoch %d: train loss %f 138 0.32602654695510863\n",
      "Epoch 138: val loss 0.753756\n",
      "\n",
      "Epoch %d: train loss %f 139 0.3414326697587967\n",
      "Epoch 139: val loss 0.761387\n",
      "\n",
      "Epoch %d: train loss %f 140 0.3060307905077934\n",
      "Epoch 140: val loss 0.732536\n",
      "\n",
      "Epoch %d: train loss %f 141 0.2954335391521454\n",
      "Epoch 141: val loss 0.718171\n",
      "\n",
      "Epoch %d: train loss %f 142 0.3339198812842369\n",
      "Epoch 142: val loss 0.734700\n",
      "\n",
      "Epoch %d: train loss %f 143 0.2808862715959549\n",
      "Epoch 143: val loss 0.762671\n",
      "\n",
      "Epoch %d: train loss %f 144 0.2517635889351368\n",
      "Epoch 144: val loss 0.761095\n",
      "\n",
      "Epoch %d: train loss %f 145 0.27588951587677\n",
      "Epoch 145: val loss 0.747912\n",
      "\n",
      "Epoch %d: train loss %f 146 0.30161443501710894\n",
      "Epoch 146: val loss 0.736067\n",
      "\n",
      "Epoch %d: train loss %f 147 0.3113437116146088\n",
      "Epoch 147: val loss 0.728774\n",
      "\n",
      "Epoch %d: train loss %f 148 0.31303400099277495\n",
      "Epoch 148: val loss 0.750907\n",
      "\n",
      "Epoch %d: train loss %f 149 0.28054740875959394\n",
      "Epoch 149: val loss 0.774848\n",
      "\n",
      "Epoch %d: train loss %f 150 0.29427103102207186\n",
      "Epoch 150: val loss 0.784924\n",
      "\n",
      "Epoch %d: train loss %f 151 0.3047637686133385\n",
      "Epoch 151: val loss 0.781947\n",
      "\n",
      "Epoch %d: train loss %f 152 0.3077175557613373\n",
      "Epoch 152: val loss 0.758785\n",
      "\n",
      "Epoch %d: train loss %f 153 0.31736665666103364\n",
      "Epoch 153: val loss 0.761556\n",
      "\n",
      "Epoch %d: train loss %f 154 0.31520171612501147\n",
      "Epoch 154: val loss 0.767842\n",
      "\n",
      "Epoch %d: train loss %f 155 0.281787545979023\n",
      "Epoch 155: val loss 0.761164\n",
      "\n",
      "Epoch %d: train loss %f 156 0.2551377035677433\n",
      "Epoch 156: val loss 0.776248\n",
      "\n",
      "Epoch %d: train loss %f 157 0.2516666784882545\n",
      "Epoch 157: val loss 0.810574\n",
      "\n",
      "Epoch %d: train loss %f 158 0.30617573410272597\n",
      "Epoch 158: val loss 0.800949\n",
      "\n",
      "Epoch %d: train loss %f 159 0.2832160651683807\n",
      "Epoch 159: val loss 0.770597\n",
      "\n",
      "Epoch %d: train loss %f 160 0.2546206459403038\n",
      "Epoch 160: val loss 0.774182\n",
      "\n",
      "Epoch %d: train loss %f 161 0.31831973046064377\n",
      "Epoch 161: val loss 0.782773\n",
      "\n",
      "Epoch %d: train loss %f 162 0.31204460486769675\n",
      "Epoch 162: val loss 0.785266\n",
      "\n",
      "Epoch %d: train loss %f 163 0.27295489311218263\n",
      "Epoch 163: val loss 0.799272\n",
      "\n",
      "Epoch %d: train loss %f 164 0.25821904838085175\n",
      "Epoch 164: val loss 0.828605\n",
      "\n",
      "Epoch %d: train loss %f 165 0.2900562584400177\n",
      "Epoch 165: val loss 0.834923\n",
      "\n",
      "Epoch %d: train loss %f 166 0.3192572221159935\n",
      "Epoch 166: val loss 0.792974\n",
      "\n",
      "Epoch %d: train loss %f 167 0.2845730185508728\n",
      "Epoch 167: val loss 0.793574\n",
      "\n",
      "Epoch %d: train loss %f 168 0.29748573154211044\n",
      "Epoch 168: val loss 0.795323\n",
      "\n",
      "Epoch %d: train loss %f 169 0.28193650394678116\n",
      "Epoch 169: val loss 0.769109\n",
      "\n",
      "Epoch %d: train loss %f 170 0.30506025105714796\n",
      "Epoch 170: val loss 0.770856\n",
      "\n",
      "Epoch %d: train loss %f 171 0.2577286347746849\n",
      "Epoch 171: val loss 0.814158\n",
      "\n",
      "Epoch %d: train loss %f 172 0.29266400188207625\n",
      "Epoch 172: val loss 0.841779\n",
      "\n",
      "Epoch %d: train loss %f 173 0.30016967952251433\n",
      "Epoch 173: val loss 0.781266\n",
      "\n",
      "Epoch %d: train loss %f 174 0.32057998329401016\n",
      "Epoch 174: val loss 0.769799\n",
      "\n",
      "Epoch %d: train loss %f 175 0.3206719860434532\n",
      "Epoch 175: val loss 0.767241\n",
      "\n",
      "Epoch %d: train loss %f 176 0.27977096289396286\n",
      "Epoch 176: val loss 0.769651\n",
      "\n",
      "Epoch %d: train loss %f 177 0.27657012045383456\n",
      "Epoch 177: val loss 0.777058\n",
      "\n",
      "Epoch %d: train loss %f 178 0.31420307606458664\n",
      "Epoch 178: val loss 0.777054\n",
      "\n",
      "Epoch %d: train loss %f 179 0.2770479328930378\n",
      "Epoch 179: val loss 0.796558\n",
      "\n",
      "Epoch %d: train loss %f 180 0.2943520054221153\n",
      "Epoch 180: val loss 0.782004\n",
      "\n",
      "Epoch %d: train loss %f 181 0.27752395868301394\n",
      "Epoch 181: val loss 0.798955\n",
      "\n",
      "Epoch %d: train loss %f 182 0.29249695390462876\n",
      "Epoch 182: val loss 0.794957\n",
      "\n",
      "Epoch %d: train loss %f 183 0.2750566527247429\n",
      "Epoch 183: val loss 0.773959\n",
      "\n",
      "Epoch %d: train loss %f 184 0.26755521073937416\n",
      "Epoch 184: val loss 0.784536\n",
      "\n",
      "Epoch %d: train loss %f 185 0.28151688277721404\n",
      "Epoch 185: val loss 0.801784\n",
      "\n",
      "Epoch %d: train loss %f 186 0.2567141816020012\n",
      "Epoch 186: val loss 0.853938\n",
      "\n",
      "Epoch %d: train loss %f 187 0.2531818263232708\n",
      "Epoch 187: val loss 0.812644\n",
      "\n",
      "Epoch %d: train loss %f 188 0.2974431231617928\n",
      "Epoch 188: val loss 0.805782\n",
      "\n",
      "Epoch %d: train loss %f 189 0.2552425146102905\n",
      "Epoch 189: val loss 0.841498\n",
      "\n",
      "Epoch %d: train loss %f 190 0.30360292792320254\n",
      "Epoch 190: val loss 0.825422\n",
      "\n",
      "Epoch %d: train loss %f 191 0.264227694272995\n",
      "Epoch 191: val loss 0.832671\n",
      "\n",
      "Epoch %d: train loss %f 192 0.30825444161891935\n",
      "Epoch 192: val loss 0.825152\n",
      "\n",
      "Epoch %d: train loss %f 193 0.28355164974927904\n",
      "Epoch 193: val loss 0.844709\n",
      "\n",
      "Epoch %d: train loss %f 194 0.25427528023719786\n",
      "Epoch 194: val loss 0.850762\n",
      "\n",
      "Epoch %d: train loss %f 195 0.27203122079372405\n",
      "Epoch 195: val loss 0.831852\n",
      "\n",
      "Epoch %d: train loss %f 196 0.32621205002069475\n",
      "Epoch 196: val loss 0.814147\n",
      "\n",
      "Epoch %d: train loss %f 197 0.27760030552744863\n",
      "Epoch 197: val loss 0.810381\n",
      "\n",
      "Epoch %d: train loss %f 198 0.27879871875047685\n",
      "Epoch 198: val loss 0.822232\n",
      "\n",
      "Epoch %d: train loss %f 199 0.269430760294199\n",
      "Epoch 199: val loss 0.813958\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6725041270256042\n",
      "Epoch 0: val loss 0.673200\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6517737874617944\n",
      "Epoch 1: val loss 0.662499\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6213212792689984\n",
      "Epoch 2: val loss 0.634986\n",
      "\n",
      "Epoch %d: train loss %f 3 0.5892453056115371\n",
      "Epoch 3: val loss 0.606776\n",
      "\n",
      "Epoch %d: train loss %f 4 0.5451632623489087\n",
      "Epoch 4: val loss 0.581467\n",
      "\n",
      "Epoch %d: train loss %f 5 0.5410917149140284\n",
      "Epoch 5: val loss 0.582826\n",
      "\n",
      "Epoch %d: train loss %f 6 0.5195980713917658\n",
      "Epoch 6: val loss 0.568355\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5189873553239382\n",
      "Epoch 7: val loss 0.575370\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5018956913397863\n",
      "Epoch 8: val loss 0.560128\n",
      "\n",
      "Epoch %d: train loss %f 9 0.4971081935442411\n",
      "Epoch 9: val loss 0.555400\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5234512893053201\n",
      "Epoch 10: val loss 0.552153\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5001463339878962\n",
      "Epoch 11: val loss 0.550132\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5021742628170893\n",
      "Epoch 12: val loss 0.557449\n",
      "\n",
      "Epoch %d: train loss %f 13 0.4837752122145433\n",
      "Epoch 13: val loss 0.559489\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5106671681770911\n",
      "Epoch 14: val loss 0.553811\n",
      "\n",
      "Epoch %d: train loss %f 15 0.4930405570910527\n",
      "Epoch 15: val loss 0.545405\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5035110734976255\n",
      "Epoch 16: val loss 0.553446\n",
      "\n",
      "Epoch %d: train loss %f 17 0.4908903722579663\n",
      "Epoch 17: val loss 0.551677\n",
      "\n",
      "Epoch %d: train loss %f 18 0.4764906351382916\n",
      "Epoch 18: val loss 0.553804\n",
      "\n",
      "Epoch %d: train loss %f 19 0.4758453965187073\n",
      "Epoch 19: val loss 0.556834\n",
      "\n",
      "Epoch %d: train loss %f 20 0.4907908875208635\n",
      "Epoch 20: val loss 0.551870\n",
      "\n",
      "Epoch %d: train loss %f 21 0.48042139410972595\n",
      "Epoch 21: val loss 0.557330\n",
      "\n",
      "Epoch %d: train loss %f 22 0.47664273931429935\n",
      "Epoch 22: val loss 0.543442\n",
      "\n",
      "Epoch %d: train loss %f 23 0.46795530961110043\n",
      "Epoch 23: val loss 0.540454\n",
      "\n",
      "Epoch %d: train loss %f 24 0.4694399833679199\n",
      "Epoch 24: val loss 0.539569\n",
      "\n",
      "Epoch %d: train loss %f 25 0.48512598184438854\n",
      "Epoch 25: val loss 0.546151\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4749499192604652\n",
      "Epoch 26: val loss 0.545460\n",
      "\n",
      "Epoch %d: train loss %f 27 0.4730372360119453\n",
      "Epoch 27: val loss 0.541377\n",
      "\n",
      "Epoch %d: train loss %f 28 0.486751152918889\n",
      "Epoch 28: val loss 0.538920\n",
      "\n",
      "Epoch %d: train loss %f 29 0.47521900672179\n",
      "Epoch 29: val loss 0.540043\n",
      "\n",
      "Epoch %d: train loss %f 30 0.47873011460671056\n",
      "Epoch 30: val loss 0.535893\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4600439232129317\n",
      "Epoch 31: val loss 0.534336\n",
      "\n",
      "Epoch %d: train loss %f 32 0.46104137943341184\n",
      "Epoch 32: val loss 0.540147\n",
      "\n",
      "Epoch %d: train loss %f 33 0.48015512182162357\n",
      "Epoch 33: val loss 0.536767\n",
      "\n",
      "Epoch %d: train loss %f 34 0.4603469899067512\n",
      "Epoch 34: val loss 0.529891\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4646739111496852\n",
      "Epoch 35: val loss 0.539930\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4701757820752951\n",
      "Epoch 36: val loss 0.527204\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4414912829032311\n",
      "Epoch 37: val loss 0.535126\n",
      "\n",
      "Epoch %d: train loss %f 38 0.4335622856250176\n",
      "Epoch 38: val loss 0.547109\n",
      "\n",
      "Epoch %d: train loss %f 39 0.44305275953733003\n",
      "Epoch 39: val loss 0.546462\n",
      "\n",
      "Epoch %d: train loss %f 40 0.447109213242164\n",
      "Epoch 40: val loss 0.548019\n",
      "\n",
      "Epoch %d: train loss %f 41 0.44373523272000825\n",
      "Epoch 41: val loss 0.548455\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4511431868259723\n",
      "Epoch 42: val loss 0.564590\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4337876049371866\n",
      "Epoch 43: val loss 0.557615\n",
      "\n",
      "Epoch %d: train loss %f 44 0.4916326724565946\n",
      "Epoch 44: val loss 0.556435\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6917804016007317\n",
      "Epoch 0: val loss 0.692388\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6882284416092767\n",
      "Epoch 1: val loss 0.691208\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6835920148425632\n",
      "Epoch 2: val loss 0.688546\n",
      "\n",
      "Epoch %d: train loss %f 3 0.678021616405911\n",
      "Epoch 3: val loss 0.683824\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6667516893810697\n",
      "Epoch 4: val loss 0.677041\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6532390978601244\n",
      "Epoch 5: val loss 0.663438\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6419220500522189\n",
      "Epoch 6: val loss 0.660480\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6310097111596001\n",
      "Epoch 7: val loss 0.673386\n",
      "\n",
      "Epoch %d: train loss %f 8 0.6304391490088569\n",
      "Epoch 8: val loss 0.648937\n",
      "\n",
      "Epoch %d: train loss %f 9 0.6053131222724915\n",
      "Epoch 9: val loss 0.660800\n",
      "\n",
      "Epoch %d: train loss %f 10 0.6010010573599074\n",
      "Epoch 10: val loss 0.643124\n",
      "\n",
      "Epoch %d: train loss %f 11 0.6057438419924842\n",
      "Epoch 11: val loss 0.625471\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5833080212275187\n",
      "Epoch 12: val loss 0.638314\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5788562297821045\n",
      "Epoch 13: val loss 0.621680\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5598672959539626\n",
      "Epoch 14: val loss 0.620694\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5538119475046793\n",
      "Epoch 15: val loss 0.626841\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5470259818765852\n",
      "Epoch 16: val loss 0.616880\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5550300776958466\n",
      "Epoch 17: val loss 0.609840\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5547143982516395\n",
      "Epoch 18: val loss 0.621908\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5385788844691383\n",
      "Epoch 19: val loss 0.612690\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5222143464618259\n",
      "Epoch 20: val loss 0.594831\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5327234433756934\n",
      "Epoch 21: val loss 0.602889\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5335067179467943\n",
      "Epoch 22: val loss 0.606066\n",
      "\n",
      "Epoch %d: train loss %f 23 0.5165779789288839\n",
      "Epoch 23: val loss 0.593210\n",
      "\n",
      "Epoch %d: train loss %f 24 0.516486237446467\n",
      "Epoch 24: val loss 0.597329\n",
      "\n",
      "Epoch %d: train loss %f 25 0.5174264676041074\n",
      "Epoch 25: val loss 0.597418\n",
      "\n",
      "Epoch %d: train loss %f 26 0.5164107746548123\n",
      "Epoch 26: val loss 0.606977\n",
      "\n",
      "Epoch %d: train loss %f 27 0.5162583788235983\n",
      "Epoch 27: val loss 0.594456\n",
      "\n",
      "Epoch %d: train loss %f 28 0.49699929025438094\n",
      "Epoch 28: val loss 0.622520\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4910566012064616\n",
      "Epoch 29: val loss 0.595574\n",
      "\n",
      "Epoch %d: train loss %f 30 0.4882686469289992\n",
      "Epoch 30: val loss 0.586256\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4922366605864631\n",
      "Epoch 31: val loss 0.594097\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4932029942671458\n",
      "Epoch 32: val loss 0.596219\n",
      "\n",
      "Epoch %d: train loss %f 33 0.500860763920678\n",
      "Epoch 33: val loss 0.573475\n",
      "\n",
      "Epoch %d: train loss %f 34 0.4835109743807051\n",
      "Epoch 34: val loss 0.592875\n",
      "\n",
      "Epoch %d: train loss %f 35 0.47180215186542934\n",
      "Epoch 35: val loss 0.628227\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4771958655781216\n",
      "Epoch 36: val loss 0.618811\n",
      "\n",
      "Epoch %d: train loss %f 37 0.45871821377012467\n",
      "Epoch 37: val loss 0.607958\n",
      "\n",
      "Epoch %d: train loss %f 38 0.4626839260260264\n",
      "Epoch 38: val loss 0.632366\n",
      "\n",
      "Epoch %d: train loss %f 39 0.44122132990095353\n",
      "Epoch 39: val loss 0.657055\n",
      "\n",
      "Epoch %d: train loss %f 40 0.4453300072087182\n",
      "Epoch 40: val loss 0.630612\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4445159998204973\n",
      "Epoch 41: val loss 0.612548\n",
      "\n",
      "Epoch %d: train loss %f 42 0.46326425009303623\n",
      "Epoch 42: val loss 0.641835\n",
      "\n",
      "Epoch %d: train loss %f 43 0.44729920890596175\n",
      "Epoch 43: val loss 0.650245\n",
      "\n",
      "Epoch %d: train loss %f 44 0.42262176010343766\n",
      "Epoch 44: val loss 0.631846\n",
      "\n",
      "Epoch %d: train loss %f 45 0.4598618447780609\n",
      "Epoch 45: val loss 0.642775\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4410758382744259\n",
      "Epoch 46: val loss 0.644802\n",
      "\n",
      "Epoch %d: train loss %f 47 0.43119873603185016\n",
      "Epoch 47: val loss 0.640938\n",
      "\n",
      "Epoch %d: train loss %f 48 0.4303140838940938\n",
      "Epoch 48: val loss 0.661893\n",
      "\n",
      "Epoch %d: train loss %f 49 0.39312342802683514\n",
      "Epoch 49: val loss 0.644045\n",
      "\n",
      "Epoch %d: train loss %f 50 0.4272056056393517\n",
      "Epoch 50: val loss 0.626077\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4100925193892585\n",
      "Epoch 51: val loss 0.665169\n",
      "\n",
      "Epoch %d: train loss %f 52 0.4293624526924557\n",
      "Epoch 52: val loss 0.674931\n",
      "\n",
      "Epoch %d: train loss %f 53 0.4219544761710697\n",
      "Epoch 53: val loss 0.660983\n",
      "\n",
      "Epoch %d: train loss %f 54 0.4331841799947951\n",
      "Epoch 54: val loss 0.661795\n",
      "\n",
      "Epoch %d: train loss %f 55 0.40999875134891933\n",
      "Epoch 55: val loss 0.666415\n",
      "\n",
      "Epoch %d: train loss %f 56 0.4197709825303819\n",
      "Epoch 56: val loss 0.664253\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4014639655749003\n",
      "Epoch 57: val loss 0.658354\n",
      "\n",
      "Epoch %d: train loss %f 58 0.3711740838156806\n",
      "Epoch 58: val loss 0.680493\n",
      "\n",
      "Epoch %d: train loss %f 59 0.3858341375986735\n",
      "Epoch 59: val loss 0.675000\n",
      "\n",
      "Epoch %d: train loss %f 60 0.3801850709650252\n",
      "Epoch 60: val loss 0.687399\n",
      "\n",
      "Epoch %d: train loss %f 61 0.41486281818813747\n",
      "Epoch 61: val loss 0.684642\n",
      "\n",
      "Epoch %d: train loss %f 62 0.34031494127379525\n",
      "Epoch 62: val loss 0.681482\n",
      "\n",
      "Epoch %d: train loss %f 63 0.39142049352327984\n",
      "Epoch 63: val loss 0.697764\n",
      "\n",
      "Epoch %d: train loss %f 64 0.33303557998604244\n",
      "Epoch 64: val loss 0.686998\n",
      "\n",
      "Epoch %d: train loss %f 65 0.3689742088317871\n",
      "Epoch 65: val loss 0.705401\n",
      "\n",
      "Epoch %d: train loss %f 66 0.3802781105041504\n",
      "Epoch 66: val loss 0.694669\n",
      "\n",
      "Epoch %d: train loss %f 67 0.36565617554717594\n",
      "Epoch 67: val loss 0.711322\n",
      "\n",
      "Epoch %d: train loss %f 68 0.41822710633277893\n",
      "Epoch 68: val loss 0.667216\n",
      "\n",
      "Epoch %d: train loss %f 69 0.3733972907066345\n",
      "Epoch 69: val loss 0.690486\n",
      "\n",
      "Epoch %d: train loss %f 70 0.39152104324764675\n",
      "Epoch 70: val loss 0.704327\n",
      "\n",
      "Epoch %d: train loss %f 71 0.37749896943569183\n",
      "Epoch 71: val loss 0.711409\n",
      "\n",
      "Epoch %d: train loss %f 72 0.35208724439144135\n",
      "Epoch 72: val loss 0.743219\n",
      "\n",
      "Epoch %d: train loss %f 73 0.40230003330442643\n",
      "Epoch 73: val loss 0.706214\n",
      "\n",
      "Epoch %d: train loss %f 74 0.36168767015139264\n",
      "Epoch 74: val loss 0.707433\n",
      "\n",
      "Epoch %d: train loss %f 75 0.3836083130704032\n",
      "Epoch 75: val loss 0.711849\n",
      "\n",
      "Epoch %d: train loss %f 76 0.37295809388160706\n",
      "Epoch 76: val loss 0.702109\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3322436213493347\n",
      "Epoch 77: val loss 0.718344\n",
      "\n",
      "Epoch %d: train loss %f 78 0.38441646761364406\n",
      "Epoch 78: val loss 0.749715\n",
      "\n",
      "Epoch %d: train loss %f 79 0.35672011309199864\n",
      "Epoch 79: val loss 0.718106\n",
      "\n",
      "Epoch %d: train loss %f 80 0.38109421730041504\n",
      "Epoch 80: val loss 0.731227\n",
      "\n",
      "Epoch %d: train loss %f 81 0.3674466808636983\n",
      "Epoch 81: val loss 0.740081\n",
      "\n",
      "Epoch %d: train loss %f 82 0.39107268386416966\n",
      "Epoch 82: val loss 0.724201\n",
      "\n",
      "Epoch %d: train loss %f 83 0.3537626051240497\n",
      "Epoch 83: val loss 0.743657\n",
      "\n",
      "Epoch %d: train loss %f 84 0.3358633451991611\n",
      "Epoch 84: val loss 0.739122\n",
      "\n",
      "Epoch %d: train loss %f 85 0.3747449815273285\n",
      "Epoch 85: val loss 0.717677\n",
      "\n",
      "Epoch %d: train loss %f 86 0.39826157357957626\n",
      "Epoch 86: val loss 0.716502\n",
      "\n",
      "Epoch %d: train loss %f 87 0.3578752825657527\n",
      "Epoch 87: val loss 0.738726\n",
      "\n",
      "Epoch %d: train loss %f 88 0.36539827121628654\n",
      "Epoch 88: val loss 0.723258\n",
      "\n",
      "Epoch %d: train loss %f 89 0.3859225312868754\n",
      "Epoch 89: val loss 0.744712\n",
      "\n",
      "Epoch %d: train loss %f 90 0.32735923760467106\n",
      "Epoch 90: val loss 0.752721\n",
      "\n",
      "Epoch %d: train loss %f 91 0.3582910829120212\n",
      "Epoch 91: val loss 0.747371\n",
      "\n",
      "Epoch %d: train loss %f 92 0.35322634047932094\n",
      "Epoch 92: val loss 0.741054\n",
      "\n",
      "Epoch %d: train loss %f 93 0.36356517010264927\n",
      "Epoch 93: val loss 0.744335\n",
      "\n",
      "Epoch %d: train loss %f 94 0.35869202348921037\n",
      "Epoch 94: val loss 0.785047\n",
      "\n",
      "Epoch %d: train loss %f 95 0.35555049445894027\n",
      "Epoch 95: val loss 0.752855\n",
      "\n",
      "Epoch %d: train loss %f 96 0.3537435664070977\n",
      "Epoch 96: val loss 0.761154\n",
      "\n",
      "Epoch %d: train loss %f 97 0.35448242392804885\n",
      "Epoch 97: val loss 0.728733\n",
      "\n",
      "Epoch %d: train loss %f 98 0.34589072730806136\n",
      "Epoch 98: val loss 0.755518\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3569049636522929\n",
      "Epoch 99: val loss 0.770605\n",
      "\n",
      "Epoch %d: train loss %f 100 0.3555256923039754\n",
      "Epoch 100: val loss 0.775974\n",
      "\n",
      "Epoch %d: train loss %f 101 0.35222838819026947\n",
      "Epoch 101: val loss 0.776935\n",
      "\n",
      "Epoch %d: train loss %f 102 0.30302830537160236\n",
      "Epoch 102: val loss 0.773255\n",
      "\n",
      "Epoch %d: train loss %f 103 0.35920223593711853\n",
      "Epoch 103: val loss 0.790983\n",
      "\n",
      "Epoch %d: train loss %f 104 0.3213979767428504\n",
      "Epoch 104: val loss 0.764619\n",
      "\n",
      "Epoch %d: train loss %f 105 0.28631840811835396\n",
      "Epoch 105: val loss 0.775966\n",
      "\n",
      "Epoch %d: train loss %f 106 0.3416787286599477\n",
      "Epoch 106: val loss 0.794905\n",
      "\n",
      "Epoch %d: train loss %f 107 0.32432277831766343\n",
      "Epoch 107: val loss 0.800573\n",
      "\n",
      "Epoch %d: train loss %f 108 0.3288762635654873\n",
      "Epoch 108: val loss 0.762865\n",
      "\n",
      "Epoch %d: train loss %f 109 0.2980172584454219\n",
      "Epoch 109: val loss 0.767135\n",
      "\n",
      "Epoch %d: train loss %f 110 0.336076988114251\n",
      "Epoch 110: val loss 0.801047\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3135979672273\n",
      "Epoch 111: val loss 0.781154\n",
      "\n",
      "Epoch %d: train loss %f 112 0.3703020579285092\n",
      "Epoch 112: val loss 0.855238\n",
      "\n",
      "Epoch %d: train loss %f 113 0.34843506912390393\n",
      "Epoch 113: val loss 0.783204\n",
      "\n",
      "Epoch %d: train loss %f 114 0.2981531222661336\n",
      "Epoch 114: val loss 0.817469\n",
      "\n",
      "Epoch %d: train loss %f 115 0.32301946315500474\n",
      "Epoch 115: val loss 0.845720\n",
      "\n",
      "Epoch %d: train loss %f 116 0.33675852252377403\n",
      "Epoch 116: val loss 0.786282\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3151317172580295\n",
      "Epoch 117: val loss 0.833382\n",
      "\n",
      "Epoch %d: train loss %f 118 0.3288869775003857\n",
      "Epoch 118: val loss 0.864706\n",
      "\n",
      "Epoch %d: train loss %f 119 0.3241890072822571\n",
      "Epoch 119: val loss 0.876622\n",
      "\n",
      "Epoch %d: train loss %f 120 0.3197580658727222\n",
      "Epoch 120: val loss 0.848628\n",
      "\n",
      "Epoch %d: train loss %f 121 0.2946462316645516\n",
      "Epoch 121: val loss 0.875979\n",
      "\n",
      "Epoch %d: train loss %f 122 0.31940678589873844\n",
      "Epoch 122: val loss 0.831655\n",
      "\n",
      "Epoch %d: train loss %f 123 0.3343144721455044\n",
      "Epoch 123: val loss 0.818294\n",
      "\n",
      "Epoch %d: train loss %f 124 0.32321904599666595\n",
      "Epoch 124: val loss 0.866126\n",
      "\n",
      "Epoch %d: train loss %f 125 0.34936924278736115\n",
      "Epoch 125: val loss 0.880008\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3076520330376095\n",
      "Epoch 126: val loss 0.840572\n",
      "\n",
      "Epoch %d: train loss %f 127 0.35812729597091675\n",
      "Epoch 127: val loss 0.830909\n",
      "\n",
      "Epoch %d: train loss %f 128 0.3128221415811115\n",
      "Epoch 128: val loss 0.862444\n",
      "\n",
      "Epoch %d: train loss %f 129 0.29734401570426094\n",
      "Epoch 129: val loss 0.871090\n",
      "\n",
      "Epoch %d: train loss %f 130 0.32042091919316185\n",
      "Epoch 130: val loss 0.829804\n",
      "\n",
      "Epoch %d: train loss %f 131 0.34139514134989846\n",
      "Epoch 131: val loss 0.875918\n",
      "\n",
      "Epoch %d: train loss %f 132 0.3409322251876195\n",
      "Epoch 132: val loss 0.841458\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3354626132382287\n",
      "Epoch 133: val loss 0.864884\n",
      "\n",
      "Epoch %d: train loss %f 134 0.30026141968038345\n",
      "Epoch 134: val loss 0.887966\n",
      "\n",
      "Epoch %d: train loss %f 135 0.27854948739210766\n",
      "Epoch 135: val loss 0.852482\n",
      "\n",
      "Epoch %d: train loss %f 136 0.31337492499086594\n",
      "Epoch 136: val loss 0.862228\n",
      "\n",
      "Epoch %d: train loss %f 137 0.2773798058430354\n",
      "Epoch 137: val loss 0.890915\n",
      "\n",
      "Epoch %d: train loss %f 138 0.2917272349198659\n",
      "Epoch 138: val loss 0.887769\n",
      "\n",
      "Epoch %d: train loss %f 139 0.3108898152907689\n",
      "Epoch 139: val loss 0.882576\n",
      "\n",
      "Epoch %d: train loss %f 140 0.2994615783294042\n",
      "Epoch 140: val loss 0.898339\n",
      "\n",
      "Epoch %d: train loss %f 141 0.2891271528270509\n",
      "Epoch 141: val loss 0.842631\n",
      "\n",
      "Epoch %d: train loss %f 142 0.29494841562377083\n",
      "Epoch 142: val loss 0.870061\n",
      "\n",
      "Epoch %d: train loss %f 143 0.32590775854057735\n",
      "Epoch 143: val loss 0.887842\n",
      "\n",
      "Epoch %d: train loss %f 144 0.33692552314864266\n",
      "Epoch 144: val loss 0.881934\n",
      "\n",
      "Epoch %d: train loss %f 145 0.35216270718309617\n",
      "Epoch 145: val loss 0.884824\n",
      "\n",
      "Epoch %d: train loss %f 146 0.3120512101385329\n",
      "Epoch 146: val loss 0.903742\n",
      "\n",
      "Epoch %d: train loss %f 147 0.33604562116993797\n",
      "Epoch 147: val loss 0.913025\n",
      "\n",
      "Epoch %d: train loss %f 148 0.2848929448260201\n",
      "Epoch 148: val loss 0.887474\n",
      "\n",
      "Epoch %d: train loss %f 149 0.2831321391794417\n",
      "Epoch 149: val loss 0.915667\n",
      "\n",
      "Epoch %d: train loss %f 150 0.2891094105111228\n",
      "Epoch 150: val loss 0.923161\n",
      "\n",
      "Epoch %d: train loss %f 151 0.26092974179320866\n",
      "Epoch 151: val loss 0.941606\n",
      "\n",
      "Epoch %d: train loss %f 152 0.29476796090602875\n",
      "Epoch 152: val loss 0.892939\n",
      "\n",
      "Epoch %d: train loss %f 153 0.25536510679456925\n",
      "Epoch 153: val loss 0.949975\n",
      "\n",
      "Epoch %d: train loss %f 154 0.2912333673901028\n",
      "Epoch 154: val loss 0.959361\n",
      "\n",
      "Epoch %d: train loss %f 155 0.28813815944724613\n",
      "Epoch 155: val loss 0.915701\n",
      "\n",
      "Epoch %d: train loss %f 156 0.34613580505053204\n",
      "Epoch 156: val loss 0.981835\n",
      "\n",
      "Epoch %d: train loss %f 157 0.28991307152642143\n",
      "Epoch 157: val loss 0.976137\n",
      "\n",
      "Epoch %d: train loss %f 158 0.27688826951715684\n",
      "Epoch 158: val loss 1.027302\n",
      "\n",
      "Epoch %d: train loss %f 159 0.2668788896666633\n",
      "Epoch 159: val loss 1.008464\n",
      "\n",
      "Epoch %d: train loss %f 160 0.31447545025083756\n",
      "Epoch 160: val loss 0.969245\n",
      "\n",
      "Epoch %d: train loss %f 161 0.29944903486304814\n",
      "Epoch 161: val loss 0.932522\n",
      "\n",
      "Epoch %d: train loss %f 162 0.26362569795714486\n",
      "Epoch 162: val loss 1.025141\n",
      "\n",
      "Epoch %d: train loss %f 163 0.28739167584313285\n",
      "Epoch 163: val loss 0.994143\n",
      "\n",
      "Epoch %d: train loss %f 164 0.3075860407617357\n",
      "Epoch 164: val loss 0.934054\n",
      "\n",
      "Epoch %d: train loss %f 165 0.30323410862021977\n",
      "Epoch 165: val loss 0.908319\n",
      "\n",
      "Epoch %d: train loss %f 166 0.27448703596989316\n",
      "Epoch 166: val loss 1.007014\n",
      "\n",
      "Epoch %d: train loss %f 167 0.3314230442047119\n",
      "Epoch 167: val loss 0.989273\n",
      "\n",
      "Epoch %d: train loss %f 168 0.28487365278932786\n",
      "Epoch 168: val loss 0.950797\n",
      "\n",
      "Epoch %d: train loss %f 169 0.27652360333336723\n",
      "Epoch 169: val loss 0.941905\n",
      "\n",
      "Epoch %d: train loss %f 170 0.27040303084585404\n",
      "Epoch 170: val loss 0.970601\n",
      "\n",
      "Epoch %d: train loss %f 171 0.3013170642985238\n",
      "Epoch 171: val loss 0.980370\n",
      "\n",
      "Epoch %d: train loss %f 172 0.29938170313835144\n",
      "Epoch 172: val loss 0.926180\n",
      "\n",
      "Epoch %d: train loss %f 173 0.2848435921801461\n",
      "Epoch 173: val loss 0.931631\n",
      "\n",
      "Epoch %d: train loss %f 174 0.28339922924836475\n",
      "Epoch 174: val loss 0.945742\n",
      "\n",
      "Epoch %d: train loss %f 175 0.268956270482805\n",
      "Epoch 175: val loss 0.945678\n",
      "\n",
      "Epoch %d: train loss %f 176 0.27207549578613704\n",
      "Epoch 176: val loss 0.960919\n",
      "\n",
      "Epoch %d: train loss %f 177 0.25811577174398637\n",
      "Epoch 177: val loss 1.017207\n",
      "\n",
      "Epoch %d: train loss %f 178 0.2869996163580153\n",
      "Epoch 178: val loss 0.979685\n",
      "\n",
      "Epoch %d: train loss %f 179 0.24744004838996464\n",
      "Epoch 179: val loss 1.027367\n",
      "\n",
      "Epoch %d: train loss %f 180 0.29132859739992356\n",
      "Epoch 180: val loss 1.000217\n",
      "\n",
      "Epoch %d: train loss %f 181 0.2838961184024811\n",
      "Epoch 181: val loss 1.022493\n",
      "\n",
      "Epoch %d: train loss %f 182 0.2747666322522693\n",
      "Epoch 182: val loss 1.050534\n",
      "\n",
      "Epoch %d: train loss %f 183 0.26584963003794354\n",
      "Epoch 183: val loss 1.008631\n",
      "\n",
      "Epoch %d: train loss %f 184 0.27751054366429645\n",
      "Epoch 184: val loss 1.015283\n",
      "\n",
      "Epoch %d: train loss %f 185 0.29368214971489376\n",
      "Epoch 185: val loss 1.091056\n",
      "\n",
      "Epoch %d: train loss %f 186 0.2600925879346\n",
      "Epoch 186: val loss 1.085042\n",
      "\n",
      "Epoch %d: train loss %f 187 0.27683624459637535\n",
      "Epoch 187: val loss 1.002976\n",
      "\n",
      "Epoch %d: train loss %f 188 0.29907650748888653\n",
      "Epoch 188: val loss 1.106546\n",
      "\n",
      "Epoch %d: train loss %f 189 0.30318426920308006\n",
      "Epoch 189: val loss 1.087322\n",
      "\n",
      "Epoch %d: train loss %f 190 0.28615832328796387\n",
      "Epoch 190: val loss 1.065872\n",
      "\n",
      "Epoch %d: train loss %f 191 0.27238503595193225\n",
      "Epoch 191: val loss 1.049980\n",
      "\n",
      "Epoch %d: train loss %f 192 0.24588310221831003\n",
      "Epoch 192: val loss 0.989850\n",
      "\n",
      "Epoch %d: train loss %f 193 0.25251706772380406\n",
      "Epoch 193: val loss 1.058327\n",
      "\n",
      "Epoch %d: train loss %f 194 0.25746284590827095\n",
      "Epoch 194: val loss 1.136708\n",
      "\n",
      "Epoch %d: train loss %f 195 0.23914938502841526\n",
      "Epoch 195: val loss 1.048047\n",
      "\n",
      "Epoch %d: train loss %f 196 0.3247966716686885\n",
      "Epoch 196: val loss 1.085894\n",
      "\n",
      "Epoch %d: train loss %f 197 0.26895494759082794\n",
      "Epoch 197: val loss 1.086585\n",
      "\n",
      "Epoch %d: train loss %f 198 0.2504689147075017\n",
      "Epoch 198: val loss 1.070108\n",
      "\n",
      "Epoch %d: train loss %f 199 0.2505662540594737\n",
      "Epoch 199: val loss 1.060412\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6943406131532457\n",
      "Epoch 0: val loss 0.694359\n",
      "\n",
      "Epoch %d: train loss %f 1 0.689815534485711\n",
      "Epoch 1: val loss 0.693290\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6868233879407247\n",
      "Epoch 2: val loss 0.691186\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6826616260740492\n",
      "Epoch 3: val loss 0.687077\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6769046849674649\n",
      "Epoch 4: val loss 0.679908\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6696773105197482\n",
      "Epoch 5: val loss 0.668084\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6637884179751078\n",
      "Epoch 6: val loss 0.654004\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6468333005905151\n",
      "Epoch 7: val loss 0.635132\n",
      "\n",
      "Epoch %d: train loss %f 8 0.6413120163811578\n",
      "Epoch 8: val loss 0.618172\n",
      "\n",
      "Epoch %d: train loss %f 9 0.6287748350037469\n",
      "Epoch 9: val loss 0.602186\n",
      "\n",
      "Epoch %d: train loss %f 10 0.607458319928911\n",
      "Epoch 10: val loss 0.581573\n",
      "\n",
      "Epoch %d: train loss %f 11 0.6090254717402988\n",
      "Epoch 11: val loss 0.572785\n",
      "\n",
      "Epoch %d: train loss %f 12 0.591327879163954\n",
      "Epoch 12: val loss 0.562059\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5850674907366434\n",
      "Epoch 13: val loss 0.550955\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5646636022461785\n",
      "Epoch 14: val loss 0.549111\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5709143016073439\n",
      "Epoch 15: val loss 0.547236\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5559654235839844\n",
      "Epoch 16: val loss 0.534019\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5608366231123606\n",
      "Epoch 17: val loss 0.530257\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5545579294363657\n",
      "Epoch 18: val loss 0.534035\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5354530215263367\n",
      "Epoch 19: val loss 0.530822\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5460651516914368\n",
      "Epoch 20: val loss 0.529731\n",
      "\n",
      "Epoch %d: train loss %f 21 0.539041668176651\n",
      "Epoch 21: val loss 0.535262\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5231412318017747\n",
      "Epoch 22: val loss 0.528521\n",
      "\n",
      "Epoch %d: train loss %f 23 0.532166408167945\n",
      "Epoch 23: val loss 0.523975\n",
      "\n",
      "Epoch %d: train loss %f 24 0.512081997262107\n",
      "Epoch 24: val loss 0.512907\n",
      "\n",
      "Epoch %d: train loss %f 25 0.5180893076790704\n",
      "Epoch 25: val loss 0.519564\n",
      "\n",
      "Epoch %d: train loss %f 26 0.5341147747304704\n",
      "Epoch 26: val loss 0.517715\n",
      "\n",
      "Epoch %d: train loss %f 27 0.5169446369012197\n",
      "Epoch 27: val loss 0.521437\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4928220378028022\n",
      "Epoch 28: val loss 0.517777\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4961341420809428\n",
      "Epoch 29: val loss 0.523635\n",
      "\n",
      "Epoch %d: train loss %f 30 0.49025366372532314\n",
      "Epoch 30: val loss 0.519217\n",
      "\n",
      "Epoch %d: train loss %f 31 0.48727180891566807\n",
      "Epoch 31: val loss 0.518901\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4928118851449754\n",
      "Epoch 32: val loss 0.517780\n",
      "\n",
      "Epoch %d: train loss %f 33 0.47712209820747375\n",
      "Epoch 33: val loss 0.522848\n",
      "\n",
      "Epoch %d: train loss %f 34 0.48812637726465863\n",
      "Epoch 34: val loss 0.521928\n",
      "\n",
      "Epoch %d: train loss %f 35 0.456336604224311\n",
      "Epoch 35: val loss 0.514507\n",
      "\n",
      "Epoch %d: train loss %f 36 0.462177359395557\n",
      "Epoch 36: val loss 0.517281\n",
      "\n",
      "Epoch %d: train loss %f 37 0.46820547845628524\n",
      "Epoch 37: val loss 0.512910\n",
      "\n",
      "Epoch %d: train loss %f 38 0.45960156785117257\n",
      "Epoch 38: val loss 0.518360\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4574130111270481\n",
      "Epoch 39: val loss 0.525774\n",
      "\n",
      "Epoch %d: train loss %f 40 0.46256115701463485\n",
      "Epoch 40: val loss 0.528064\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4596696694691976\n",
      "Epoch 41: val loss 0.519694\n",
      "\n",
      "Epoch %d: train loss %f 42 0.46581389175521004\n",
      "Epoch 42: val loss 0.518928\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4639865855375926\n",
      "Epoch 43: val loss 0.520257\n",
      "\n",
      "Epoch %d: train loss %f 44 0.4707135160764058\n",
      "Epoch 44: val loss 0.526877\n",
      "\n",
      "Epoch %d: train loss %f 45 0.4568810330496894\n",
      "Epoch 45: val loss 0.531868\n",
      "\n",
      "Epoch %d: train loss %f 46 0.440545873509513\n",
      "Epoch 46: val loss 0.538699\n",
      "\n",
      "Epoch %d: train loss %f 47 0.4482149150636461\n",
      "Epoch 47: val loss 0.518020\n",
      "\n",
      "Epoch %d: train loss %f 48 0.4213416543271806\n",
      "Epoch 48: val loss 0.519971\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4653499590026008\n",
      "Epoch 49: val loss 0.530010\n",
      "\n",
      "Epoch %d: train loss %f 50 0.45124543375439113\n",
      "Epoch 50: val loss 0.545103\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4325099156962501\n",
      "Epoch 51: val loss 0.533355\n",
      "\n",
      "Epoch %d: train loss %f 52 0.43792855739593506\n",
      "Epoch 52: val loss 0.539108\n",
      "\n",
      "Epoch %d: train loss %f 53 0.42294081383281285\n",
      "Epoch 53: val loss 0.527690\n",
      "\n",
      "Epoch %d: train loss %f 54 0.44400643640094334\n",
      "Epoch 54: val loss 0.538278\n",
      "\n",
      "Epoch %d: train loss %f 55 0.39959970778889126\n",
      "Epoch 55: val loss 0.536398\n",
      "\n",
      "Epoch %d: train loss %f 56 0.4153444833225674\n",
      "Epoch 56: val loss 0.523273\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4475083284907871\n",
      "Epoch 57: val loss 0.526430\n",
      "\n",
      "Epoch %d: train loss %f 58 0.4084469543562995\n",
      "Epoch 58: val loss 0.538591\n",
      "\n",
      "Epoch %d: train loss %f 59 0.44532809986008537\n",
      "Epoch 59: val loss 0.546317\n",
      "\n",
      "Epoch %d: train loss %f 60 0.43762606713506913\n",
      "Epoch 60: val loss 0.532279\n",
      "\n",
      "Epoch %d: train loss %f 61 0.4209911889500088\n",
      "Epoch 61: val loss 0.542450\n",
      "\n",
      "Epoch %d: train loss %f 62 0.3847928047180176\n",
      "Epoch 62: val loss 0.551908\n",
      "\n",
      "Epoch %d: train loss %f 63 0.37388652231958175\n",
      "Epoch 63: val loss 0.546812\n",
      "\n",
      "Epoch %d: train loss %f 64 0.3773915767669678\n",
      "Epoch 64: val loss 0.549199\n",
      "\n",
      "Epoch %d: train loss %f 65 0.410004585981369\n",
      "Epoch 65: val loss 0.547460\n",
      "\n",
      "Epoch %d: train loss %f 66 0.398964610364702\n",
      "Epoch 66: val loss 0.543891\n",
      "\n",
      "Epoch %d: train loss %f 67 0.3910068803363376\n",
      "Epoch 67: val loss 0.533727\n",
      "\n",
      "Epoch %d: train loss %f 68 0.3888164659341176\n",
      "Epoch 68: val loss 0.565857\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4187759459018707\n",
      "Epoch 69: val loss 0.559677\n",
      "\n",
      "Epoch %d: train loss %f 70 0.3627430631054772\n",
      "Epoch 70: val loss 0.557565\n",
      "\n",
      "Epoch %d: train loss %f 71 0.40898090932104325\n",
      "Epoch 71: val loss 0.544244\n",
      "\n",
      "Epoch %d: train loss %f 72 0.3713626265525818\n",
      "Epoch 72: val loss 0.542153\n",
      "\n",
      "Epoch %d: train loss %f 73 0.39959514141082764\n",
      "Epoch 73: val loss 0.552231\n",
      "\n",
      "Epoch %d: train loss %f 74 0.40953944954607224\n",
      "Epoch 74: val loss 0.549251\n",
      "\n",
      "Epoch %d: train loss %f 75 0.36477862629625535\n",
      "Epoch 75: val loss 0.554526\n",
      "\n",
      "Epoch %d: train loss %f 76 0.38975123895539177\n",
      "Epoch 76: val loss 0.581307\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3997866776254442\n",
      "Epoch 77: val loss 0.575880\n",
      "\n",
      "Epoch %d: train loss %f 78 0.4173717035187615\n",
      "Epoch 78: val loss 0.586744\n",
      "\n",
      "Epoch %d: train loss %f 79 0.3632798261112637\n",
      "Epoch 79: val loss 0.565799\n",
      "\n",
      "Epoch %d: train loss %f 80 0.3854103088378906\n",
      "Epoch 80: val loss 0.558430\n",
      "\n",
      "Epoch %d: train loss %f 81 0.39004850222004783\n",
      "Epoch 81: val loss 0.577882\n",
      "\n",
      "Epoch %d: train loss %f 82 0.40513977739546037\n",
      "Epoch 82: val loss 0.585441\n",
      "\n",
      "Epoch %d: train loss %f 83 0.4052115993367301\n",
      "Epoch 83: val loss 0.560784\n",
      "\n",
      "Epoch %d: train loss %f 84 0.36748342712720233\n",
      "Epoch 84: val loss 0.576991\n",
      "\n",
      "Epoch %d: train loss %f 85 0.38602857126129997\n",
      "Epoch 85: val loss 0.597610\n",
      "\n",
      "Epoch %d: train loss %f 86 0.3716826339562734\n",
      "Epoch 86: val loss 0.584775\n",
      "\n",
      "Epoch %d: train loss %f 87 0.3509312603208754\n",
      "Epoch 87: val loss 0.560165\n",
      "\n",
      "Epoch %d: train loss %f 88 0.34633703033129376\n",
      "Epoch 88: val loss 0.571872\n",
      "\n",
      "Epoch %d: train loss %f 89 0.3479433258374532\n",
      "Epoch 89: val loss 0.573890\n",
      "\n",
      "Epoch %d: train loss %f 90 0.399407082133823\n",
      "Epoch 90: val loss 0.573934\n",
      "\n",
      "Epoch %d: train loss %f 91 0.3737141556209988\n",
      "Epoch 91: val loss 0.583637\n",
      "\n",
      "Epoch %d: train loss %f 92 0.34986822969383663\n",
      "Epoch 92: val loss 0.577791\n",
      "\n",
      "Epoch %d: train loss %f 93 0.360118235150973\n",
      "Epoch 93: val loss 0.582932\n",
      "\n",
      "Epoch %d: train loss %f 94 0.37801043192545575\n",
      "Epoch 94: val loss 0.570371\n",
      "\n",
      "Epoch %d: train loss %f 95 0.3463171703947915\n",
      "Epoch 95: val loss 0.568940\n",
      "\n",
      "Epoch %d: train loss %f 96 0.3527142372396257\n",
      "Epoch 96: val loss 0.577213\n",
      "\n",
      "Epoch %d: train loss %f 97 0.3559915026028951\n",
      "Epoch 97: val loss 0.560006\n",
      "\n",
      "Epoch %d: train loss %f 98 0.4034898844030168\n",
      "Epoch 98: val loss 0.564854\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3885318371984694\n",
      "Epoch 99: val loss 0.623233\n",
      "\n",
      "Epoch %d: train loss %f 100 0.32892367078198326\n",
      "Epoch 100: val loss 0.582176\n",
      "\n",
      "Epoch %d: train loss %f 101 0.37465880314509076\n",
      "Epoch 101: val loss 0.563851\n",
      "\n",
      "Epoch %d: train loss %f 102 0.35110770331488717\n",
      "Epoch 102: val loss 0.596047\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3284224536683824\n",
      "Epoch 103: val loss 0.589150\n",
      "\n",
      "Epoch %d: train loss %f 104 0.34256841407881844\n",
      "Epoch 104: val loss 0.613763\n",
      "\n",
      "Epoch %d: train loss %f 105 0.34881198075082565\n",
      "Epoch 105: val loss 0.604372\n",
      "\n",
      "Epoch %d: train loss %f 106 0.32932481666405994\n",
      "Epoch 106: val loss 0.582700\n",
      "\n",
      "Epoch %d: train loss %f 107 0.36871105432510376\n",
      "Epoch 107: val loss 0.586278\n",
      "\n",
      "Epoch %d: train loss %f 108 0.35548468430836994\n",
      "Epoch 108: val loss 0.593994\n",
      "\n",
      "Epoch %d: train loss %f 109 0.3717451790968577\n",
      "Epoch 109: val loss 0.624667\n",
      "\n",
      "Epoch %d: train loss %f 110 0.3312108897500568\n",
      "Epoch 110: val loss 0.594467\n",
      "\n",
      "Epoch %d: train loss %f 111 0.30181925495465595\n",
      "Epoch 111: val loss 0.592577\n",
      "\n",
      "Epoch %d: train loss %f 112 0.30098028315438163\n",
      "Epoch 112: val loss 0.592061\n",
      "\n",
      "Epoch %d: train loss %f 113 0.34699416160583496\n",
      "Epoch 113: val loss 0.618271\n",
      "\n",
      "Epoch %d: train loss %f 114 0.3565187586678399\n",
      "Epoch 114: val loss 0.591603\n",
      "\n",
      "Epoch %d: train loss %f 115 0.3478049056397544\n",
      "Epoch 115: val loss 0.608687\n",
      "\n",
      "Epoch %d: train loss %f 116 0.34127025802930194\n",
      "Epoch 116: val loss 0.633534\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3532556907998191\n",
      "Epoch 117: val loss 0.632999\n",
      "\n",
      "Epoch %d: train loss %f 118 0.3445080916086833\n",
      "Epoch 118: val loss 0.589517\n",
      "\n",
      "Epoch %d: train loss %f 119 0.3386978142791324\n",
      "Epoch 119: val loss 0.601680\n",
      "\n",
      "Epoch %d: train loss %f 120 0.35527390076054466\n",
      "Epoch 120: val loss 0.606910\n",
      "\n",
      "Epoch %d: train loss %f 121 0.31958944267696804\n",
      "Epoch 121: val loss 0.605081\n",
      "\n",
      "Epoch %d: train loss %f 122 0.3521761761771308\n",
      "Epoch 122: val loss 0.605750\n",
      "\n",
      "Epoch %d: train loss %f 123 0.34447604252232444\n",
      "Epoch 123: val loss 0.612025\n",
      "\n",
      "Epoch %d: train loss %f 124 0.3172928144534429\n",
      "Epoch 124: val loss 0.622401\n",
      "\n",
      "Epoch %d: train loss %f 125 0.34496233032809365\n",
      "Epoch 125: val loss 0.609030\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3250101837846968\n",
      "Epoch 126: val loss 0.619202\n",
      "\n",
      "Epoch %d: train loss %f 127 0.3281378861930635\n",
      "Epoch 127: val loss 0.646029\n",
      "\n",
      "Epoch %d: train loss %f 128 0.3242959976196289\n",
      "Epoch 128: val loss 0.617721\n",
      "\n",
      "Epoch %d: train loss %f 129 0.32990244030952454\n",
      "Epoch 129: val loss 0.628134\n",
      "\n",
      "Epoch %d: train loss %f 130 0.31197865969604915\n",
      "Epoch 130: val loss 0.621094\n",
      "\n",
      "Epoch %d: train loss %f 131 0.30012497968143886\n",
      "Epoch 131: val loss 0.634941\n",
      "\n",
      "Epoch %d: train loss %f 132 0.2883723990784751\n",
      "Epoch 132: val loss 0.658176\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3236418267091115\n",
      "Epoch 133: val loss 0.640423\n",
      "\n",
      "Epoch %d: train loss %f 134 0.30793796810838914\n",
      "Epoch 134: val loss 0.667180\n",
      "\n",
      "Epoch %d: train loss %f 135 0.3203480972184075\n",
      "Epoch 135: val loss 0.690611\n",
      "\n",
      "Epoch %d: train loss %f 136 0.3177766187323464\n",
      "Epoch 136: val loss 0.656150\n",
      "\n",
      "Epoch %d: train loss %f 137 0.3139460219277276\n",
      "Epoch 137: val loss 0.627070\n",
      "\n",
      "Epoch %d: train loss %f 138 0.28922371400727165\n",
      "Epoch 138: val loss 0.641355\n",
      "\n",
      "Epoch %d: train loss %f 139 0.3214818669690026\n",
      "Epoch 139: val loss 0.678726\n",
      "\n",
      "Epoch %d: train loss %f 140 0.34780647688441807\n",
      "Epoch 140: val loss 0.687552\n",
      "\n",
      "Epoch %d: train loss %f 141 0.3201057149304284\n",
      "Epoch 141: val loss 0.654393\n",
      "\n",
      "Epoch %d: train loss %f 142 0.30706309609942967\n",
      "Epoch 142: val loss 0.652829\n",
      "\n",
      "Epoch %d: train loss %f 143 0.33747753004233044\n",
      "Epoch 143: val loss 0.632393\n",
      "\n",
      "Epoch %d: train loss %f 144 0.28879043956597644\n",
      "Epoch 144: val loss 0.641529\n",
      "\n",
      "Epoch %d: train loss %f 145 0.2948676066266166\n",
      "Epoch 145: val loss 0.666652\n",
      "\n",
      "Epoch %d: train loss %f 146 0.2687696764866511\n",
      "Epoch 146: val loss 0.660435\n",
      "\n",
      "Epoch %d: train loss %f 147 0.28836579124132794\n",
      "Epoch 147: val loss 0.645594\n",
      "\n",
      "Epoch %d: train loss %f 148 0.3149781972169876\n",
      "Epoch 148: val loss 0.632502\n",
      "\n",
      "Epoch %d: train loss %f 149 0.3193543040090137\n",
      "Epoch 149: val loss 0.658451\n",
      "\n",
      "Epoch %d: train loss %f 150 0.31058607664373183\n",
      "Epoch 150: val loss 0.666769\n",
      "\n",
      "Epoch %d: train loss %f 151 0.32180268731382156\n",
      "Epoch 151: val loss 0.670500\n",
      "\n",
      "Epoch %d: train loss %f 152 0.3110686159796185\n",
      "Epoch 152: val loss 0.660063\n",
      "\n",
      "Epoch %d: train loss %f 153 0.28088311271535027\n",
      "Epoch 153: val loss 0.678801\n",
      "\n",
      "Epoch %d: train loss %f 154 0.3020482212305069\n",
      "Epoch 154: val loss 0.713027\n",
      "\n",
      "Epoch %d: train loss %f 155 0.26671038568019867\n",
      "Epoch 155: val loss 0.682436\n",
      "\n",
      "Epoch %d: train loss %f 156 0.31149978107876247\n",
      "Epoch 156: val loss 0.680568\n",
      "\n",
      "Epoch %d: train loss %f 157 0.3528121809164683\n",
      "Epoch 157: val loss 0.658653\n",
      "\n",
      "Epoch %d: train loss %f 158 0.25677671531836194\n",
      "Epoch 158: val loss 0.640240\n",
      "\n",
      "Epoch %d: train loss %f 159 0.2905346502860387\n",
      "Epoch 159: val loss 0.630978\n",
      "\n",
      "Epoch %d: train loss %f 160 0.277985875805219\n",
      "Epoch 160: val loss 0.654705\n",
      "\n",
      "Epoch %d: train loss %f 161 0.33845554126633537\n",
      "Epoch 161: val loss 0.691578\n",
      "\n",
      "Epoch %d: train loss %f 162 0.342542654938168\n",
      "Epoch 162: val loss 0.689925\n",
      "\n",
      "Epoch %d: train loss %f 163 0.27405568129486507\n",
      "Epoch 163: val loss 0.660534\n",
      "\n",
      "Epoch %d: train loss %f 164 0.3019147829877006\n",
      "Epoch 164: val loss 0.683086\n",
      "\n",
      "Epoch %d: train loss %f 165 0.30238243440787\n",
      "Epoch 165: val loss 0.691393\n",
      "\n",
      "Epoch %d: train loss %f 166 0.24753780166308084\n",
      "Epoch 166: val loss 0.685527\n",
      "\n",
      "Epoch %d: train loss %f 167 0.26085039145416683\n",
      "Epoch 167: val loss 0.680925\n",
      "\n",
      "Epoch %d: train loss %f 168 0.3010239866044786\n",
      "Epoch 168: val loss 0.683939\n",
      "\n",
      "Epoch %d: train loss %f 169 0.31098200215233696\n",
      "Epoch 169: val loss 0.694227\n",
      "\n",
      "Epoch %d: train loss %f 170 0.32473302715354496\n",
      "Epoch 170: val loss 0.679055\n",
      "\n",
      "Epoch %d: train loss %f 171 0.30551769336064655\n",
      "Epoch 171: val loss 0.655121\n",
      "\n",
      "Epoch %d: train loss %f 172 0.28475407924917007\n",
      "Epoch 172: val loss 0.700607\n",
      "\n",
      "Epoch %d: train loss %f 173 0.2892955028348499\n",
      "Epoch 173: val loss 0.719412\n",
      "\n",
      "Epoch %d: train loss %f 174 0.32361845506562126\n",
      "Epoch 174: val loss 0.699282\n",
      "\n",
      "Epoch %d: train loss %f 175 0.2801000128189723\n",
      "Epoch 175: val loss 0.717191\n",
      "\n",
      "Epoch %d: train loss %f 176 0.32259099351035225\n",
      "Epoch 176: val loss 0.722411\n",
      "\n",
      "Epoch %d: train loss %f 177 0.31637343929873574\n",
      "Epoch 177: val loss 0.708425\n",
      "\n",
      "Epoch %d: train loss %f 178 0.27683114674356246\n",
      "Epoch 178: val loss 0.697860\n",
      "\n",
      "Epoch %d: train loss %f 179 0.2826843708753586\n",
      "Epoch 179: val loss 0.704120\n",
      "\n",
      "Epoch %d: train loss %f 180 0.3101895335647795\n",
      "Epoch 180: val loss 0.707170\n",
      "\n",
      "Epoch %d: train loss %f 181 0.25082919167147744\n",
      "Epoch 181: val loss 0.711785\n",
      "\n",
      "Epoch %d: train loss %f 182 0.2879849655760659\n",
      "Epoch 182: val loss 0.712740\n",
      "\n",
      "Epoch %d: train loss %f 183 0.3033597129914496\n",
      "Epoch 183: val loss 0.717080\n",
      "\n",
      "Epoch %d: train loss %f 184 0.28895315693484414\n",
      "Epoch 184: val loss 0.714012\n",
      "\n",
      "Epoch %d: train loss %f 185 0.26839183105362785\n",
      "Epoch 185: val loss 0.691597\n",
      "\n",
      "Epoch %d: train loss %f 186 0.29159465928872425\n",
      "Epoch 186: val loss 0.717627\n",
      "\n",
      "Epoch %d: train loss %f 187 0.2499873720937305\n",
      "Epoch 187: val loss 0.750612\n",
      "\n",
      "Epoch %d: train loss %f 188 0.26090580721696216\n",
      "Epoch 188: val loss 0.702086\n",
      "\n",
      "Epoch %d: train loss %f 189 0.27724936770068276\n",
      "Epoch 189: val loss 0.690961\n",
      "\n",
      "Epoch %d: train loss %f 190 0.2985585712724262\n",
      "Epoch 190: val loss 0.718543\n",
      "\n",
      "Epoch %d: train loss %f 191 0.3087318821085824\n",
      "Epoch 191: val loss 0.741179\n",
      "\n",
      "Epoch %d: train loss %f 192 0.31661856340037453\n",
      "Epoch 192: val loss 0.703365\n",
      "\n",
      "Epoch %d: train loss %f 193 0.2687871406475703\n",
      "Epoch 193: val loss 0.722783\n",
      "\n",
      "Epoch %d: train loss %f 194 0.28179146680567\n",
      "Epoch 194: val loss 0.724679\n",
      "\n",
      "Epoch %d: train loss %f 195 0.26295888755056596\n",
      "Epoch 195: val loss 0.714733\n",
      "\n",
      "Epoch %d: train loss %f 196 0.2531633178393046\n",
      "Epoch 196: val loss 0.698205\n",
      "\n",
      "Epoch %d: train loss %f 197 0.2841436035103268\n",
      "Epoch 197: val loss 0.701975\n",
      "\n",
      "Epoch %d: train loss %f 198 0.3067218065261841\n",
      "Epoch 198: val loss 0.706724\n",
      "\n",
      "Epoch %d: train loss %f 199 0.23900063335895538\n",
      "Epoch 199: val loss 0.700823\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6918264627456665\n",
      "Epoch 0: val loss 0.692184\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6884957220819261\n",
      "Epoch 1: val loss 0.691091\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6823301116625468\n",
      "Epoch 2: val loss 0.688817\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6741606261995103\n",
      "Epoch 3: val loss 0.684483\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6639321711328294\n",
      "Epoch 4: val loss 0.676447\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6453528006871542\n",
      "Epoch 5: val loss 0.662501\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6272657116254171\n",
      "Epoch 6: val loss 0.650411\n",
      "\n",
      "Epoch %d: train loss %f 7 0.611352801322937\n",
      "Epoch 7: val loss 0.646845\n",
      "\n",
      "Epoch %d: train loss %f 8 0.6015883617930942\n",
      "Epoch 8: val loss 0.643070\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5815928843286302\n",
      "Epoch 9: val loss 0.631158\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5752195351653628\n",
      "Epoch 10: val loss 0.620895\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5722129113144345\n",
      "Epoch 11: val loss 0.617375\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5643636782964071\n",
      "Epoch 12: val loss 0.613613\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5690195063749949\n",
      "Epoch 13: val loss 0.615661\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5554446478684744\n",
      "Epoch 14: val loss 0.610635\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5567763050397238\n",
      "Epoch 15: val loss 0.603236\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5394444863001505\n",
      "Epoch 16: val loss 0.594397\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5375503732098473\n",
      "Epoch 17: val loss 0.591784\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5367390513420105\n",
      "Epoch 18: val loss 0.598234\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5388442840841081\n",
      "Epoch 19: val loss 0.589310\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5218623611662123\n",
      "Epoch 20: val loss 0.582346\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5233690705564287\n",
      "Epoch 21: val loss 0.586742\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5309844348165724\n",
      "Epoch 22: val loss 0.591453\n",
      "\n",
      "Epoch %d: train loss %f 23 0.5051024523046281\n",
      "Epoch 23: val loss 0.582901\n",
      "\n",
      "Epoch %d: train loss %f 24 0.508532186349233\n",
      "Epoch 24: val loss 0.576645\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4929661552111308\n",
      "Epoch 25: val loss 0.587459\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4930489526854621\n",
      "Epoch 26: val loss 0.579320\n",
      "\n",
      "Epoch %d: train loss %f 27 0.48051320513089496\n",
      "Epoch 27: val loss 0.572319\n",
      "\n",
      "Epoch %d: train loss %f 28 0.46777789129151237\n",
      "Epoch 28: val loss 0.576956\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4740758505132463\n",
      "Epoch 29: val loss 0.577858\n",
      "\n",
      "Epoch %d: train loss %f 30 0.504314147763782\n",
      "Epoch 30: val loss 0.579721\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4828800857067108\n",
      "Epoch 31: val loss 0.573742\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4631416102250417\n",
      "Epoch 32: val loss 0.581296\n",
      "\n",
      "Epoch %d: train loss %f 33 0.4666786955462562\n",
      "Epoch 33: val loss 0.577500\n",
      "\n",
      "Epoch %d: train loss %f 34 0.4579666521814134\n",
      "Epoch 34: val loss 0.572761\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4557870030403137\n",
      "Epoch 35: val loss 0.590568\n",
      "\n",
      "Epoch %d: train loss %f 36 0.45947666962941486\n",
      "Epoch 36: val loss 0.592008\n",
      "\n",
      "Epoch %d: train loss %f 37 0.45965640743573505\n",
      "Epoch 37: val loss 0.571014\n",
      "\n",
      "Epoch %d: train loss %f 38 0.47119367453787064\n",
      "Epoch 38: val loss 0.577435\n",
      "\n",
      "Epoch %d: train loss %f 39 0.44648968511157566\n",
      "Epoch 39: val loss 0.576907\n",
      "\n",
      "Epoch %d: train loss %f 40 0.4308115773730808\n",
      "Epoch 40: val loss 0.585995\n",
      "\n",
      "Epoch %d: train loss %f 41 0.456963880194558\n",
      "Epoch 41: val loss 0.593876\n",
      "\n",
      "Epoch %d: train loss %f 42 0.42796481980217826\n",
      "Epoch 42: val loss 0.589303\n",
      "\n",
      "Epoch %d: train loss %f 43 0.42575207352638245\n",
      "Epoch 43: val loss 0.593748\n",
      "\n",
      "Epoch %d: train loss %f 44 0.43086954951286316\n",
      "Epoch 44: val loss 0.585910\n",
      "\n",
      "Epoch %d: train loss %f 45 0.4218574927912818\n",
      "Epoch 45: val loss 0.582512\n",
      "\n",
      "Epoch %d: train loss %f 46 0.42203734152846867\n",
      "Epoch 46: val loss 0.594750\n",
      "\n",
      "Epoch %d: train loss %f 47 0.3943188322914971\n",
      "Epoch 47: val loss 0.594458\n",
      "\n",
      "Epoch %d: train loss %f 48 0.417342119746738\n",
      "Epoch 48: val loss 0.608060\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4272059268421597\n",
      "Epoch 49: val loss 0.600610\n",
      "\n",
      "Epoch %d: train loss %f 50 0.435859739780426\n",
      "Epoch 50: val loss 0.620926\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4045398202207353\n",
      "Epoch 51: val loss 0.601646\n",
      "\n",
      "Epoch %d: train loss %f 52 0.4116746452119615\n",
      "Epoch 52: val loss 0.602236\n",
      "\n",
      "Epoch %d: train loss %f 53 0.38774893350071377\n",
      "Epoch 53: val loss 0.614140\n",
      "\n",
      "Epoch %d: train loss %f 54 0.38115038143263924\n",
      "Epoch 54: val loss 0.629638\n",
      "\n",
      "Epoch %d: train loss %f 55 0.3734119070900811\n",
      "Epoch 55: val loss 0.620438\n",
      "\n",
      "Epoch %d: train loss %f 56 0.4220139980316162\n",
      "Epoch 56: val loss 0.616438\n",
      "\n",
      "Epoch %d: train loss %f 57 0.43183209167586434\n",
      "Epoch 57: val loss 0.618316\n",
      "\n",
      "Epoch %d: train loss %f 58 0.3851488067044152\n",
      "Epoch 58: val loss 0.619510\n",
      "\n",
      "Epoch %d: train loss %f 59 0.391220102707545\n",
      "Epoch 59: val loss 0.601651\n",
      "\n",
      "Epoch %d: train loss %f 60 0.39846822288301254\n",
      "Epoch 60: val loss 0.601108\n",
      "\n",
      "Epoch %d: train loss %f 61 0.37153662906752694\n",
      "Epoch 61: val loss 0.613115\n",
      "\n",
      "Epoch %d: train loss %f 62 0.3674847235282262\n",
      "Epoch 62: val loss 0.643524\n",
      "\n",
      "Epoch %d: train loss %f 63 0.399600303835339\n",
      "Epoch 63: val loss 0.637102\n",
      "\n",
      "Epoch %d: train loss %f 64 0.41143012709087795\n",
      "Epoch 64: val loss 0.624781\n",
      "\n",
      "Epoch %d: train loss %f 65 0.38028233912256026\n",
      "Epoch 65: val loss 0.617356\n",
      "\n",
      "Epoch %d: train loss %f 66 0.35181116064389545\n",
      "Epoch 66: val loss 0.606314\n",
      "\n",
      "Epoch %d: train loss %f 67 0.36270057492785984\n",
      "Epoch 67: val loss 0.612544\n",
      "\n",
      "Epoch %d: train loss %f 68 0.3712040003803041\n",
      "Epoch 68: val loss 0.634478\n",
      "\n",
      "Epoch %d: train loss %f 69 0.3824092066950268\n",
      "Epoch 69: val loss 0.664293\n",
      "\n",
      "Epoch %d: train loss %f 70 0.37104208601845634\n",
      "Epoch 70: val loss 0.664836\n",
      "\n",
      "Epoch %d: train loss %f 71 0.3800678865777122\n",
      "Epoch 71: val loss 0.643888\n",
      "\n",
      "Epoch %d: train loss %f 72 0.381696214278539\n",
      "Epoch 72: val loss 0.660752\n",
      "\n",
      "Epoch %d: train loss %f 73 0.374743468231625\n",
      "Epoch 73: val loss 0.646579\n",
      "\n",
      "Epoch %d: train loss %f 74 0.3451680971516503\n",
      "Epoch 74: val loss 0.662826\n",
      "\n",
      "Epoch %d: train loss %f 75 0.37846678495407104\n",
      "Epoch 75: val loss 0.671963\n",
      "\n",
      "Epoch %d: train loss %f 76 0.3737721112039354\n",
      "Epoch 76: val loss 0.659285\n",
      "\n",
      "Epoch %d: train loss %f 77 0.35209721326828003\n",
      "Epoch 77: val loss 0.653162\n",
      "\n",
      "Epoch %d: train loss %f 78 0.4206620951493581\n",
      "Epoch 78: val loss 0.647140\n",
      "\n",
      "Epoch %d: train loss %f 79 0.36895867851045394\n",
      "Epoch 79: val loss 0.652174\n",
      "\n",
      "Epoch %d: train loss %f 80 0.3883167902628581\n",
      "Epoch 80: val loss 0.634594\n",
      "\n",
      "Epoch %d: train loss %f 81 0.35578935345013935\n",
      "Epoch 81: val loss 0.640817\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3567903935909271\n",
      "Epoch 82: val loss 0.651668\n",
      "\n",
      "Epoch %d: train loss %f 83 0.37131430705388385\n",
      "Epoch 83: val loss 0.649386\n",
      "\n",
      "Epoch %d: train loss %f 84 0.36175305313534206\n",
      "Epoch 84: val loss 0.653141\n",
      "\n",
      "Epoch %d: train loss %f 85 0.36399773094389176\n",
      "Epoch 85: val loss 0.671499\n",
      "\n",
      "Epoch %d: train loss %f 86 0.35498414436976117\n",
      "Epoch 86: val loss 0.672689\n",
      "\n",
      "Epoch %d: train loss %f 87 0.35106636914942\n",
      "Epoch 87: val loss 0.675131\n",
      "\n",
      "Epoch %d: train loss %f 88 0.3488365842236413\n",
      "Epoch 88: val loss 0.676465\n",
      "\n",
      "Epoch %d: train loss %f 89 0.3452881822983424\n",
      "Epoch 89: val loss 0.681724\n",
      "\n",
      "Epoch %d: train loss %f 90 0.350614532828331\n",
      "Epoch 90: val loss 0.695105\n",
      "\n",
      "Epoch %d: train loss %f 91 0.35202638142638737\n",
      "Epoch 91: val loss 0.715793\n",
      "\n",
      "Epoch %d: train loss %f 92 0.3443833556440141\n",
      "Epoch 92: val loss 0.676644\n",
      "\n",
      "Epoch %d: train loss %f 93 0.35760392745335895\n",
      "Epoch 93: val loss 0.669612\n",
      "\n",
      "Epoch %d: train loss %f 94 0.3526177472538418\n",
      "Epoch 94: val loss 0.670174\n",
      "\n",
      "Epoch %d: train loss %f 95 0.37354209687974715\n",
      "Epoch 95: val loss 0.660644\n",
      "\n",
      "Epoch %d: train loss %f 96 0.3643074714475208\n",
      "Epoch 96: val loss 0.686291\n",
      "\n",
      "Epoch %d: train loss %f 97 0.36706510848469204\n",
      "Epoch 97: val loss 0.691560\n",
      "\n",
      "Epoch %d: train loss %f 98 0.35414303342501324\n",
      "Epoch 98: val loss 0.667108\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3406713323460685\n",
      "Epoch 99: val loss 0.665131\n",
      "\n",
      "Epoch %d: train loss %f 100 0.31241655846436817\n",
      "Epoch 100: val loss 0.658941\n",
      "\n",
      "Epoch %d: train loss %f 101 0.3388247854179806\n",
      "Epoch 101: val loss 0.669268\n",
      "\n",
      "Epoch %d: train loss %f 102 0.31769862439897323\n",
      "Epoch 102: val loss 0.677707\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3128182590007782\n",
      "Epoch 103: val loss 0.695548\n",
      "\n",
      "Epoch %d: train loss %f 104 0.3565291729238298\n",
      "Epoch 104: val loss 0.704239\n",
      "\n",
      "Epoch %d: train loss %f 105 0.34124624563588035\n",
      "Epoch 105: val loss 0.689741\n",
      "\n",
      "Epoch %d: train loss %f 106 0.36404341128137374\n",
      "Epoch 106: val loss 0.679330\n",
      "\n",
      "Epoch %d: train loss %f 107 0.3341353237628937\n",
      "Epoch 107: val loss 0.678083\n",
      "\n",
      "Epoch %d: train loss %f 108 0.3242568506134881\n",
      "Epoch 108: val loss 0.673585\n",
      "\n",
      "Epoch %d: train loss %f 109 0.3491078664859136\n",
      "Epoch 109: val loss 0.679440\n",
      "\n",
      "Epoch %d: train loss %f 110 0.3413282699055142\n",
      "Epoch 110: val loss 0.682773\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3052819636132982\n",
      "Epoch 111: val loss 0.694239\n",
      "\n",
      "Epoch %d: train loss %f 112 0.32779373559686875\n",
      "Epoch 112: val loss 0.705086\n",
      "\n",
      "Epoch %d: train loss %f 113 0.3387708415587743\n",
      "Epoch 113: val loss 0.711197\n",
      "\n",
      "Epoch %d: train loss %f 114 0.3029566788011127\n",
      "Epoch 114: val loss 0.726341\n",
      "\n",
      "Epoch %d: train loss %f 115 0.31223273939556545\n",
      "Epoch 115: val loss 0.713927\n",
      "\n",
      "Epoch %d: train loss %f 116 0.344706224070655\n",
      "Epoch 116: val loss 0.711061\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3284606370660994\n",
      "Epoch 117: val loss 0.700028\n",
      "\n",
      "Epoch %d: train loss %f 118 0.32532156507174176\n",
      "Epoch 118: val loss 0.702168\n",
      "\n",
      "Epoch %d: train loss %f 119 0.3106688078906801\n",
      "Epoch 119: val loss 0.736831\n",
      "\n",
      "Epoch %d: train loss %f 120 0.2987300902605057\n",
      "Epoch 120: val loss 0.722508\n",
      "\n",
      "Epoch %d: train loss %f 121 0.33990005320972866\n",
      "Epoch 121: val loss 0.723546\n",
      "\n",
      "Epoch %d: train loss %f 122 0.3770922190613217\n",
      "Epoch 122: val loss 0.715682\n",
      "\n",
      "Epoch %d: train loss %f 123 0.31637629204326206\n",
      "Epoch 123: val loss 0.721113\n",
      "\n",
      "Epoch %d: train loss %f 124 0.3420627961556117\n",
      "Epoch 124: val loss 0.724001\n",
      "\n",
      "Epoch %d: train loss %f 125 0.28813741935624015\n",
      "Epoch 125: val loss 0.715341\n",
      "\n",
      "Epoch %d: train loss %f 126 0.33589281638463336\n",
      "Epoch 126: val loss 0.732670\n",
      "\n",
      "Epoch %d: train loss %f 127 0.3097391012642119\n",
      "Epoch 127: val loss 0.761109\n",
      "\n",
      "Epoch %d: train loss %f 128 0.3176638235648473\n",
      "Epoch 128: val loss 0.751699\n",
      "\n",
      "Epoch %d: train loss %f 129 0.2927705521384875\n",
      "Epoch 129: val loss 0.746471\n",
      "\n",
      "Epoch %d: train loss %f 130 0.33700112832917106\n",
      "Epoch 130: val loss 0.746534\n",
      "\n",
      "Epoch %d: train loss %f 131 0.31209177937772536\n",
      "Epoch 131: val loss 0.758513\n",
      "\n",
      "Epoch %d: train loss %f 132 0.28561701211664414\n",
      "Epoch 132: val loss 0.768595\n",
      "\n",
      "Epoch %d: train loss %f 133 0.31316768957508934\n",
      "Epoch 133: val loss 0.771945\n",
      "\n",
      "Epoch %d: train loss %f 134 0.3096485618087981\n",
      "Epoch 134: val loss 0.735018\n",
      "\n",
      "Epoch %d: train loss %f 135 0.30738123920228744\n",
      "Epoch 135: val loss 0.732496\n",
      "\n",
      "Epoch %d: train loss %f 136 0.31250626345475513\n",
      "Epoch 136: val loss 0.746504\n",
      "\n",
      "Epoch %d: train loss %f 137 0.317837913831075\n",
      "Epoch 137: val loss 0.787361\n",
      "\n",
      "Epoch %d: train loss %f 138 0.3287283115916782\n",
      "Epoch 138: val loss 0.752940\n",
      "\n",
      "Epoch %d: train loss %f 139 0.3306528180837631\n",
      "Epoch 139: val loss 0.764407\n",
      "\n",
      "Epoch %d: train loss %f 140 0.339680724673801\n",
      "Epoch 140: val loss 0.741549\n",
      "\n",
      "Epoch %d: train loss %f 141 0.3168014917108748\n",
      "Epoch 141: val loss 0.746380\n",
      "\n",
      "Epoch %d: train loss %f 142 0.3221305244498783\n",
      "Epoch 142: val loss 0.750654\n",
      "\n",
      "Epoch %d: train loss %f 143 0.33216311203108895\n",
      "Epoch 143: val loss 0.749976\n",
      "\n",
      "Epoch %d: train loss %f 144 0.31846194134818184\n",
      "Epoch 144: val loss 0.741307\n",
      "\n",
      "Epoch %d: train loss %f 145 0.29095536635981667\n",
      "Epoch 145: val loss 0.730215\n",
      "\n",
      "Epoch %d: train loss %f 146 0.3037662108739217\n",
      "Epoch 146: val loss 0.766307\n",
      "\n",
      "Epoch %d: train loss %f 147 0.2537154389752282\n",
      "Epoch 147: val loss 0.781316\n",
      "\n",
      "Epoch %d: train loss %f 148 0.2893276313940684\n",
      "Epoch 148: val loss 0.780097\n",
      "\n",
      "Epoch %d: train loss %f 149 0.2899693200985591\n",
      "Epoch 149: val loss 0.769062\n",
      "\n",
      "Epoch %d: train loss %f 150 0.31556705633799237\n",
      "Epoch 150: val loss 0.752805\n",
      "\n",
      "Epoch %d: train loss %f 151 0.3155769540203942\n",
      "Epoch 151: val loss 0.763543\n",
      "\n",
      "Epoch %d: train loss %f 152 0.3116447478532791\n",
      "Epoch 152: val loss 0.759787\n",
      "\n",
      "Epoch %d: train loss %f 153 0.30179645948939854\n",
      "Epoch 153: val loss 0.756119\n",
      "\n",
      "Epoch %d: train loss %f 154 0.3083183169364929\n",
      "Epoch 154: val loss 0.781365\n",
      "\n",
      "Epoch %d: train loss %f 155 0.29796339240339065\n",
      "Epoch 155: val loss 0.798166\n",
      "\n",
      "Epoch %d: train loss %f 156 0.2735327482223511\n",
      "Epoch 156: val loss 0.778036\n",
      "\n",
      "Epoch %d: train loss %f 157 0.30158663127157426\n",
      "Epoch 157: val loss 0.794206\n",
      "\n",
      "Epoch %d: train loss %f 158 0.2880023916562398\n",
      "Epoch 158: val loss 0.814193\n",
      "\n",
      "Epoch %d: train loss %f 159 0.2657727185222838\n",
      "Epoch 159: val loss 0.836109\n",
      "\n",
      "Epoch %d: train loss %f 160 0.3087991409831577\n",
      "Epoch 160: val loss 0.839316\n",
      "\n",
      "Epoch %d: train loss %f 161 0.3144117444753647\n",
      "Epoch 161: val loss 0.819842\n",
      "\n",
      "Epoch %d: train loss %f 162 0.2507313672039244\n",
      "Epoch 162: val loss 0.805879\n",
      "\n",
      "Epoch %d: train loss %f 163 0.2509497039847904\n",
      "Epoch 163: val loss 0.826580\n",
      "\n",
      "Epoch %d: train loss %f 164 0.3033091309997771\n",
      "Epoch 164: val loss 0.825215\n",
      "\n",
      "Epoch %d: train loss %f 165 0.23266429868009356\n",
      "Epoch 165: val loss 0.840856\n",
      "\n",
      "Epoch %d: train loss %f 166 0.26202833818064797\n",
      "Epoch 166: val loss 0.843985\n",
      "\n",
      "Epoch %d: train loss %f 167 0.30146682096852195\n",
      "Epoch 167: val loss 0.840827\n",
      "\n",
      "Epoch %d: train loss %f 168 0.26095107528898454\n",
      "Epoch 168: val loss 0.865660\n",
      "\n",
      "Epoch %d: train loss %f 169 0.2493471105893453\n",
      "Epoch 169: val loss 0.828836\n",
      "\n",
      "Epoch %d: train loss %f 170 0.27325676050451064\n",
      "Epoch 170: val loss 0.845731\n",
      "\n",
      "Epoch %d: train loss %f 171 0.28219709793726605\n",
      "Epoch 171: val loss 0.817417\n",
      "\n",
      "Epoch %d: train loss %f 172 0.26437440017859143\n",
      "Epoch 172: val loss 0.813816\n",
      "\n",
      "Epoch %d: train loss %f 173 0.26885892781946397\n",
      "Epoch 173: val loss 0.838913\n",
      "\n",
      "Epoch %d: train loss %f 174 0.2777092903852463\n",
      "Epoch 174: val loss 0.828011\n",
      "\n",
      "Epoch %d: train loss %f 175 0.2887916780180401\n",
      "Epoch 175: val loss 0.853932\n",
      "\n",
      "Epoch %d: train loss %f 176 0.24530017210377586\n",
      "Epoch 176: val loss 0.865236\n",
      "\n",
      "Epoch %d: train loss %f 177 0.3126247483823035\n",
      "Epoch 177: val loss 0.853547\n",
      "\n",
      "Epoch %d: train loss %f 178 0.28824331363042194\n",
      "Epoch 178: val loss 0.843944\n",
      "\n",
      "Epoch %d: train loss %f 179 0.24811825487348768\n",
      "Epoch 179: val loss 0.815973\n",
      "\n",
      "Epoch %d: train loss %f 180 0.26748620139227974\n",
      "Epoch 180: val loss 0.858340\n",
      "\n",
      "Epoch %d: train loss %f 181 0.26404817154010135\n",
      "Epoch 181: val loss 0.857575\n",
      "\n",
      "Epoch %d: train loss %f 182 0.23613054719236162\n",
      "Epoch 182: val loss 0.840446\n",
      "\n",
      "Epoch %d: train loss %f 183 0.2951178004344304\n",
      "Epoch 183: val loss 0.833527\n",
      "\n",
      "Epoch %d: train loss %f 184 0.26051395303673214\n",
      "Epoch 184: val loss 0.832810\n",
      "\n",
      "Epoch %d: train loss %f 185 0.26176873180601334\n",
      "Epoch 185: val loss 0.820350\n",
      "\n",
      "Epoch %d: train loss %f 186 0.25978660252359176\n",
      "Epoch 186: val loss 0.839653\n",
      "\n",
      "Epoch %d: train loss %f 187 0.23717018051279914\n",
      "Epoch 187: val loss 0.833769\n",
      "\n",
      "Epoch %d: train loss %f 188 0.2929248975382911\n",
      "Epoch 188: val loss 0.842934\n",
      "\n",
      "Epoch %d: train loss %f 189 0.3132081495390998\n",
      "Epoch 189: val loss 0.863461\n",
      "\n",
      "Epoch %d: train loss %f 190 0.251735399166743\n",
      "Epoch 190: val loss 0.845322\n",
      "\n",
      "Epoch %d: train loss %f 191 0.25195811688899994\n",
      "Epoch 191: val loss 0.849187\n",
      "\n",
      "Epoch %d: train loss %f 192 0.2394969943496916\n",
      "Epoch 192: val loss 0.859518\n",
      "\n",
      "Epoch %d: train loss %f 193 0.2651700907283359\n",
      "Epoch 193: val loss 0.864163\n",
      "\n",
      "Epoch %d: train loss %f 194 0.2705759074952867\n",
      "Epoch 194: val loss 0.860154\n",
      "\n",
      "Epoch %d: train loss %f 195 0.24211615241236156\n",
      "Epoch 195: val loss 0.890514\n",
      "\n",
      "Epoch %d: train loss %f 196 0.3096253640121884\n",
      "Epoch 196: val loss 0.851375\n",
      "\n",
      "Epoch %d: train loss %f 197 0.279562019639545\n",
      "Epoch 197: val loss 0.882637\n",
      "\n",
      "Epoch %d: train loss %f 198 0.27209439128637314\n",
      "Epoch 198: val loss 0.864441\n",
      "\n",
      "Epoch %d: train loss %f 199 0.2710760169559055\n",
      "Epoch 199: val loss 0.831006\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6939867006407844\n",
      "Epoch 0: val loss 0.694776\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6894519925117493\n",
      "Epoch 1: val loss 0.694411\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6818929182158576\n",
      "Epoch 2: val loss 0.693994\n",
      "\n",
      "Epoch %d: train loss %f 3 0.670260939333174\n",
      "Epoch 3: val loss 0.693670\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6557824280526903\n",
      "Epoch 4: val loss 0.693381\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6345208419693841\n",
      "Epoch 5: val loss 0.699112\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6086946328481039\n",
      "Epoch 6: val loss 0.712259\n",
      "\n",
      "Epoch %d: train loss %f 7 0.595290117793613\n",
      "Epoch 7: val loss 0.728207\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5743543770578172\n",
      "Epoch 8: val loss 0.735439\n",
      "\n",
      "Epoch %d: train loss %f 9 0.565253347158432\n",
      "Epoch 9: val loss 0.749646\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5418201353814867\n",
      "Epoch 10: val loss 0.741362\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5207383963796828\n",
      "Epoch 11: val loss 0.776348\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5111013584666781\n",
      "Epoch 12: val loss 0.800576\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5115639666716257\n",
      "Epoch 13: val loss 0.787564\n",
      "\n",
      "Epoch %d: train loss %f 14 0.525954630639818\n",
      "Epoch 14: val loss 0.771398\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5230157607131534\n",
      "Epoch 15: val loss 0.773888\n",
      "\n",
      "Epoch %d: train loss %f 16 0.4857755932543013\n",
      "Epoch 16: val loss 0.765120\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5022304356098175\n",
      "Epoch 17: val loss 0.771049\n",
      "\n",
      "Epoch %d: train loss %f 18 0.4898902144696977\n",
      "Epoch 18: val loss 0.787816\n",
      "\n",
      "Epoch %d: train loss %f 19 0.4892331262429555\n",
      "Epoch 19: val loss 0.782796\n",
      "\n",
      "Epoch %d: train loss %f 20 0.4620155658986833\n",
      "Epoch 20: val loss 0.806465\n",
      "\n",
      "Epoch %d: train loss %f 21 0.4661109248797099\n",
      "Epoch 21: val loss 0.822456\n",
      "\n",
      "Epoch %d: train loss %f 22 0.45761240190929836\n",
      "Epoch 22: val loss 0.803575\n",
      "\n",
      "Epoch %d: train loss %f 23 0.48489401406712\n",
      "Epoch 23: val loss 0.786634\n",
      "\n",
      "Epoch %d: train loss %f 24 0.46373962693744236\n",
      "Epoch 24: val loss 0.771685\n",
      "\n",
      "Epoch %d: train loss %f 25 0.466232309738795\n",
      "Epoch 25: val loss 0.768413\n",
      "\n",
      "Epoch %d: train loss %f 26 0.44789037108421326\n",
      "Epoch 26: val loss 0.800737\n",
      "\n",
      "Epoch %d: train loss %f 27 0.4353982044590844\n",
      "Epoch 27: val loss 0.827874\n",
      "\n",
      "Epoch %d: train loss %f 28 0.45587970150841606\n",
      "Epoch 28: val loss 0.818979\n",
      "\n",
      "Epoch %d: train loss %f 29 0.44130707118246293\n",
      "Epoch 29: val loss 0.808826\n",
      "\n",
      "Epoch %d: train loss %f 30 0.42863338854577804\n",
      "Epoch 30: val loss 0.817944\n",
      "\n",
      "Epoch %d: train loss %f 31 0.43519306513998246\n",
      "Epoch 31: val loss 0.820911\n",
      "\n",
      "Epoch %d: train loss %f 32 0.431462738249037\n",
      "Epoch 32: val loss 0.828029\n",
      "\n",
      "Epoch %d: train loss %f 33 0.41490286588668823\n",
      "Epoch 33: val loss 0.804164\n",
      "\n",
      "Epoch %d: train loss %f 34 0.44787975152333576\n",
      "Epoch 34: val loss 0.803761\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4373042980829875\n",
      "Epoch 35: val loss 0.823873\n",
      "\n",
      "Epoch %d: train loss %f 36 0.3978988130887349\n",
      "Epoch 36: val loss 0.832653\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4123318824503157\n",
      "Epoch 37: val loss 0.829294\n",
      "\n",
      "Epoch %d: train loss %f 38 0.4094047049681346\n",
      "Epoch 38: val loss 0.848993\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4141932527224223\n",
      "Epoch 39: val loss 0.823399\n",
      "\n",
      "Epoch %d: train loss %f 40 0.4313536054558224\n",
      "Epoch 40: val loss 0.849595\n",
      "\n",
      "Epoch %d: train loss %f 41 0.42180554072062176\n",
      "Epoch 41: val loss 0.868091\n",
      "\n",
      "Epoch %d: train loss %f 42 0.3954633441236284\n",
      "Epoch 42: val loss 0.840503\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4220094018512302\n",
      "Epoch 43: val loss 0.823169\n",
      "\n",
      "Epoch %d: train loss %f 44 0.3803968131542206\n",
      "Epoch 44: val loss 0.890185\n",
      "\n",
      "Epoch %d: train loss %f 45 0.3667117158571879\n",
      "Epoch 45: val loss 0.844867\n",
      "\n",
      "Epoch %d: train loss %f 46 0.40214233265982735\n",
      "Epoch 46: val loss 0.879683\n",
      "\n",
      "Epoch %d: train loss %f 47 0.4026762909359402\n",
      "Epoch 47: val loss 0.905556\n",
      "\n",
      "Epoch %d: train loss %f 48 0.37482619947857326\n",
      "Epoch 48: val loss 0.891594\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4017025828361511\n",
      "Epoch 49: val loss 0.905360\n",
      "\n",
      "Epoch %d: train loss %f 50 0.3716841439406077\n",
      "Epoch 50: val loss 0.894460\n",
      "\n",
      "Epoch %d: train loss %f 51 0.3857116964128282\n",
      "Epoch 51: val loss 0.920325\n",
      "\n",
      "Epoch %d: train loss %f 52 0.36676398581928676\n",
      "Epoch 52: val loss 0.874724\n",
      "\n",
      "Epoch %d: train loss %f 53 0.3824280864662594\n",
      "Epoch 53: val loss 0.898289\n",
      "\n",
      "Epoch %d: train loss %f 54 0.3615059223439958\n",
      "Epoch 54: val loss 0.929118\n",
      "\n",
      "Epoch %d: train loss %f 55 0.36281569136513603\n",
      "Epoch 55: val loss 0.945254\n",
      "\n",
      "Epoch %d: train loss %f 56 0.36557185649871826\n",
      "Epoch 56: val loss 0.891991\n",
      "\n",
      "Epoch %d: train loss %f 57 0.35470540159278446\n",
      "Epoch 57: val loss 0.932982\n",
      "\n",
      "Epoch %d: train loss %f 58 0.35859085619449615\n",
      "Epoch 58: val loss 0.933952\n",
      "\n",
      "Epoch %d: train loss %f 59 0.3516417096058528\n",
      "Epoch 59: val loss 0.943029\n",
      "\n",
      "Epoch %d: train loss %f 60 0.4147116409407722\n",
      "Epoch 60: val loss 0.971037\n",
      "\n",
      "Epoch %d: train loss %f 61 0.3667195869816674\n",
      "Epoch 61: val loss 0.931782\n",
      "\n",
      "Epoch %d: train loss %f 62 0.36100443535380894\n",
      "Epoch 62: val loss 0.930743\n",
      "\n",
      "Epoch %d: train loss %f 63 0.3760001195801629\n",
      "Epoch 63: val loss 0.958685\n",
      "\n",
      "Epoch %d: train loss %f 64 0.363502550456259\n",
      "Epoch 64: val loss 0.927347\n",
      "\n",
      "Epoch %d: train loss %f 65 0.31888290411896175\n",
      "Epoch 65: val loss 0.950208\n",
      "\n",
      "Epoch %d: train loss %f 66 0.37021876374880475\n",
      "Epoch 66: val loss 0.984117\n",
      "\n",
      "Epoch %d: train loss %f 67 0.3455973035759396\n",
      "Epoch 67: val loss 0.972250\n",
      "\n",
      "Epoch %d: train loss %f 68 0.3418455604049895\n",
      "Epoch 68: val loss 0.981966\n",
      "\n",
      "Epoch %d: train loss %f 69 0.3438577420181698\n",
      "Epoch 69: val loss 0.959934\n",
      "\n",
      "Epoch %d: train loss %f 70 0.2930788877937529\n",
      "Epoch 70: val loss 0.983342\n",
      "\n",
      "Epoch %d: train loss %f 71 0.3365270710653729\n",
      "Epoch 71: val loss 0.993372\n",
      "\n",
      "Epoch %d: train loss %f 72 0.30784377455711365\n",
      "Epoch 72: val loss 1.032966\n",
      "\n",
      "Epoch %d: train loss %f 73 0.31296515464782715\n",
      "Epoch 73: val loss 1.000832\n",
      "\n",
      "Epoch %d: train loss %f 74 0.3431902908616596\n",
      "Epoch 74: val loss 1.062289\n",
      "\n",
      "Epoch %d: train loss %f 75 0.3332192583216561\n",
      "Epoch 75: val loss 1.042119\n",
      "\n",
      "Epoch %d: train loss %f 76 0.3549090690082974\n",
      "Epoch 76: val loss 1.024166\n",
      "\n",
      "Epoch %d: train loss %f 77 0.33301732771926457\n",
      "Epoch 77: val loss 1.017460\n",
      "\n",
      "Epoch %d: train loss %f 78 0.32795270946290755\n",
      "Epoch 78: val loss 1.070345\n",
      "\n",
      "Epoch %d: train loss %f 79 0.35428083936373395\n",
      "Epoch 79: val loss 1.070099\n",
      "\n",
      "Epoch %d: train loss %f 80 0.3504247847530577\n",
      "Epoch 80: val loss 1.086294\n",
      "\n",
      "Epoch %d: train loss %f 81 0.33959225316842395\n",
      "Epoch 81: val loss 1.088916\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3236161321401596\n",
      "Epoch 82: val loss 1.082701\n",
      "\n",
      "Epoch %d: train loss %f 83 0.32156260146035087\n",
      "Epoch 83: val loss 1.066478\n",
      "\n",
      "Epoch %d: train loss %f 84 0.32480209734704757\n",
      "Epoch 84: val loss 1.084511\n",
      "\n",
      "Epoch %d: train loss %f 85 0.3122651908132765\n",
      "Epoch 85: val loss 1.054302\n",
      "\n",
      "Epoch %d: train loss %f 86 0.3388773517476188\n",
      "Epoch 86: val loss 1.031546\n",
      "\n",
      "Epoch %d: train loss %f 87 0.3170141660504871\n",
      "Epoch 87: val loss 1.024155\n",
      "\n",
      "Epoch %d: train loss %f 88 0.3321179681354099\n",
      "Epoch 88: val loss 1.074186\n",
      "\n",
      "Epoch %d: train loss %f 89 0.32931552827358246\n",
      "Epoch 89: val loss 1.085411\n",
      "\n",
      "Epoch %d: train loss %f 90 0.32382996214760673\n",
      "Epoch 90: val loss 1.080544\n",
      "\n",
      "Epoch %d: train loss %f 91 0.32400137517187333\n",
      "Epoch 91: val loss 1.055798\n",
      "\n",
      "Epoch %d: train loss %f 92 0.2964046531253391\n",
      "Epoch 92: val loss 1.083875\n",
      "\n",
      "Epoch %d: train loss %f 93 0.34349499973985886\n",
      "Epoch 93: val loss 1.047586\n",
      "\n",
      "Epoch %d: train loss %f 94 0.3124578495820363\n",
      "Epoch 94: val loss 1.064065\n",
      "\n",
      "Epoch %d: train loss %f 95 0.2751256790426042\n",
      "Epoch 95: val loss 1.060686\n",
      "\n",
      "Epoch %d: train loss %f 96 0.2998875495460298\n",
      "Epoch 96: val loss 1.072653\n",
      "\n",
      "Epoch %d: train loss %f 97 0.3132486442724864\n",
      "Epoch 97: val loss 1.098363\n",
      "\n",
      "Epoch %d: train loss %f 98 0.3257162852419747\n",
      "Epoch 98: val loss 1.088370\n",
      "\n",
      "Epoch %d: train loss %f 99 0.29024610420068103\n",
      "Epoch 99: val loss 1.093824\n",
      "\n",
      "Epoch %d: train loss %f 100 0.31644676625728607\n",
      "Epoch 100: val loss 1.121081\n",
      "\n",
      "Epoch %d: train loss %f 101 0.25004129608472186\n",
      "Epoch 101: val loss 1.137997\n",
      "\n",
      "Epoch %d: train loss %f 102 0.28512024217181736\n",
      "Epoch 102: val loss 1.147529\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3331174784236484\n",
      "Epoch 103: val loss 1.106168\n",
      "\n",
      "Epoch %d: train loss %f 104 0.3117216279109319\n",
      "Epoch 104: val loss 1.132812\n",
      "\n",
      "Epoch %d: train loss %f 105 0.3013369407918718\n",
      "Epoch 105: val loss 1.163169\n",
      "\n",
      "Epoch %d: train loss %f 106 0.30746418403254616\n",
      "Epoch 106: val loss 1.170303\n",
      "\n",
      "Epoch %d: train loss %f 107 0.2655523916085561\n",
      "Epoch 107: val loss 1.169268\n",
      "\n",
      "Epoch %d: train loss %f 108 0.2849142750104268\n",
      "Epoch 108: val loss 1.142365\n",
      "\n",
      "Epoch %d: train loss %f 109 0.2762747009595235\n",
      "Epoch 109: val loss 1.193522\n",
      "\n",
      "Epoch %d: train loss %f 110 0.29568540553251904\n",
      "Epoch 110: val loss 1.144704\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3221413377258513\n",
      "Epoch 111: val loss 1.199364\n",
      "\n",
      "Epoch %d: train loss %f 112 0.2930617357293765\n",
      "Epoch 112: val loss 1.187332\n",
      "\n",
      "Epoch %d: train loss %f 113 0.2385498500532574\n",
      "Epoch 113: val loss 1.148416\n",
      "\n",
      "Epoch %d: train loss %f 114 0.27899114622010124\n",
      "Epoch 114: val loss 1.198975\n",
      "\n",
      "Epoch %d: train loss %f 115 0.33055879672368366\n",
      "Epoch 115: val loss 1.211220\n",
      "\n",
      "Epoch %d: train loss %f 116 0.30172957645522225\n",
      "Epoch 116: val loss 1.217241\n",
      "\n",
      "Epoch %d: train loss %f 117 0.30760767890347374\n",
      "Epoch 117: val loss 1.175676\n",
      "\n",
      "Epoch %d: train loss %f 118 0.2727511194017198\n",
      "Epoch 118: val loss 1.129483\n",
      "\n",
      "Epoch %d: train loss %f 119 0.30915201207002\n",
      "Epoch 119: val loss 1.241472\n",
      "\n",
      "Epoch %d: train loss %f 120 0.2630332079198625\n",
      "Epoch 120: val loss 1.161518\n",
      "\n",
      "Epoch %d: train loss %f 121 0.26848338213231826\n",
      "Epoch 121: val loss 1.230521\n",
      "\n",
      "Epoch %d: train loss %f 122 0.2733614825540119\n",
      "Epoch 122: val loss 1.272253\n",
      "\n",
      "Epoch %d: train loss %f 123 0.2784534924560123\n",
      "Epoch 123: val loss 1.258118\n",
      "\n",
      "Epoch %d: train loss %f 124 0.26863158411449856\n",
      "Epoch 124: val loss 1.210985\n",
      "\n",
      "Epoch %d: train loss %f 125 0.29723626044061446\n",
      "Epoch 125: val loss 1.292508\n",
      "\n",
      "Epoch %d: train loss %f 126 0.26053180793921155\n",
      "Epoch 126: val loss 1.238433\n",
      "\n",
      "Epoch %d: train loss %f 127 0.24519579278098214\n",
      "Epoch 127: val loss 1.252993\n",
      "\n",
      "Epoch %d: train loss %f 128 0.29828593134880066\n",
      "Epoch 128: val loss 1.246690\n",
      "\n",
      "Epoch %d: train loss %f 129 0.2631491853131188\n",
      "Epoch 129: val loss 1.186266\n",
      "\n",
      "Epoch %d: train loss %f 130 0.28343923886617023\n",
      "Epoch 130: val loss 1.222077\n",
      "\n",
      "Epoch %d: train loss %f 131 0.26312483847141266\n",
      "Epoch 131: val loss 1.343776\n",
      "\n",
      "Epoch %d: train loss %f 132 0.24992954068713719\n",
      "Epoch 132: val loss 1.284284\n",
      "\n",
      "Epoch %d: train loss %f 133 0.25590670108795166\n",
      "Epoch 133: val loss 1.262975\n",
      "\n",
      "Epoch %d: train loss %f 134 0.2623741692966885\n",
      "Epoch 134: val loss 1.205412\n",
      "\n",
      "Epoch %d: train loss %f 135 0.26999226378069985\n",
      "Epoch 135: val loss 1.222707\n",
      "\n",
      "Epoch %d: train loss %f 136 0.2505234413676792\n",
      "Epoch 136: val loss 1.276754\n",
      "\n",
      "Epoch %d: train loss %f 137 0.29366063906086814\n",
      "Epoch 137: val loss 1.290768\n",
      "\n",
      "Epoch %d: train loss %f 138 0.26193351712491775\n",
      "Epoch 138: val loss 1.267851\n",
      "\n",
      "Epoch %d: train loss %f 139 0.2913003232744005\n",
      "Epoch 139: val loss 1.310102\n",
      "\n",
      "Epoch %d: train loss %f 140 0.27536337574323017\n",
      "Epoch 140: val loss 1.280316\n",
      "\n",
      "Epoch %d: train loss %f 141 0.2487084385421541\n",
      "Epoch 141: val loss 1.320701\n",
      "\n",
      "Epoch %d: train loss %f 142 0.2616237931781345\n",
      "Epoch 142: val loss 1.336623\n",
      "\n",
      "Epoch %d: train loss %f 143 0.2824592474434111\n",
      "Epoch 143: val loss 1.356092\n",
      "\n",
      "Epoch %d: train loss %f 144 0.24447292420599195\n",
      "Epoch 144: val loss 1.331104\n",
      "\n",
      "Epoch %d: train loss %f 145 0.2745143539375729\n",
      "Epoch 145: val loss 1.310390\n",
      "\n",
      "Epoch %d: train loss %f 146 0.269155556956927\n",
      "Epoch 146: val loss 1.364144\n",
      "\n",
      "Epoch %d: train loss %f 147 0.2843996352619595\n",
      "Epoch 147: val loss 1.296935\n",
      "\n",
      "Epoch %d: train loss %f 148 0.3010009543763267\n",
      "Epoch 148: val loss 1.302343\n",
      "\n",
      "Epoch %d: train loss %f 149 0.2524990629818704\n",
      "Epoch 149: val loss 1.264323\n",
      "\n",
      "Epoch %d: train loss %f 150 0.2399347441063987\n",
      "Epoch 150: val loss 1.279892\n",
      "\n",
      "Epoch %d: train loss %f 151 0.23198004563649496\n",
      "Epoch 151: val loss 1.278796\n",
      "\n",
      "Epoch %d: train loss %f 152 0.27325093415048385\n",
      "Epoch 152: val loss 1.319600\n",
      "\n",
      "Epoch %d: train loss %f 153 0.2528730630874634\n",
      "Epoch 153: val loss 1.304670\n",
      "\n",
      "Epoch %d: train loss %f 154 0.3007208738062117\n",
      "Epoch 154: val loss 1.307075\n",
      "\n",
      "Epoch %d: train loss %f 155 0.2607116169399685\n",
      "Epoch 155: val loss 1.316531\n",
      "\n",
      "Epoch %d: train loss %f 156 0.25399889714188045\n",
      "Epoch 156: val loss 1.261252\n",
      "\n",
      "Epoch %d: train loss %f 157 0.23326069944434696\n",
      "Epoch 157: val loss 1.336344\n",
      "\n",
      "Epoch %d: train loss %f 158 0.21601878934436375\n",
      "Epoch 158: val loss 1.295859\n",
      "\n",
      "Epoch %d: train loss %f 159 0.2084116968843672\n",
      "Epoch 159: val loss 1.299591\n",
      "\n",
      "Epoch %d: train loss %f 160 0.23570468607876036\n",
      "Epoch 160: val loss 1.323362\n",
      "\n",
      "Epoch %d: train loss %f 161 0.1976044467753834\n",
      "Epoch 161: val loss 1.343016\n",
      "\n",
      "Epoch %d: train loss %f 162 0.22929204338126713\n",
      "Epoch 162: val loss 1.461743\n",
      "\n",
      "Epoch %d: train loss %f 163 0.28104284405708313\n",
      "Epoch 163: val loss 1.375940\n",
      "\n",
      "Epoch %d: train loss %f 164 0.2452753633260727\n",
      "Epoch 164: val loss 1.360984\n",
      "\n",
      "Epoch %d: train loss %f 165 0.23750121891498566\n",
      "Epoch 165: val loss 1.286257\n",
      "\n",
      "Epoch %d: train loss %f 166 0.25789935141801834\n",
      "Epoch 166: val loss 1.369734\n",
      "\n",
      "Epoch %d: train loss %f 167 0.24216055207782322\n",
      "Epoch 167: val loss 1.389855\n",
      "\n",
      "Epoch %d: train loss %f 168 0.20754010809792411\n",
      "Epoch 168: val loss 1.479220\n",
      "\n",
      "Epoch %d: train loss %f 169 0.25481804377502865\n",
      "Epoch 169: val loss 1.454723\n",
      "\n",
      "Epoch %d: train loss %f 170 0.23938529358969796\n",
      "Epoch 170: val loss 1.362748\n",
      "\n",
      "Epoch %d: train loss %f 171 0.22559851904710135\n",
      "Epoch 171: val loss 1.394402\n",
      "\n",
      "Epoch %d: train loss %f 172 0.2186603984898991\n",
      "Epoch 172: val loss 1.337006\n",
      "\n",
      "Epoch %d: train loss %f 173 0.25856183138158584\n",
      "Epoch 173: val loss 1.421501\n",
      "\n",
      "Epoch %d: train loss %f 174 0.22666469547483656\n",
      "Epoch 174: val loss 1.428515\n",
      "\n",
      "Epoch %d: train loss %f 175 0.2265722296304173\n",
      "Epoch 175: val loss 1.446904\n",
      "\n",
      "Epoch %d: train loss %f 176 0.24921823706891802\n",
      "Epoch 176: val loss 1.393064\n",
      "\n",
      "Epoch %d: train loss %f 177 0.23576430314117008\n",
      "Epoch 177: val loss 1.423897\n",
      "\n",
      "Epoch %d: train loss %f 178 0.22586499154567719\n",
      "Epoch 178: val loss 1.404998\n",
      "\n",
      "Epoch %d: train loss %f 179 0.21983544694052803\n",
      "Epoch 179: val loss 1.485607\n",
      "\n",
      "Epoch %d: train loss %f 180 0.21187892225053576\n",
      "Epoch 180: val loss 1.435339\n",
      "\n",
      "Epoch %d: train loss %f 181 0.22936633808745277\n",
      "Epoch 181: val loss 1.445533\n",
      "\n",
      "Epoch %d: train loss %f 182 0.2124081833495034\n",
      "Epoch 182: val loss 1.371413\n",
      "\n",
      "Epoch %d: train loss %f 183 0.2291522439983156\n",
      "Epoch 183: val loss 1.459348\n",
      "\n",
      "Epoch %d: train loss %f 184 0.2659251673354043\n",
      "Epoch 184: val loss 1.460272\n",
      "\n",
      "Epoch %d: train loss %f 185 0.21801244053575727\n",
      "Epoch 185: val loss 1.465154\n",
      "\n",
      "Epoch %d: train loss %f 186 0.23296993639734057\n",
      "Epoch 186: val loss 1.435952\n",
      "\n",
      "Epoch %d: train loss %f 187 0.23269716484679115\n",
      "Epoch 187: val loss 1.397379\n",
      "\n",
      "Epoch %d: train loss %f 188 0.20795783607496154\n",
      "Epoch 188: val loss 1.334505\n",
      "\n",
      "Epoch %d: train loss %f 189 0.24242001440789965\n",
      "Epoch 189: val loss 1.436916\n",
      "\n",
      "Epoch %d: train loss %f 190 0.19957341916031307\n",
      "Epoch 190: val loss 1.432498\n",
      "\n",
      "Epoch %d: train loss %f 191 0.23016134897867838\n",
      "Epoch 191: val loss 1.533133\n",
      "\n",
      "Epoch %d: train loss %f 192 0.2287599883145756\n",
      "Epoch 192: val loss 1.505378\n",
      "\n",
      "Epoch %d: train loss %f 193 0.1859011236164305\n",
      "Epoch 193: val loss 1.461127\n",
      "\n",
      "Epoch %d: train loss %f 194 0.19355076303084692\n",
      "Epoch 194: val loss 1.480392\n",
      "\n",
      "Epoch %d: train loss %f 195 0.24846303462982178\n",
      "Epoch 195: val loss 1.447714\n",
      "\n",
      "Epoch %d: train loss %f 196 0.18414012839396796\n",
      "Epoch 196: val loss 1.473653\n",
      "\n",
      "Epoch %d: train loss %f 197 0.2072753823465771\n",
      "Epoch 197: val loss 1.448156\n",
      "\n",
      "Epoch %d: train loss %f 198 0.22311158809396955\n",
      "Epoch 198: val loss 1.416580\n",
      "\n",
      "Epoch %d: train loss %f 199 0.21613764431741503\n",
      "Epoch 199: val loss 1.453397\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6903359889984131\n",
      "Epoch 0: val loss 0.690237\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6852671355009079\n",
      "Epoch 1: val loss 0.688203\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6796732793251673\n",
      "Epoch 2: val loss 0.682468\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6668112576007843\n",
      "Epoch 3: val loss 0.671353\n",
      "\n",
      "Epoch %d: train loss %f 4 0.650208498040835\n",
      "Epoch 4: val loss 0.658667\n",
      "\n",
      "Epoch %d: train loss %f 5 0.63093101978302\n",
      "Epoch 5: val loss 0.640448\n",
      "\n",
      "Epoch %d: train loss %f 6 0.614468182126681\n",
      "Epoch 6: val loss 0.621758\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5902130355437597\n",
      "Epoch 7: val loss 0.606979\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5758172074953715\n",
      "Epoch 8: val loss 0.585593\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5707198257247607\n",
      "Epoch 9: val loss 0.575903\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5742300699154536\n",
      "Epoch 10: val loss 0.571146\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5393026495973269\n",
      "Epoch 11: val loss 0.553370\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5544369543592135\n",
      "Epoch 12: val loss 0.550522\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5400250554084778\n",
      "Epoch 13: val loss 0.552853\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5268121932943662\n",
      "Epoch 14: val loss 0.567780\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5378857776522636\n",
      "Epoch 15: val loss 0.554491\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5342948064208031\n",
      "Epoch 16: val loss 0.544477\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5366152003407478\n",
      "Epoch 17: val loss 0.556206\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5152404978871346\n",
      "Epoch 18: val loss 0.550055\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5180898457765579\n",
      "Epoch 19: val loss 0.548464\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5170143768191338\n",
      "Epoch 20: val loss 0.559805\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5230994150042534\n",
      "Epoch 21: val loss 0.555227\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5104318186640739\n",
      "Epoch 22: val loss 0.548349\n",
      "\n",
      "Epoch %d: train loss %f 23 0.49342886358499527\n",
      "Epoch 23: val loss 0.552257\n",
      "\n",
      "Epoch %d: train loss %f 24 0.492035078505675\n",
      "Epoch 24: val loss 0.554262\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4913799539208412\n",
      "Epoch 25: val loss 0.539971\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4914382795492808\n",
      "Epoch 26: val loss 0.559009\n",
      "\n",
      "Epoch %d: train loss %f 27 0.5090538238485655\n",
      "Epoch 27: val loss 0.536661\n",
      "\n",
      "Epoch %d: train loss %f 28 0.48737367739280063\n",
      "Epoch 28: val loss 0.539040\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4907746637860934\n",
      "Epoch 29: val loss 0.539952\n",
      "\n",
      "Epoch %d: train loss %f 30 0.488984336455663\n",
      "Epoch 30: val loss 0.542598\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4698384652535121\n",
      "Epoch 31: val loss 0.549302\n",
      "\n",
      "Epoch %d: train loss %f 32 0.49552610516548157\n",
      "Epoch 32: val loss 0.536949\n",
      "\n",
      "Epoch %d: train loss %f 33 0.49372508376836777\n",
      "Epoch 33: val loss 0.541085\n",
      "\n",
      "Epoch %d: train loss %f 34 0.4719579443335533\n",
      "Epoch 34: val loss 0.546812\n",
      "\n",
      "Epoch %d: train loss %f 35 0.494746337334315\n",
      "Epoch 35: val loss 0.547245\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4756283387541771\n",
      "Epoch 36: val loss 0.559855\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4856363559762637\n",
      "Epoch 37: val loss 0.538410\n",
      "\n",
      "Epoch %d: train loss %f 38 0.48729978253444034\n",
      "Epoch 38: val loss 0.567322\n",
      "\n",
      "Epoch %d: train loss %f 39 0.46295413623253506\n",
      "Epoch 39: val loss 0.557283\n",
      "\n",
      "Epoch %d: train loss %f 40 0.4510406280557315\n",
      "Epoch 40: val loss 0.565225\n",
      "\n",
      "Epoch %d: train loss %f 41 0.46224012474219006\n",
      "Epoch 41: val loss 0.561976\n",
      "\n",
      "Epoch %d: train loss %f 42 0.47527500490347546\n",
      "Epoch 42: val loss 0.546833\n",
      "\n",
      "Epoch %d: train loss %f 43 0.44383351504802704\n",
      "Epoch 43: val loss 0.560098\n",
      "\n",
      "Epoch %d: train loss %f 44 0.44547344992558163\n",
      "Epoch 44: val loss 0.557767\n",
      "\n",
      "Epoch %d: train loss %f 45 0.45581603546937305\n",
      "Epoch 45: val loss 0.553315\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4296236013372739\n",
      "Epoch 46: val loss 0.549961\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6874869303269819\n",
      "Epoch 0: val loss 0.689501\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6814917759461836\n",
      "Epoch 1: val loss 0.685867\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6751790642738342\n",
      "Epoch 2: val loss 0.677246\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6529213081706654\n",
      "Epoch 3: val loss 0.662332\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6481095118956133\n",
      "Epoch 4: val loss 0.644855\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6333776170557196\n",
      "Epoch 5: val loss 0.633726\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6228990988297896\n",
      "Epoch 6: val loss 0.614395\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6119019118222323\n",
      "Epoch 7: val loss 0.596918\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5975741202181036\n",
      "Epoch 8: val loss 0.590235\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5982569727030668\n",
      "Epoch 9: val loss 0.577705\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5628135421059348\n",
      "Epoch 10: val loss 0.570837\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5839089344848286\n",
      "Epoch 11: val loss 0.561207\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5555641705339606\n",
      "Epoch 12: val loss 0.564838\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5542581243948503\n",
      "Epoch 13: val loss 0.562896\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5740862813862887\n",
      "Epoch 14: val loss 0.556171\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5547019053589214\n",
      "Epoch 15: val loss 0.552787\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5526512373577465\n",
      "Epoch 16: val loss 0.543847\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5441859797997908\n",
      "Epoch 17: val loss 0.554685\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5451694781130011\n",
      "Epoch 18: val loss 0.544394\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5286599722775546\n",
      "Epoch 19: val loss 0.548422\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5485825186425989\n",
      "Epoch 20: val loss 0.541871\n",
      "\n",
      "Epoch %d: train loss %f 21 0.514570257880471\n",
      "Epoch 21: val loss 0.547935\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5036299445412376\n",
      "Epoch 22: val loss 0.535482\n",
      "\n",
      "Epoch %d: train loss %f 23 0.526616784659299\n",
      "Epoch 23: val loss 0.535158\n",
      "\n",
      "Epoch %d: train loss %f 24 0.512434569272128\n",
      "Epoch 24: val loss 0.531131\n",
      "\n",
      "Epoch %d: train loss %f 25 0.5248497209765695\n",
      "Epoch 25: val loss 0.533843\n",
      "\n",
      "Epoch %d: train loss %f 26 0.5218759937719866\n",
      "Epoch 26: val loss 0.530052\n",
      "\n",
      "Epoch %d: train loss %f 27 0.5168698700991544\n",
      "Epoch 27: val loss 0.535012\n",
      "\n",
      "Epoch %d: train loss %f 28 0.5093748000535098\n",
      "Epoch 28: val loss 0.526111\n",
      "\n",
      "Epoch %d: train loss %f 29 0.5507098571820692\n",
      "Epoch 29: val loss 0.527561\n",
      "\n",
      "Epoch %d: train loss %f 30 0.5327071384950117\n",
      "Epoch 30: val loss 0.520864\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4943232102827592\n",
      "Epoch 31: val loss 0.537498\n",
      "\n",
      "Epoch %d: train loss %f 32 0.5031049874695864\n",
      "Epoch 32: val loss 0.523099\n",
      "\n",
      "Epoch %d: train loss %f 33 0.48954923857342114\n",
      "Epoch 33: val loss 0.520382\n",
      "\n",
      "Epoch %d: train loss %f 34 0.492051056840203\n",
      "Epoch 34: val loss 0.527693\n",
      "\n",
      "Epoch %d: train loss %f 35 0.5016599649732764\n",
      "Epoch 35: val loss 0.534479\n",
      "\n",
      "Epoch %d: train loss %f 36 0.49826380068605597\n",
      "Epoch 36: val loss 0.522161\n",
      "\n",
      "Epoch %d: train loss %f 37 0.5127848034555261\n",
      "Epoch 37: val loss 0.542104\n",
      "\n",
      "Epoch %d: train loss %f 38 0.47682337056506763\n",
      "Epoch 38: val loss 0.533796\n",
      "\n",
      "Epoch %d: train loss %f 39 0.48680994998325\n",
      "Epoch 39: val loss 0.524962\n",
      "\n",
      "Epoch %d: train loss %f 40 0.4592366706241261\n",
      "Epoch 40: val loss 0.524311\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4667622284455733\n",
      "Epoch 41: val loss 0.527923\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4804461815140464\n",
      "Epoch 42: val loss 0.529420\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4744393446228721\n",
      "Epoch 43: val loss 0.546010\n",
      "\n",
      "Epoch %d: train loss %f 44 0.47237001765858044\n",
      "Epoch 44: val loss 0.541237\n",
      "\n",
      "Epoch %d: train loss %f 45 0.46282162178646435\n",
      "Epoch 45: val loss 0.534522\n",
      "\n",
      "Epoch %d: train loss %f 46 0.46136983145367017\n",
      "Epoch 46: val loss 0.539636\n",
      "\n",
      "Epoch %d: train loss %f 47 0.5109183652834459\n",
      "Epoch 47: val loss 0.515889\n",
      "\n",
      "Epoch %d: train loss %f 48 0.47864779559048737\n",
      "Epoch 48: val loss 0.513546\n",
      "\n",
      "Epoch %d: train loss %f 49 0.47929399934681977\n",
      "Epoch 49: val loss 0.511282\n",
      "\n",
      "Epoch %d: train loss %f 50 0.4286397153680975\n",
      "Epoch 50: val loss 0.542156\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4421782872893594\n",
      "Epoch 51: val loss 0.526828\n",
      "\n",
      "Epoch %d: train loss %f 52 0.4283501614223827\n",
      "Epoch 52: val loss 0.520580\n",
      "\n",
      "Epoch %d: train loss %f 53 0.4176023548299616\n",
      "Epoch 53: val loss 0.529464\n",
      "\n",
      "Epoch %d: train loss %f 54 0.43866377527063544\n",
      "Epoch 54: val loss 0.530878\n",
      "\n",
      "Epoch %d: train loss %f 55 0.4403147480704568\n",
      "Epoch 55: val loss 0.528742\n",
      "\n",
      "Epoch %d: train loss %f 56 0.46548597650094464\n",
      "Epoch 56: val loss 0.534368\n",
      "\n",
      "Epoch %d: train loss %f 57 0.444112398407676\n",
      "Epoch 57: val loss 0.538154\n",
      "\n",
      "Epoch %d: train loss %f 58 0.44392467899756\n",
      "Epoch 58: val loss 0.517977\n",
      "\n",
      "Epoch %d: train loss %f 59 0.42884398861364886\n",
      "Epoch 59: val loss 0.522814\n",
      "\n",
      "Epoch %d: train loss %f 60 0.42148506099527533\n",
      "Epoch 60: val loss 0.520119\n",
      "\n",
      "Epoch %d: train loss %f 61 0.4285731613636017\n",
      "Epoch 61: val loss 0.551190\n",
      "\n",
      "Epoch %d: train loss %f 62 0.4184356115081094\n",
      "Epoch 62: val loss 0.528583\n",
      "\n",
      "Epoch %d: train loss %f 63 0.37587209994142706\n",
      "Epoch 63: val loss 0.520545\n",
      "\n",
      "Epoch %d: train loss %f 64 0.4210923585024747\n",
      "Epoch 64: val loss 0.527192\n",
      "\n",
      "Epoch %d: train loss %f 65 0.4178747074170546\n",
      "Epoch 65: val loss 0.531809\n",
      "\n",
      "Epoch %d: train loss %f 66 0.41485962542620575\n",
      "Epoch 66: val loss 0.529332\n",
      "\n",
      "Epoch %d: train loss %f 67 0.39592087268829346\n",
      "Epoch 67: val loss 0.519778\n",
      "\n",
      "Epoch %d: train loss %f 68 0.46767588637091895\n",
      "Epoch 68: val loss 0.517082\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4136599166826768\n",
      "Epoch 69: val loss 0.511434\n",
      "\n",
      "Epoch %d: train loss %f 70 0.38146832856264984\n",
      "Epoch 70: val loss 0.524103\n",
      "\n",
      "Epoch %d: train loss %f 71 0.4093373959714716\n",
      "Epoch 71: val loss 0.526848\n",
      "\n",
      "Epoch %d: train loss %f 72 0.4376719295978546\n",
      "Epoch 72: val loss 0.534179\n",
      "\n",
      "Epoch %d: train loss %f 73 0.4011432582681829\n",
      "Epoch 73: val loss 0.540655\n",
      "\n",
      "Epoch %d: train loss %f 74 0.4057838022708893\n",
      "Epoch 74: val loss 0.524746\n",
      "\n",
      "Epoch %d: train loss %f 75 0.4081789417700334\n",
      "Epoch 75: val loss 0.529618\n",
      "\n",
      "Epoch %d: train loss %f 76 0.4213415736501867\n",
      "Epoch 76: val loss 0.506165\n",
      "\n",
      "Epoch %d: train loss %f 77 0.43258406492796814\n",
      "Epoch 77: val loss 0.525766\n",
      "\n",
      "Epoch %d: train loss %f 78 0.3882495706731623\n",
      "Epoch 78: val loss 0.523518\n",
      "\n",
      "Epoch %d: train loss %f 79 0.38292821835387836\n",
      "Epoch 79: val loss 0.506454\n",
      "\n",
      "Epoch %d: train loss %f 80 0.36330190842801874\n",
      "Epoch 80: val loss 0.523176\n",
      "\n",
      "Epoch %d: train loss %f 81 0.3908225812695243\n",
      "Epoch 81: val loss 0.535077\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3714443472298709\n",
      "Epoch 82: val loss 0.534863\n",
      "\n",
      "Epoch %d: train loss %f 83 0.37217523834922095\n",
      "Epoch 83: val loss 0.541308\n",
      "\n",
      "Epoch %d: train loss %f 84 0.3660959113727916\n",
      "Epoch 84: val loss 0.529773\n",
      "\n",
      "Epoch %d: train loss %f 85 0.3331165313720703\n",
      "Epoch 85: val loss 0.539517\n",
      "\n",
      "Epoch %d: train loss %f 86 0.34033760699358856\n",
      "Epoch 86: val loss 0.534901\n",
      "\n",
      "Epoch %d: train loss %f 87 0.3650040897456082\n",
      "Epoch 87: val loss 0.526792\n",
      "\n",
      "Epoch %d: train loss %f 88 0.35541871054606006\n",
      "Epoch 88: val loss 0.535316\n",
      "\n",
      "Epoch %d: train loss %f 89 0.3648878606882962\n",
      "Epoch 89: val loss 0.541551\n",
      "\n",
      "Epoch %d: train loss %f 90 0.3911080631342801\n",
      "Epoch 90: val loss 0.554237\n",
      "\n",
      "Epoch %d: train loss %f 91 0.32592556964267383\n",
      "Epoch 91: val loss 0.551984\n",
      "\n",
      "Epoch %d: train loss %f 92 0.328936675732786\n",
      "Epoch 92: val loss 0.545517\n",
      "\n",
      "Epoch %d: train loss %f 93 0.34187511151487177\n",
      "Epoch 93: val loss 0.555305\n",
      "\n",
      "Epoch %d: train loss %f 94 0.38089441982182587\n",
      "Epoch 94: val loss 0.579500\n",
      "\n",
      "Epoch %d: train loss %f 95 0.38898821581493725\n",
      "Epoch 95: val loss 0.569089\n",
      "\n",
      "Epoch %d: train loss %f 96 0.33809907327998767\n",
      "Epoch 96: val loss 0.571407\n",
      "\n",
      "Epoch %d: train loss %f 97 0.30903140794147144\n",
      "Epoch 97: val loss 0.583483\n",
      "\n",
      "Epoch %d: train loss %f 98 0.3421238376335664\n",
      "Epoch 98: val loss 0.577078\n",
      "\n",
      "Epoch %d: train loss %f 99 0.37297814948992297\n",
      "Epoch 99: val loss 0.563223\n",
      "\n",
      "Epoch %d: train loss %f 100 0.3476640676910227\n",
      "Epoch 100: val loss 0.584726\n",
      "\n",
      "Epoch %d: train loss %f 101 0.3118004527958957\n",
      "Epoch 101: val loss 0.571581\n",
      "\n",
      "Epoch %d: train loss %f 102 0.370100891048258\n",
      "Epoch 102: val loss 0.556476\n",
      "\n",
      "Epoch %d: train loss %f 103 0.38099824298511853\n",
      "Epoch 103: val loss 0.585613\n",
      "\n",
      "Epoch %d: train loss %f 104 0.33809848400679504\n",
      "Epoch 104: val loss 0.568477\n",
      "\n",
      "Epoch %d: train loss %f 105 0.31148183887655084\n",
      "Epoch 105: val loss 0.566389\n",
      "\n",
      "Epoch %d: train loss %f 106 0.3172216605056416\n",
      "Epoch 106: val loss 0.560293\n",
      "\n",
      "Epoch %d: train loss %f 107 0.2876484597271139\n",
      "Epoch 107: val loss 0.572194\n",
      "\n",
      "Epoch %d: train loss %f 108 0.2658060003410686\n",
      "Epoch 108: val loss 0.580457\n",
      "\n",
      "Epoch %d: train loss %f 109 0.3101646832444451\n",
      "Epoch 109: val loss 0.572095\n",
      "\n",
      "Epoch %d: train loss %f 110 0.29103654487566516\n",
      "Epoch 110: val loss 0.581937\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3408103639429266\n",
      "Epoch 111: val loss 0.603543\n",
      "\n",
      "Epoch %d: train loss %f 112 0.3472194129770452\n",
      "Epoch 112: val loss 0.586137\n",
      "\n",
      "Epoch %d: train loss %f 113 0.2940522588112138\n",
      "Epoch 113: val loss 0.588877\n",
      "\n",
      "Epoch %d: train loss %f 114 0.2652223286303607\n",
      "Epoch 114: val loss 0.587545\n",
      "\n",
      "Epoch %d: train loss %f 115 0.3317930420691317\n",
      "Epoch 115: val loss 0.567208\n",
      "\n",
      "Epoch %d: train loss %f 116 0.2890588505701585\n",
      "Epoch 116: val loss 0.570546\n",
      "\n",
      "Epoch %d: train loss %f 117 0.33146732774647797\n",
      "Epoch 117: val loss 0.592862\n",
      "\n",
      "Epoch %d: train loss %f 118 0.3219574967568571\n",
      "Epoch 118: val loss 0.588770\n",
      "\n",
      "Epoch %d: train loss %f 119 0.29010177471420984\n",
      "Epoch 119: val loss 0.614637\n",
      "\n",
      "Epoch %d: train loss %f 120 0.3042446591637351\n",
      "Epoch 120: val loss 0.607104\n",
      "\n",
      "Epoch %d: train loss %f 121 0.3032203438607129\n",
      "Epoch 121: val loss 0.587699\n",
      "\n",
      "Epoch %d: train loss %f 122 0.32673606276512146\n",
      "Epoch 122: val loss 0.567206\n",
      "\n",
      "Epoch %d: train loss %f 123 0.32015631144697015\n",
      "Epoch 123: val loss 0.575971\n",
      "\n",
      "Epoch %d: train loss %f 124 0.28887433355504816\n",
      "Epoch 124: val loss 0.581365\n",
      "\n",
      "Epoch %d: train loss %f 125 0.31326309659264306\n",
      "Epoch 125: val loss 0.591552\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3115552717989141\n",
      "Epoch 126: val loss 0.597340\n",
      "\n",
      "Epoch %d: train loss %f 127 0.30328432673757727\n",
      "Epoch 127: val loss 0.571376\n",
      "\n",
      "Epoch %d: train loss %f 128 0.36954326792196796\n",
      "Epoch 128: val loss 0.578579\n",
      "\n",
      "Epoch %d: train loss %f 129 0.30720137466083874\n",
      "Epoch 129: val loss 0.577648\n",
      "\n",
      "Epoch %d: train loss %f 130 0.3577589053999294\n",
      "Epoch 130: val loss 0.595935\n",
      "\n",
      "Epoch %d: train loss %f 131 0.3196617771278728\n",
      "Epoch 131: val loss 0.597013\n",
      "\n",
      "Epoch %d: train loss %f 132 0.2911906906149604\n",
      "Epoch 132: val loss 0.576513\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3069393878633326\n",
      "Epoch 133: val loss 0.609130\n",
      "\n",
      "Epoch %d: train loss %f 134 0.31350553035736084\n",
      "Epoch 134: val loss 0.595514\n",
      "\n",
      "Epoch %d: train loss %f 135 0.27711680260571564\n",
      "Epoch 135: val loss 0.592542\n",
      "\n",
      "Epoch %d: train loss %f 136 0.24115303565155377\n",
      "Epoch 136: val loss 0.584784\n",
      "\n",
      "Epoch %d: train loss %f 137 0.31251627477732574\n",
      "Epoch 137: val loss 0.583879\n",
      "\n",
      "Epoch %d: train loss %f 138 0.3069487078623338\n",
      "Epoch 138: val loss 0.592076\n",
      "\n",
      "Epoch %d: train loss %f 139 0.3044948198578574\n",
      "Epoch 139: val loss 0.617104\n",
      "\n",
      "Epoch %d: train loss %f 140 0.2753506045449864\n",
      "Epoch 140: val loss 0.623754\n",
      "\n",
      "Epoch %d: train loss %f 141 0.29758339713920245\n",
      "Epoch 141: val loss 0.637439\n",
      "\n",
      "Epoch %d: train loss %f 142 0.32469409568743274\n",
      "Epoch 142: val loss 0.643196\n",
      "\n",
      "Epoch %d: train loss %f 143 0.2681529013948007\n",
      "Epoch 143: val loss 0.624498\n",
      "\n",
      "Epoch %d: train loss %f 144 0.25806484845551575\n",
      "Epoch 144: val loss 0.649463\n",
      "\n",
      "Epoch %d: train loss %f 145 0.2791651284152811\n",
      "Epoch 145: val loss 0.633040\n",
      "\n",
      "Epoch %d: train loss %f 146 0.28597813980145886\n",
      "Epoch 146: val loss 0.640356\n",
      "\n",
      "Epoch %d: train loss %f 147 0.33585322309624066\n",
      "Epoch 147: val loss 0.639342\n",
      "\n",
      "Epoch %d: train loss %f 148 0.27114791897210205\n",
      "Epoch 148: val loss 0.634789\n",
      "\n",
      "Epoch %d: train loss %f 149 0.2698532911864194\n",
      "Epoch 149: val loss 0.643703\n",
      "\n",
      "Epoch %d: train loss %f 150 0.28721312365748664\n",
      "Epoch 150: val loss 0.623366\n",
      "\n",
      "Epoch %d: train loss %f 151 0.3004485151984475\n",
      "Epoch 151: val loss 0.631241\n",
      "\n",
      "Epoch %d: train loss %f 152 0.2856012433767319\n",
      "Epoch 152: val loss 0.625532\n",
      "\n",
      "Epoch %d: train loss %f 153 0.28433440828865225\n",
      "Epoch 153: val loss 0.631464\n",
      "\n",
      "Epoch %d: train loss %f 154 0.2810379524122585\n",
      "Epoch 154: val loss 0.623595\n",
      "\n",
      "Epoch %d: train loss %f 155 0.25045699693939905\n",
      "Epoch 155: val loss 0.629751\n",
      "\n",
      "Epoch %d: train loss %f 156 0.22151323475620963\n",
      "Epoch 156: val loss 0.650740\n",
      "\n",
      "Epoch %d: train loss %f 157 0.26579243215647613\n",
      "Epoch 157: val loss 0.647975\n",
      "\n",
      "Epoch %d: train loss %f 158 0.2843617634339766\n",
      "Epoch 158: val loss 0.619926\n",
      "\n",
      "Epoch %d: train loss %f 159 0.26050368357788434\n",
      "Epoch 159: val loss 0.645088\n",
      "\n",
      "Epoch %d: train loss %f 160 0.28781377456404944\n",
      "Epoch 160: val loss 0.629621\n",
      "\n",
      "Epoch %d: train loss %f 161 0.32745700532739813\n",
      "Epoch 161: val loss 0.641572\n",
      "\n",
      "Epoch %d: train loss %f 162 0.2895640202543952\n",
      "Epoch 162: val loss 0.662955\n",
      "\n",
      "Epoch %d: train loss %f 163 0.22559727728366852\n",
      "Epoch 163: val loss 0.642532\n",
      "\n",
      "Epoch %d: train loss %f 164 0.2374589741230011\n",
      "Epoch 164: val loss 0.647362\n",
      "\n",
      "Epoch %d: train loss %f 165 0.2650306218049743\n",
      "Epoch 165: val loss 0.673255\n",
      "\n",
      "Epoch %d: train loss %f 166 0.2941185330802744\n",
      "Epoch 166: val loss 0.680960\n",
      "\n",
      "Epoch %d: train loss %f 167 0.2649517181244763\n",
      "Epoch 167: val loss 0.687090\n",
      "\n",
      "Epoch %d: train loss %f 168 0.2645123451948166\n",
      "Epoch 168: val loss 0.650989\n",
      "\n",
      "Epoch %d: train loss %f 169 0.29187990860505536\n",
      "Epoch 169: val loss 0.630237\n",
      "\n",
      "Epoch %d: train loss %f 170 0.2913861538876187\n",
      "Epoch 170: val loss 0.637259\n",
      "\n",
      "Epoch %d: train loss %f 171 0.2534311603416096\n",
      "Epoch 171: val loss 0.635106\n",
      "\n",
      "Epoch %d: train loss %f 172 0.23393436047163876\n",
      "Epoch 172: val loss 0.645512\n",
      "\n",
      "Epoch %d: train loss %f 173 0.26206865771250293\n",
      "Epoch 173: val loss 0.651350\n",
      "\n",
      "Epoch %d: train loss %f 174 0.28397104279561475\n",
      "Epoch 174: val loss 0.633902\n",
      "\n",
      "Epoch %d: train loss %f 175 0.2809876569292762\n",
      "Epoch 175: val loss 0.677373\n",
      "\n",
      "Epoch %d: train loss %f 176 0.28356666592034424\n",
      "Epoch 176: val loss 0.643589\n",
      "\n",
      "Epoch %d: train loss %f 177 0.26915597915649414\n",
      "Epoch 177: val loss 0.662665\n",
      "\n",
      "Epoch %d: train loss %f 178 0.2719605971466411\n",
      "Epoch 178: val loss 0.681250\n",
      "\n",
      "Epoch %d: train loss %f 179 0.2936888269402764\n",
      "Epoch 179: val loss 0.673101\n",
      "\n",
      "Epoch %d: train loss %f 180 0.2674414718692953\n",
      "Epoch 180: val loss 0.626919\n",
      "\n",
      "Epoch %d: train loss %f 181 0.2867465073412115\n",
      "Epoch 181: val loss 0.644550\n",
      "\n",
      "Epoch %d: train loss %f 182 0.22756110538135876\n",
      "Epoch 182: val loss 0.671293\n",
      "\n",
      "Epoch %d: train loss %f 183 0.2403316314924847\n",
      "Epoch 183: val loss 0.645612\n",
      "\n",
      "Epoch %d: train loss %f 184 0.25554181838577444\n",
      "Epoch 184: val loss 0.649965\n",
      "\n",
      "Epoch %d: train loss %f 185 0.2639467641711235\n",
      "Epoch 185: val loss 0.659864\n",
      "\n",
      "Epoch %d: train loss %f 186 0.20773908360437912\n",
      "Epoch 186: val loss 0.645045\n",
      "\n",
      "Epoch %d: train loss %f 187 0.26408302783966064\n",
      "Epoch 187: val loss 0.649768\n",
      "\n",
      "Epoch %d: train loss %f 188 0.2752458575096997\n",
      "Epoch 188: val loss 0.645615\n",
      "\n",
      "Epoch %d: train loss %f 189 0.2702227939258922\n",
      "Epoch 189: val loss 0.677099\n",
      "\n",
      "Epoch %d: train loss %f 190 0.2451236674731428\n",
      "Epoch 190: val loss 0.660247\n",
      "\n",
      "Epoch %d: train loss %f 191 0.17689199339259754\n",
      "Epoch 191: val loss 0.678955\n",
      "\n",
      "Epoch %d: train loss %f 192 0.19928294555707413\n",
      "Epoch 192: val loss 0.678295\n",
      "\n",
      "Epoch %d: train loss %f 193 0.20119896937500348\n",
      "Epoch 193: val loss 0.677961\n",
      "\n",
      "Epoch %d: train loss %f 194 0.23800475556742062\n",
      "Epoch 194: val loss 0.671582\n",
      "\n",
      "Epoch %d: train loss %f 195 0.2552109563892538\n",
      "Epoch 195: val loss 0.680013\n",
      "\n",
      "Epoch %d: train loss %f 196 0.25428151000629773\n",
      "Epoch 196: val loss 0.678793\n",
      "\n",
      "Epoch %d: train loss %f 197 0.22550961781631818\n",
      "Epoch 197: val loss 0.632218\n",
      "\n",
      "Epoch %d: train loss %f 198 0.3313958468762311\n",
      "Epoch 198: val loss 0.658161\n",
      "\n",
      "Epoch %d: train loss %f 199 0.257175704294985\n",
      "Epoch 199: val loss 0.690007\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6946483146060597\n",
      "Epoch 0: val loss 0.694590\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6882831400090997\n",
      "Epoch 1: val loss 0.692147\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6794837604869496\n",
      "Epoch 2: val loss 0.687197\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6637565439397638\n",
      "Epoch 3: val loss 0.675813\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6528731855479154\n",
      "Epoch 4: val loss 0.653874\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6094570430842313\n",
      "Epoch 5: val loss 0.639623\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6008106903596357\n",
      "Epoch 6: val loss 0.636749\n",
      "\n",
      "Epoch %d: train loss %f 7 0.579433874650435\n",
      "Epoch 7: val loss 0.640952\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5771176517009735\n",
      "Epoch 8: val loss 0.632076\n",
      "\n",
      "Epoch %d: train loss %f 9 0.552796558900313\n",
      "Epoch 9: val loss 0.619492\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5457751588387922\n",
      "Epoch 10: val loss 0.607072\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5762531892819838\n",
      "Epoch 11: val loss 0.609559\n",
      "\n",
      "Epoch %d: train loss %f 12 0.535054393789985\n",
      "Epoch 12: val loss 0.631787\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5441605746746063\n",
      "Epoch 13: val loss 0.624918\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5227835124189203\n",
      "Epoch 14: val loss 0.610230\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5137776976281946\n",
      "Epoch 15: val loss 0.611739\n",
      "\n",
      "Epoch %d: train loss %f 16 0.48938813534649933\n",
      "Epoch 16: val loss 0.603530\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5089634982022372\n",
      "Epoch 17: val loss 0.598895\n",
      "\n",
      "Epoch %d: train loss %f 18 0.4873525310646404\n",
      "Epoch 18: val loss 0.588782\n",
      "\n",
      "Epoch %d: train loss %f 19 0.47728209874846717\n",
      "Epoch 19: val loss 0.594891\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5057038800282911\n",
      "Epoch 20: val loss 0.585840\n",
      "\n",
      "Epoch %d: train loss %f 21 0.47809713266112586\n",
      "Epoch 21: val loss 0.592470\n",
      "\n",
      "Epoch %d: train loss %f 22 0.47342238372022455\n",
      "Epoch 22: val loss 0.582304\n",
      "\n",
      "Epoch %d: train loss %f 23 0.46748497811230744\n",
      "Epoch 23: val loss 0.577797\n",
      "\n",
      "Epoch %d: train loss %f 24 0.4815891981124878\n",
      "Epoch 24: val loss 0.572598\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4561038992621682\n",
      "Epoch 25: val loss 0.591175\n",
      "\n",
      "Epoch %d: train loss %f 26 0.47504617409272626\n",
      "Epoch 26: val loss 0.597618\n",
      "\n",
      "Epoch %d: train loss %f 27 0.45611314611001447\n",
      "Epoch 27: val loss 0.553331\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4378839812495492\n",
      "Epoch 28: val loss 0.574408\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4540217247876254\n",
      "Epoch 29: val loss 0.584377\n",
      "\n",
      "Epoch %d: train loss %f 30 0.4287343973463232\n",
      "Epoch 30: val loss 0.571737\n",
      "\n",
      "Epoch %d: train loss %f 31 0.43441287224942987\n",
      "Epoch 31: val loss 0.573751\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4423076564615423\n",
      "Epoch 32: val loss 0.546229\n",
      "\n",
      "Epoch %d: train loss %f 33 0.3944564841010354\n",
      "Epoch 33: val loss 0.551929\n",
      "\n",
      "Epoch %d: train loss %f 34 0.40560814467343415\n",
      "Epoch 34: val loss 0.561475\n",
      "\n",
      "Epoch %d: train loss %f 35 0.39808031781153247\n",
      "Epoch 35: val loss 0.545885\n",
      "\n",
      "Epoch %d: train loss %f 36 0.43455634062940424\n",
      "Epoch 36: val loss 0.537763\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4208754165606065\n",
      "Epoch 37: val loss 0.557268\n",
      "\n",
      "Epoch %d: train loss %f 38 0.40150039439851587\n",
      "Epoch 38: val loss 0.569183\n",
      "\n",
      "Epoch %d: train loss %f 39 0.40983528982509265\n",
      "Epoch 39: val loss 0.588937\n",
      "\n",
      "Epoch %d: train loss %f 40 0.3937115560878407\n",
      "Epoch 40: val loss 0.564438\n",
      "\n",
      "Epoch %d: train loss %f 41 0.39091222936456854\n",
      "Epoch 41: val loss 0.560334\n",
      "\n",
      "Epoch %d: train loss %f 42 0.3955839547243985\n",
      "Epoch 42: val loss 0.573624\n",
      "\n",
      "Epoch %d: train loss %f 43 0.3538947728547183\n",
      "Epoch 43: val loss 0.566238\n",
      "\n",
      "Epoch %d: train loss %f 44 0.3836679377339103\n",
      "Epoch 44: val loss 0.594210\n",
      "\n",
      "Epoch %d: train loss %f 45 0.38337745585224847\n",
      "Epoch 45: val loss 0.599478\n",
      "\n",
      "Epoch %d: train loss %f 46 0.39433946392752905\n",
      "Epoch 46: val loss 0.598500\n",
      "\n",
      "Epoch %d: train loss %f 47 0.3750395964492451\n",
      "Epoch 47: val loss 0.581175\n",
      "\n",
      "Epoch %d: train loss %f 48 0.34336247227408667\n",
      "Epoch 48: val loss 0.563561\n",
      "\n",
      "Epoch %d: train loss %f 49 0.3841018609025262\n",
      "Epoch 49: val loss 0.602419\n",
      "\n",
      "Epoch %d: train loss %f 50 0.358407741243189\n",
      "Epoch 50: val loss 0.548385\n",
      "\n",
      "Epoch %d: train loss %f 51 0.35660371455279266\n",
      "Epoch 51: val loss 0.579276\n",
      "\n",
      "Epoch %d: train loss %f 52 0.3758034096522765\n",
      "Epoch 52: val loss 0.561352\n",
      "\n",
      "Epoch %d: train loss %f 53 0.3599970760670575\n",
      "Epoch 53: val loss 0.594650\n",
      "\n",
      "Epoch %d: train loss %f 54 0.3586000773039731\n",
      "Epoch 54: val loss 0.540111\n",
      "\n",
      "Epoch %d: train loss %f 55 0.35392980954863806\n",
      "Epoch 55: val loss 0.593799\n",
      "\n",
      "Epoch %d: train loss %f 56 0.32333830947225745\n",
      "Epoch 56: val loss 0.583859\n",
      "\n",
      "Epoch %d: train loss %f 57 0.35023479570042004\n",
      "Epoch 57: val loss 0.601302\n",
      "\n",
      "Epoch %d: train loss %f 58 0.31653233007951215\n",
      "Epoch 58: val loss 0.621908\n",
      "\n",
      "Epoch %d: train loss %f 59 0.33979636972600763\n",
      "Epoch 59: val loss 0.596481\n",
      "\n",
      "Epoch %d: train loss %f 60 0.3095511523160068\n",
      "Epoch 60: val loss 0.596571\n",
      "\n",
      "Epoch %d: train loss %f 61 0.31207062710415234\n",
      "Epoch 61: val loss 0.580826\n",
      "\n",
      "Epoch %d: train loss %f 62 0.3300307081504302\n",
      "Epoch 62: val loss 0.605704\n",
      "\n",
      "Epoch %d: train loss %f 63 0.33128252760930493\n",
      "Epoch 63: val loss 0.600684\n",
      "\n",
      "Epoch %d: train loss %f 64 0.3249810189008713\n",
      "Epoch 64: val loss 0.593605\n",
      "\n",
      "Epoch %d: train loss %f 65 0.2705931914123622\n",
      "Epoch 65: val loss 0.630686\n",
      "\n",
      "Epoch %d: train loss %f 66 0.31267795102162793\n",
      "Epoch 66: val loss 0.671394\n",
      "\n",
      "Epoch %d: train loss %f 67 0.3056950149211017\n",
      "Epoch 67: val loss 0.614843\n",
      "\n",
      "Epoch %d: train loss %f 68 0.2985780442302877\n",
      "Epoch 68: val loss 0.559603\n",
      "\n",
      "Epoch %d: train loss %f 69 0.3014975989406759\n",
      "Epoch 69: val loss 0.699542\n",
      "\n",
      "Epoch %d: train loss %f 70 0.33715679157863965\n",
      "Epoch 70: val loss 0.671325\n",
      "\n",
      "Epoch %d: train loss %f 71 0.3150419728322463\n",
      "Epoch 71: val loss 0.610883\n",
      "\n",
      "Epoch %d: train loss %f 72 0.3183437369086526\n",
      "Epoch 72: val loss 0.631222\n",
      "\n",
      "Epoch %d: train loss %f 73 0.2794808772477237\n",
      "Epoch 73: val loss 0.673610\n",
      "\n",
      "Epoch %d: train loss %f 74 0.3288291652094234\n",
      "Epoch 74: val loss 0.626852\n",
      "\n",
      "Epoch %d: train loss %f 75 0.32158676467158576\n",
      "Epoch 75: val loss 0.691934\n",
      "\n",
      "Epoch %d: train loss %f 76 0.3266692893071608\n",
      "Epoch 76: val loss 0.663775\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3199778307567943\n",
      "Epoch 77: val loss 0.666240\n",
      "\n",
      "Epoch %d: train loss %f 78 0.30800348249348725\n",
      "Epoch 78: val loss 0.675218\n",
      "\n",
      "Epoch %d: train loss %f 79 0.31859537146308203\n",
      "Epoch 79: val loss 0.659233\n",
      "\n",
      "Epoch %d: train loss %f 80 0.3638135086406361\n",
      "Epoch 80: val loss 0.596205\n",
      "\n",
      "Epoch %d: train loss %f 81 0.293584868311882\n",
      "Epoch 81: val loss 0.627762\n",
      "\n",
      "Epoch %d: train loss %f 82 0.31789239563725213\n",
      "Epoch 82: val loss 0.617185\n",
      "\n",
      "Epoch %d: train loss %f 83 0.31721408800645307\n",
      "Epoch 83: val loss 0.605560\n",
      "\n",
      "Epoch %d: train loss %f 84 0.2738669121807272\n",
      "Epoch 84: val loss 0.621654\n",
      "\n",
      "Epoch %d: train loss %f 85 0.26218110864812677\n",
      "Epoch 85: val loss 0.619380\n",
      "\n",
      "Epoch %d: train loss %f 86 0.32235005362467334\n",
      "Epoch 86: val loss 0.591413\n",
      "\n",
      "Epoch %d: train loss %f 87 0.26878572323105554\n",
      "Epoch 87: val loss 0.620266\n",
      "\n",
      "Epoch %d: train loss %f 88 0.29364022612571716\n",
      "Epoch 88: val loss 0.649415\n",
      "\n",
      "Epoch %d: train loss %f 89 0.28109718046405097\n",
      "Epoch 89: val loss 0.660268\n",
      "\n",
      "Epoch %d: train loss %f 90 0.2883916470137509\n",
      "Epoch 90: val loss 0.660913\n",
      "\n",
      "Epoch %d: train loss %f 91 0.3009834452108903\n",
      "Epoch 91: val loss 0.616487\n",
      "\n",
      "Epoch %d: train loss %f 92 0.3066367249597203\n",
      "Epoch 92: val loss 0.657262\n",
      "\n",
      "Epoch %d: train loss %f 93 0.31037588823925366\n",
      "Epoch 93: val loss 0.603835\n",
      "\n",
      "Epoch %d: train loss %f 94 0.2732052464376796\n",
      "Epoch 94: val loss 0.605279\n",
      "\n",
      "Epoch %d: train loss %f 95 0.2696324464949695\n",
      "Epoch 95: val loss 0.627942\n",
      "\n",
      "Epoch %d: train loss %f 96 0.260917586020448\n",
      "Epoch 96: val loss 0.615900\n",
      "\n",
      "Epoch %d: train loss %f 97 0.2885520336302844\n",
      "Epoch 97: val loss 0.693058\n",
      "\n",
      "Epoch %d: train loss %f 98 0.26906437359072943\n",
      "Epoch 98: val loss 0.687643\n",
      "\n",
      "Epoch %d: train loss %f 99 0.29989794303070416\n",
      "Epoch 99: val loss 0.675119\n",
      "\n",
      "Epoch %d: train loss %f 100 0.29912357980554755\n",
      "Epoch 100: val loss 0.699434\n",
      "\n",
      "Epoch %d: train loss %f 101 0.2716505134647543\n",
      "Epoch 101: val loss 0.647677\n",
      "\n",
      "Epoch %d: train loss %f 102 0.28605224734002893\n",
      "Epoch 102: val loss 0.662687\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3143471520055424\n",
      "Epoch 103: val loss 0.656913\n",
      "\n",
      "Epoch %d: train loss %f 104 0.2903688455169851\n",
      "Epoch 104: val loss 0.658693\n",
      "\n",
      "Epoch %d: train loss %f 105 0.3245875591581518\n",
      "Epoch 105: val loss 0.668566\n",
      "\n",
      "Epoch %d: train loss %f 106 0.2701107209379023\n",
      "Epoch 106: val loss 0.676256\n",
      "\n",
      "Epoch %d: train loss %f 107 0.28258735076947644\n",
      "Epoch 107: val loss 0.661307\n",
      "\n",
      "Epoch %d: train loss %f 108 0.27092107182199304\n",
      "Epoch 108: val loss 0.700048\n",
      "\n",
      "Epoch %d: train loss %f 109 0.22973152724179355\n",
      "Epoch 109: val loss 0.642973\n",
      "\n",
      "Epoch %d: train loss %f 110 0.2873450951142745\n",
      "Epoch 110: val loss 0.633264\n",
      "\n",
      "Epoch %d: train loss %f 111 0.2282428822734139\n",
      "Epoch 111: val loss 0.608932\n",
      "\n",
      "Epoch %d: train loss %f 112 0.24905345792120154\n",
      "Epoch 112: val loss 0.637612\n",
      "\n",
      "Epoch %d: train loss %f 113 0.25080170143734326\n",
      "Epoch 113: val loss 0.646757\n",
      "\n",
      "Epoch %d: train loss %f 114 0.28661695935509424\n",
      "Epoch 114: val loss 0.649386\n",
      "\n",
      "Epoch %d: train loss %f 115 0.3074831298806451\n",
      "Epoch 115: val loss 0.644653\n",
      "\n",
      "Epoch %d: train loss %f 116 0.2558099627494812\n",
      "Epoch 116: val loss 0.654092\n",
      "\n",
      "Epoch %d: train loss %f 117 0.24159992824901233\n",
      "Epoch 117: val loss 0.697936\n",
      "\n",
      "Epoch %d: train loss %f 118 0.2907667552882975\n",
      "Epoch 118: val loss 0.727360\n",
      "\n",
      "Epoch %d: train loss %f 119 0.2622726546092467\n",
      "Epoch 119: val loss 0.635612\n",
      "\n",
      "Epoch %d: train loss %f 120 0.2560660893266851\n",
      "Epoch 120: val loss 0.657073\n",
      "\n",
      "Epoch %d: train loss %f 121 0.24485421925783157\n",
      "Epoch 121: val loss 0.661828\n",
      "\n",
      "Epoch %d: train loss %f 122 0.23044463856653732\n",
      "Epoch 122: val loss 0.669514\n",
      "\n",
      "Epoch %d: train loss %f 123 0.24673602323640476\n",
      "Epoch 123: val loss 0.711809\n",
      "\n",
      "Epoch %d: train loss %f 124 0.26478979939764197\n",
      "Epoch 124: val loss 0.673312\n",
      "\n",
      "Epoch %d: train loss %f 125 0.22661043974486264\n",
      "Epoch 125: val loss 0.725940\n",
      "\n",
      "Epoch %d: train loss %f 126 0.2505450675433332\n",
      "Epoch 126: val loss 0.705950\n",
      "\n",
      "Epoch %d: train loss %f 127 0.23213339867916974\n",
      "Epoch 127: val loss 0.649334\n",
      "\n",
      "Epoch %d: train loss %f 128 0.25939610735936597\n",
      "Epoch 128: val loss 0.666807\n",
      "\n",
      "Epoch %d: train loss %f 129 0.22432774779471484\n",
      "Epoch 129: val loss 0.698747\n",
      "\n",
      "Epoch %d: train loss %f 130 0.24930552528663116\n",
      "Epoch 130: val loss 0.700121\n",
      "\n",
      "Epoch %d: train loss %f 131 0.23255512389269742\n",
      "Epoch 131: val loss 0.738059\n",
      "\n",
      "Epoch %d: train loss %f 132 0.2646813182668252\n",
      "Epoch 132: val loss 0.625254\n",
      "\n",
      "Epoch %d: train loss %f 133 0.2511802776293321\n",
      "Epoch 133: val loss 0.678031\n",
      "\n",
      "Epoch %d: train loss %f 134 0.2831164747476578\n",
      "Epoch 134: val loss 0.712263\n",
      "\n",
      "Epoch %d: train loss %f 135 0.2495136260986328\n",
      "Epoch 135: val loss 0.680998\n",
      "\n",
      "Epoch %d: train loss %f 136 0.25554256547581067\n",
      "Epoch 136: val loss 0.659747\n",
      "\n",
      "Epoch %d: train loss %f 137 0.2607468190518292\n",
      "Epoch 137: val loss 0.705162\n",
      "\n",
      "Epoch %d: train loss %f 138 0.2929861762306907\n",
      "Epoch 138: val loss 0.689107\n",
      "\n",
      "Epoch %d: train loss %f 139 0.2510324025695974\n",
      "Epoch 139: val loss 0.704051\n",
      "\n",
      "Epoch %d: train loss %f 140 0.30073836852203717\n",
      "Epoch 140: val loss 0.645387\n",
      "\n",
      "Epoch %d: train loss %f 141 0.2731431404298002\n",
      "Epoch 141: val loss 0.728162\n",
      "\n",
      "Epoch %d: train loss %f 142 0.2595268474383788\n",
      "Epoch 142: val loss 0.659758\n",
      "\n",
      "Epoch %d: train loss %f 143 0.2612567124041644\n",
      "Epoch 143: val loss 0.634404\n",
      "\n",
      "Epoch %d: train loss %f 144 0.24616158414970746\n",
      "Epoch 144: val loss 0.765151\n",
      "\n",
      "Epoch %d: train loss %f 145 0.276123048229651\n",
      "Epoch 145: val loss 0.663023\n",
      "\n",
      "Epoch %d: train loss %f 146 0.27307048033584247\n",
      "Epoch 146: val loss 0.725024\n",
      "\n",
      "Epoch %d: train loss %f 147 0.21760910681702875\n",
      "Epoch 147: val loss 0.674708\n",
      "\n",
      "Epoch %d: train loss %f 148 0.25569222190163354\n",
      "Epoch 148: val loss 0.754584\n",
      "\n",
      "Epoch %d: train loss %f 149 0.2070323797789487\n",
      "Epoch 149: val loss 0.703188\n",
      "\n",
      "Epoch %d: train loss %f 150 0.22774988141926852\n",
      "Epoch 150: val loss 0.734573\n",
      "\n",
      "Epoch %d: train loss %f 151 0.24335851994427768\n",
      "Epoch 151: val loss 0.717027\n",
      "\n",
      "Epoch %d: train loss %f 152 0.22963667999614368\n",
      "Epoch 152: val loss 0.742160\n",
      "\n",
      "Epoch %d: train loss %f 153 0.25669489394534717\n",
      "Epoch 153: val loss 0.713454\n",
      "\n",
      "Epoch %d: train loss %f 154 0.24722340228882703\n",
      "Epoch 154: val loss 0.738805\n",
      "\n",
      "Epoch %d: train loss %f 155 0.2978785051540895\n",
      "Epoch 155: val loss 0.749454\n",
      "\n",
      "Epoch %d: train loss %f 156 0.2517147341912443\n",
      "Epoch 156: val loss 0.737172\n",
      "\n",
      "Epoch %d: train loss %f 157 0.2272096953608773\n",
      "Epoch 157: val loss 0.712233\n",
      "\n",
      "Epoch %d: train loss %f 158 0.23056541992859406\n",
      "Epoch 158: val loss 0.712122\n",
      "\n",
      "Epoch %d: train loss %f 159 0.25064767626198853\n",
      "Epoch 159: val loss 0.726710\n",
      "\n",
      "Epoch %d: train loss %f 160 0.20873395624485883\n",
      "Epoch 160: val loss 0.731546\n",
      "\n",
      "Epoch %d: train loss %f 161 0.22081989727237009\n",
      "Epoch 161: val loss 0.767320\n",
      "\n",
      "Epoch %d: train loss %f 162 0.23774996264414353\n",
      "Epoch 162: val loss 0.719558\n",
      "\n",
      "Epoch %d: train loss %f 163 0.2278083644130013\n",
      "Epoch 163: val loss 0.762125\n",
      "\n",
      "Epoch %d: train loss %f 164 0.25880994309078564\n",
      "Epoch 164: val loss 0.763201\n",
      "\n",
      "Epoch %d: train loss %f 165 0.27313546192916954\n",
      "Epoch 165: val loss 0.748233\n",
      "\n",
      "Epoch %d: train loss %f 166 0.21678829328580337\n",
      "Epoch 166: val loss 0.689723\n",
      "\n",
      "Epoch %d: train loss %f 167 0.230737911706621\n",
      "Epoch 167: val loss 0.667884\n",
      "\n",
      "Epoch %d: train loss %f 168 0.2569487311623313\n",
      "Epoch 168: val loss 0.679220\n",
      "\n",
      "Epoch %d: train loss %f 169 0.2230399955402721\n",
      "Epoch 169: val loss 0.716584\n",
      "\n",
      "Epoch %d: train loss %f 170 0.20173202353444966\n",
      "Epoch 170: val loss 0.709645\n",
      "\n",
      "Epoch %d: train loss %f 171 0.23452236791225997\n",
      "Epoch 171: val loss 0.702614\n",
      "\n",
      "Epoch %d: train loss %f 172 0.19607691805471072\n",
      "Epoch 172: val loss 0.735286\n",
      "\n",
      "Epoch %d: train loss %f 173 0.21115986799651926\n",
      "Epoch 173: val loss 0.736136\n",
      "\n",
      "Epoch %d: train loss %f 174 0.22126404805616898\n",
      "Epoch 174: val loss 0.720997\n",
      "\n",
      "Epoch %d: train loss %f 175 0.27656097236004745\n",
      "Epoch 175: val loss 0.761530\n",
      "\n",
      "Epoch %d: train loss %f 176 0.22424288297241385\n",
      "Epoch 176: val loss 0.717826\n",
      "\n",
      "Epoch %d: train loss %f 177 0.22278706187551672\n",
      "Epoch 177: val loss 0.773369\n",
      "\n",
      "Epoch %d: train loss %f 178 0.2344069548628547\n",
      "Epoch 178: val loss 0.722456\n",
      "\n",
      "Epoch %d: train loss %f 179 0.20631062442606146\n",
      "Epoch 179: val loss 0.734436\n",
      "\n",
      "Epoch %d: train loss %f 180 0.23889119110324167\n",
      "Epoch 180: val loss 0.815517\n",
      "\n",
      "Epoch %d: train loss %f 181 0.21975018016316675\n",
      "Epoch 181: val loss 0.677814\n",
      "\n",
      "Epoch %d: train loss %f 182 0.25347459045323456\n",
      "Epoch 182: val loss 0.672510\n",
      "\n",
      "Epoch %d: train loss %f 183 0.19704856926744635\n",
      "Epoch 183: val loss 0.769389\n",
      "\n",
      "Epoch %d: train loss %f 184 0.26524884660135617\n",
      "Epoch 184: val loss 0.703257\n",
      "\n",
      "Epoch %d: train loss %f 185 0.18365519087422977\n",
      "Epoch 185: val loss 0.736286\n",
      "\n",
      "Epoch %d: train loss %f 186 0.20713955638083545\n",
      "Epoch 186: val loss 0.741694\n",
      "\n",
      "Epoch %d: train loss %f 187 0.200086080215194\n",
      "Epoch 187: val loss 0.787179\n",
      "\n",
      "Epoch %d: train loss %f 188 0.1928205300461162\n",
      "Epoch 188: val loss 0.767227\n",
      "\n",
      "Epoch %d: train loss %f 189 0.20649916543201965\n",
      "Epoch 189: val loss 0.779359\n",
      "\n",
      "Epoch %d: train loss %f 190 0.23414974862878973\n",
      "Epoch 190: val loss 0.721836\n",
      "\n",
      "Epoch %d: train loss %f 191 0.25867679478092626\n",
      "Epoch 191: val loss 0.718631\n",
      "\n",
      "Epoch %d: train loss %f 192 0.18644629892977801\n",
      "Epoch 192: val loss 0.762331\n",
      "\n",
      "Epoch %d: train loss %f 193 0.24267856776714325\n",
      "Epoch 193: val loss 0.793739\n",
      "\n",
      "Epoch %d: train loss %f 194 0.20619870857758957\n",
      "Epoch 194: val loss 0.803897\n",
      "\n",
      "Epoch %d: train loss %f 195 0.21815259178931062\n",
      "Epoch 195: val loss 0.780371\n",
      "\n",
      "Epoch %d: train loss %f 196 0.1984818381342021\n",
      "Epoch 196: val loss 0.685887\n",
      "\n",
      "Epoch %d: train loss %f 197 0.2713852511210875\n",
      "Epoch 197: val loss 0.697605\n",
      "\n",
      "Epoch %d: train loss %f 198 0.2186455943367698\n",
      "Epoch 198: val loss 0.699304\n",
      "\n",
      "Epoch %d: train loss %f 199 0.2602621777491136\n",
      "Epoch 199: val loss 0.788091\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6892507563937794\n",
      "Epoch 0: val loss 0.689787\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6870730736038901\n",
      "Epoch 1: val loss 0.687794\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6771210919726979\n",
      "Epoch 2: val loss 0.683996\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6686837889931418\n",
      "Epoch 3: val loss 0.675326\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6632459542968057\n",
      "Epoch 4: val loss 0.661202\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6449150226332925\n",
      "Epoch 5: val loss 0.646862\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6245775276964362\n",
      "Epoch 6: val loss 0.625815\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6182632446289062\n",
      "Epoch 7: val loss 0.608090\n",
      "\n",
      "Epoch %d: train loss %f 8 0.6101049076427113\n",
      "Epoch 8: val loss 0.603670\n",
      "\n",
      "Epoch %d: train loss %f 9 0.6053684543479573\n",
      "Epoch 9: val loss 0.587405\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5722455978393555\n",
      "Epoch 10: val loss 0.579609\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5814815407449548\n",
      "Epoch 11: val loss 0.573934\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5576576536351984\n",
      "Epoch 12: val loss 0.561762\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5710484954443845\n",
      "Epoch 13: val loss 0.560351\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5432963046160612\n",
      "Epoch 14: val loss 0.561315\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5299871780655601\n",
      "Epoch 15: val loss 0.541120\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5293683274225756\n",
      "Epoch 16: val loss 0.538480\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5231807665391401\n",
      "Epoch 17: val loss 0.542674\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5210010030052878\n",
      "Epoch 18: val loss 0.541068\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5447973852807825\n",
      "Epoch 19: val loss 0.533710\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5090335228226401\n",
      "Epoch 20: val loss 0.531688\n",
      "\n",
      "Epoch %d: train loss %f 21 0.529130989854986\n",
      "Epoch 21: val loss 0.535823\n",
      "\n",
      "Epoch %d: train loss %f 22 0.4826183454556899\n",
      "Epoch 22: val loss 0.535867\n",
      "\n",
      "Epoch %d: train loss %f 23 0.5078614516691728\n",
      "Epoch 23: val loss 0.539210\n",
      "\n",
      "Epoch %d: train loss %f 24 0.4923641058531674\n",
      "Epoch 24: val loss 0.521490\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4876603971828114\n",
      "Epoch 25: val loss 0.532610\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4711523814634843\n",
      "Epoch 26: val loss 0.520036\n",
      "\n",
      "Epoch %d: train loss %f 27 0.47334572672843933\n",
      "Epoch 27: val loss 0.532967\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4546845013445074\n",
      "Epoch 28: val loss 0.529794\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4784012626517903\n",
      "Epoch 29: val loss 0.527938\n",
      "\n",
      "Epoch %d: train loss %f 30 0.4552117206833579\n",
      "Epoch 30: val loss 0.521906\n",
      "\n",
      "Epoch %d: train loss %f 31 0.454730516130274\n",
      "Epoch 31: val loss 0.518536\n",
      "\n",
      "Epoch %d: train loss %f 32 0.47208322991024365\n",
      "Epoch 32: val loss 0.524010\n",
      "\n",
      "Epoch %d: train loss %f 33 0.45887153257023205\n",
      "Epoch 33: val loss 0.526509\n",
      "\n",
      "Epoch %d: train loss %f 34 0.44269944862885907\n",
      "Epoch 34: val loss 0.544214\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4557539305903695\n",
      "Epoch 35: val loss 0.515872\n",
      "\n",
      "Epoch %d: train loss %f 36 0.46519187092781067\n",
      "Epoch 36: val loss 0.541766\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4575776864181865\n",
      "Epoch 37: val loss 0.517570\n",
      "\n",
      "Epoch %d: train loss %f 38 0.46043942733244464\n",
      "Epoch 38: val loss 0.538318\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4396437948400324\n",
      "Epoch 39: val loss 0.510146\n",
      "\n",
      "Epoch %d: train loss %f 40 0.41598377986387775\n",
      "Epoch 40: val loss 0.529902\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4335376674478704\n",
      "Epoch 41: val loss 0.528894\n",
      "\n",
      "Epoch %d: train loss %f 42 0.40127597884698346\n",
      "Epoch 42: val loss 0.514496\n",
      "\n",
      "Epoch %d: train loss %f 43 0.43191054463386536\n",
      "Epoch 43: val loss 0.521556\n",
      "\n",
      "Epoch %d: train loss %f 44 0.446228872645985\n",
      "Epoch 44: val loss 0.539505\n",
      "\n",
      "Epoch %d: train loss %f 45 0.44549607688730414\n",
      "Epoch 45: val loss 0.535296\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4318579123778777\n",
      "Epoch 46: val loss 0.530049\n",
      "\n",
      "Epoch %d: train loss %f 47 0.46592405709353363\n",
      "Epoch 47: val loss 0.498913\n",
      "\n",
      "Epoch %d: train loss %f 48 0.4415227039293809\n",
      "Epoch 48: val loss 0.486161\n",
      "\n",
      "Epoch %d: train loss %f 49 0.44488942894068634\n",
      "Epoch 49: val loss 0.509474\n",
      "\n",
      "Epoch %d: train loss %f 50 0.44770337776704267\n",
      "Epoch 50: val loss 0.497973\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4051049148494547\n",
      "Epoch 51: val loss 0.521969\n",
      "\n",
      "Epoch %d: train loss %f 52 0.43705483729189093\n",
      "Epoch 52: val loss 0.516664\n",
      "\n",
      "Epoch %d: train loss %f 53 0.41596767035397614\n",
      "Epoch 53: val loss 0.529246\n",
      "\n",
      "Epoch %d: train loss %f 54 0.43131689591841266\n",
      "Epoch 54: val loss 0.506939\n",
      "\n",
      "Epoch %d: train loss %f 55 0.422233754938299\n",
      "Epoch 55: val loss 0.528855\n",
      "\n",
      "Epoch %d: train loss %f 56 0.4137806458906694\n",
      "Epoch 56: val loss 0.546309\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4200634170662273\n",
      "Epoch 57: val loss 0.507815\n",
      "\n",
      "Epoch %d: train loss %f 58 0.4384062344377691\n",
      "Epoch 58: val loss 0.521408\n",
      "\n",
      "Epoch %d: train loss %f 59 0.4114855771714991\n",
      "Epoch 59: val loss 0.524792\n",
      "\n",
      "Epoch %d: train loss %f 60 0.40803089466961945\n",
      "Epoch 60: val loss 0.543581\n",
      "\n",
      "Epoch %d: train loss %f 61 0.41524734822186554\n",
      "Epoch 61: val loss 0.547796\n",
      "\n",
      "Epoch %d: train loss %f 62 0.4191111109473489\n",
      "Epoch 62: val loss 0.530052\n",
      "\n",
      "Epoch %d: train loss %f 63 0.4140594831921838\n",
      "Epoch 63: val loss 0.529991\n",
      "\n",
      "Epoch %d: train loss %f 64 0.4178099280053919\n",
      "Epoch 64: val loss 0.534769\n",
      "\n",
      "Epoch %d: train loss %f 65 0.390455497936769\n",
      "Epoch 65: val loss 0.550922\n",
      "\n",
      "Epoch %d: train loss %f 66 0.44004004109989514\n",
      "Epoch 66: val loss 0.544286\n",
      "\n",
      "Epoch %d: train loss %f 67 0.41668799790469085\n",
      "Epoch 67: val loss 0.555429\n",
      "\n",
      "Epoch %d: train loss %f 68 0.3931561437520114\n",
      "Epoch 68: val loss 0.506734\n",
      "\n",
      "Epoch %d: train loss %f 69 0.38949209451675415\n",
      "Epoch 69: val loss 0.516791\n",
      "\n",
      "Epoch %d: train loss %f 70 0.448743226853284\n",
      "Epoch 70: val loss 0.528973\n",
      "\n",
      "Epoch %d: train loss %f 71 0.38803598420186475\n",
      "Epoch 71: val loss 0.500889\n",
      "\n",
      "Epoch %d: train loss %f 72 0.3825015723705292\n",
      "Epoch 72: val loss 0.540543\n",
      "\n",
      "Epoch %d: train loss %f 73 0.4243987487121062\n",
      "Epoch 73: val loss 0.524807\n",
      "\n",
      "Epoch %d: train loss %f 74 0.3764100047675046\n",
      "Epoch 74: val loss 0.547748\n",
      "\n",
      "Epoch %d: train loss %f 75 0.36777094277468597\n",
      "Epoch 75: val loss 0.535559\n",
      "\n",
      "Epoch %d: train loss %f 76 0.38766561448574066\n",
      "Epoch 76: val loss 0.555115\n",
      "\n",
      "Epoch %d: train loss %f 77 0.38342173262075946\n",
      "Epoch 77: val loss 0.532724\n",
      "\n",
      "Epoch %d: train loss %f 78 0.4039750668135556\n",
      "Epoch 78: val loss 0.527221\n",
      "\n",
      "Epoch %d: train loss %f 79 0.35537109998139466\n",
      "Epoch 79: val loss 0.525334\n",
      "\n",
      "Epoch %d: train loss %f 80 0.36282012408429926\n",
      "Epoch 80: val loss 0.526577\n",
      "\n",
      "Epoch %d: train loss %f 81 0.3394184789874337\n",
      "Epoch 81: val loss 0.535893\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3637246692722494\n",
      "Epoch 82: val loss 0.529850\n",
      "\n",
      "Epoch %d: train loss %f 83 0.37145721912384033\n",
      "Epoch 83: val loss 0.541990\n",
      "\n",
      "Epoch %d: train loss %f 84 0.36701904372735455\n",
      "Epoch 84: val loss 0.534916\n",
      "\n",
      "Epoch %d: train loss %f 85 0.3493699323047291\n",
      "Epoch 85: val loss 0.566707\n",
      "\n",
      "Epoch %d: train loss %f 86 0.4130777526985515\n",
      "Epoch 86: val loss 0.568092\n",
      "\n",
      "Epoch %d: train loss %f 87 0.38129048455845227\n",
      "Epoch 87: val loss 0.552176\n",
      "\n",
      "Epoch %d: train loss %f 88 0.37115835601633246\n",
      "Epoch 88: val loss 0.580076\n",
      "\n",
      "Epoch %d: train loss %f 89 0.3903542011976242\n",
      "Epoch 89: val loss 0.533184\n",
      "\n",
      "Epoch %d: train loss %f 90 0.374597965316339\n",
      "Epoch 90: val loss 0.560028\n",
      "\n",
      "Epoch %d: train loss %f 91 0.3551132502880963\n",
      "Epoch 91: val loss 0.523118\n",
      "\n",
      "Epoch %d: train loss %f 92 0.3505078147758137\n",
      "Epoch 92: val loss 0.518876\n",
      "\n",
      "Epoch %d: train loss %f 93 0.37175776199861005\n",
      "Epoch 93: val loss 0.523428\n",
      "\n",
      "Epoch %d: train loss %f 94 0.3889517811211673\n",
      "Epoch 94: val loss 0.547262\n",
      "\n",
      "Epoch %d: train loss %f 95 0.4117132479494268\n",
      "Epoch 95: val loss 0.512565\n",
      "\n",
      "Epoch %d: train loss %f 96 0.3415309868075631\n",
      "Epoch 96: val loss 0.566740\n",
      "\n",
      "Epoch %d: train loss %f 97 0.3747898042201996\n",
      "Epoch 97: val loss 0.511612\n",
      "\n",
      "Epoch %d: train loss %f 98 0.34323512695052405\n",
      "Epoch 98: val loss 0.541688\n",
      "\n",
      "Epoch %d: train loss %f 99 0.349167982285673\n",
      "Epoch 99: val loss 0.537703\n",
      "\n",
      "Epoch %d: train loss %f 100 0.31575216352939606\n",
      "Epoch 100: val loss 0.532176\n",
      "\n",
      "Epoch %d: train loss %f 101 0.34702313488179987\n",
      "Epoch 101: val loss 0.563465\n",
      "\n",
      "Epoch %d: train loss %f 102 0.3757889744910327\n",
      "Epoch 102: val loss 0.509271\n",
      "\n",
      "Epoch %d: train loss %f 103 0.33906606110659515\n",
      "Epoch 103: val loss 0.534706\n",
      "\n",
      "Epoch %d: train loss %f 104 0.3740431558002125\n",
      "Epoch 104: val loss 0.527170\n",
      "\n",
      "Epoch %d: train loss %f 105 0.33424049751325086\n",
      "Epoch 105: val loss 0.520600\n",
      "\n",
      "Epoch %d: train loss %f 106 0.34218824722550134\n",
      "Epoch 106: val loss 0.525184\n",
      "\n",
      "Epoch %d: train loss %f 107 0.30565034733577207\n",
      "Epoch 107: val loss 0.527229\n",
      "\n",
      "Epoch %d: train loss %f 108 0.32433889399875293\n",
      "Epoch 108: val loss 0.526327\n",
      "\n",
      "Epoch %d: train loss %f 109 0.3206686038862575\n",
      "Epoch 109: val loss 0.560068\n",
      "\n",
      "Epoch %d: train loss %f 110 0.3433463302525607\n",
      "Epoch 110: val loss 0.550699\n",
      "\n",
      "Epoch %d: train loss %f 111 0.2936288673769344\n",
      "Epoch 111: val loss 0.560891\n",
      "\n",
      "Epoch %d: train loss %f 112 0.3349377905780619\n",
      "Epoch 112: val loss 0.534834\n",
      "\n",
      "Epoch %d: train loss %f 113 0.3418303985487331\n",
      "Epoch 113: val loss 0.559614\n",
      "\n",
      "Epoch %d: train loss %f 114 0.34066758914427325\n",
      "Epoch 114: val loss 0.571709\n",
      "\n",
      "Epoch %d: train loss %f 115 0.36264335160905664\n",
      "Epoch 115: val loss 0.543376\n",
      "\n",
      "Epoch %d: train loss %f 116 0.35868320817297156\n",
      "Epoch 116: val loss 0.525942\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3396259857849641\n",
      "Epoch 117: val loss 0.511706\n",
      "\n",
      "Epoch %d: train loss %f 118 0.3312766978686506\n",
      "Epoch 118: val loss 0.514513\n",
      "\n",
      "Epoch %d: train loss %f 119 0.33592619543725794\n",
      "Epoch 119: val loss 0.534756\n",
      "\n",
      "Epoch %d: train loss %f 120 0.33766072582114826\n",
      "Epoch 120: val loss 0.519789\n",
      "\n",
      "Epoch %d: train loss %f 121 0.363607104529034\n",
      "Epoch 121: val loss 0.510805\n",
      "\n",
      "Epoch %d: train loss %f 122 0.3167259855703874\n",
      "Epoch 122: val loss 0.573002\n",
      "\n",
      "Epoch %d: train loss %f 123 0.30425355109301483\n",
      "Epoch 123: val loss 0.548507\n",
      "\n",
      "Epoch %d: train loss %f 124 0.3440499861132015\n",
      "Epoch 124: val loss 0.556993\n",
      "\n",
      "Epoch %d: train loss %f 125 0.3144787319681861\n",
      "Epoch 125: val loss 0.556130\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3413038511167873\n",
      "Epoch 126: val loss 0.527401\n",
      "\n",
      "Epoch %d: train loss %f 127 0.3545424667271701\n",
      "Epoch 127: val loss 0.564629\n",
      "\n",
      "Epoch %d: train loss %f 128 0.3127586773850701\n",
      "Epoch 128: val loss 0.526193\n",
      "\n",
      "Epoch %d: train loss %f 129 0.3194690414450385\n",
      "Epoch 129: val loss 0.562979\n",
      "\n",
      "Epoch %d: train loss %f 130 0.3864082111553712\n",
      "Epoch 130: val loss 0.584548\n",
      "\n",
      "Epoch %d: train loss %f 131 0.3477530113675378\n",
      "Epoch 131: val loss 0.510022\n",
      "\n",
      "Epoch %d: train loss %f 132 0.32528841630979016\n",
      "Epoch 132: val loss 0.547696\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3547054339538921\n",
      "Epoch 133: val loss 0.528515\n",
      "\n",
      "Epoch %d: train loss %f 134 0.33746142008087854\n",
      "Epoch 134: val loss 0.547314\n",
      "\n",
      "Epoch %d: train loss %f 135 0.324025496840477\n",
      "Epoch 135: val loss 0.533927\n",
      "\n",
      "Epoch %d: train loss %f 136 0.3697671768340198\n",
      "Epoch 136: val loss 0.572480\n",
      "\n",
      "Epoch %d: train loss %f 137 0.3285984749143774\n",
      "Epoch 137: val loss 0.563993\n",
      "\n",
      "Epoch %d: train loss %f 138 0.3475478941744024\n",
      "Epoch 138: val loss 0.522368\n",
      "\n",
      "Epoch %d: train loss %f 139 0.3098962672732093\n",
      "Epoch 139: val loss 0.573836\n",
      "\n",
      "Epoch %d: train loss %f 140 0.3185981281779029\n",
      "Epoch 140: val loss 0.535884\n",
      "\n",
      "Epoch %d: train loss %f 141 0.31535292078148236\n",
      "Epoch 141: val loss 0.565117\n",
      "\n",
      "Epoch %d: train loss %f 142 0.26551550897684967\n",
      "Epoch 142: val loss 0.527008\n",
      "\n",
      "Epoch %d: train loss %f 143 0.32095571810548956\n",
      "Epoch 143: val loss 0.544688\n",
      "\n",
      "Epoch %d: train loss %f 144 0.3047544929114255\n",
      "Epoch 144: val loss 0.535703\n",
      "\n",
      "Epoch %d: train loss %f 145 0.29028190672397614\n",
      "Epoch 145: val loss 0.531296\n",
      "\n",
      "Epoch %d: train loss %f 146 0.3342803120613098\n",
      "Epoch 146: val loss 0.513833\n",
      "\n",
      "Epoch %d: train loss %f 147 0.2768277417529713\n",
      "Epoch 147: val loss 0.503918\n",
      "\n",
      "Epoch %d: train loss %f 148 0.3136192424730821\n",
      "Epoch 148: val loss 0.520418\n",
      "\n",
      "Epoch %d: train loss %f 149 0.34481085024096747\n",
      "Epoch 149: val loss 0.511386\n",
      "\n",
      "Epoch %d: train loss %f 150 0.3373756760900671\n",
      "Epoch 150: val loss 0.496772\n",
      "\n",
      "Epoch %d: train loss %f 151 0.2897515757517381\n",
      "Epoch 151: val loss 0.512324\n",
      "\n",
      "Epoch %d: train loss %f 152 0.3002919134768573\n",
      "Epoch 152: val loss 0.522600\n",
      "\n",
      "Epoch %d: train loss %f 153 0.28628338466991077\n",
      "Epoch 153: val loss 0.569680\n",
      "\n",
      "Epoch %d: train loss %f 154 0.30265484072945337\n",
      "Epoch 154: val loss 0.532815\n",
      "\n",
      "Epoch %d: train loss %f 155 0.2904441627589139\n",
      "Epoch 155: val loss 0.567340\n",
      "\n",
      "Epoch %d: train loss %f 156 0.3181530006907203\n",
      "Epoch 156: val loss 0.526451\n",
      "\n",
      "Epoch %d: train loss %f 157 0.27687751163135876\n",
      "Epoch 157: val loss 0.532206\n",
      "\n",
      "Epoch %d: train loss %f 158 0.323948090726679\n",
      "Epoch 158: val loss 0.538363\n",
      "\n",
      "Epoch %d: train loss %f 159 0.3034310462799939\n",
      "Epoch 159: val loss 0.542833\n",
      "\n",
      "Epoch %d: train loss %f 160 0.3328108557245948\n",
      "Epoch 160: val loss 0.575570\n",
      "\n",
      "Epoch %d: train loss %f 161 0.31528751145709644\n",
      "Epoch 161: val loss 0.528709\n",
      "\n",
      "Epoch %d: train loss %f 162 0.3066026825796474\n",
      "Epoch 162: val loss 0.565297\n",
      "\n",
      "Epoch %d: train loss %f 163 0.28307376260107214\n",
      "Epoch 163: val loss 0.527495\n",
      "\n",
      "Epoch %d: train loss %f 164 0.28523660654371435\n",
      "Epoch 164: val loss 0.532329\n",
      "\n",
      "Epoch %d: train loss %f 165 0.2854444140737707\n",
      "Epoch 165: val loss 0.534683\n",
      "\n",
      "Epoch %d: train loss %f 166 0.29925216328014026\n",
      "Epoch 166: val loss 0.524336\n",
      "\n",
      "Epoch %d: train loss %f 167 0.29875839298421686\n",
      "Epoch 167: val loss 0.541953\n",
      "\n",
      "Epoch %d: train loss %f 168 0.28376633606173773\n",
      "Epoch 168: val loss 0.539892\n",
      "\n",
      "Epoch %d: train loss %f 169 0.302328269590031\n",
      "Epoch 169: val loss 0.541159\n",
      "\n",
      "Epoch %d: train loss %f 170 0.31575729088349774\n",
      "Epoch 170: val loss 0.531166\n",
      "\n",
      "Epoch %d: train loss %f 171 0.28292523324489594\n",
      "Epoch 171: val loss 0.582909\n",
      "\n",
      "Epoch %d: train loss %f 172 0.24433989145539023\n",
      "Epoch 172: val loss 0.547895\n",
      "\n",
      "Epoch %d: train loss %f 173 0.25743126666004007\n",
      "Epoch 173: val loss 0.583599\n",
      "\n",
      "Epoch %d: train loss %f 174 0.313576034524224\n",
      "Epoch 174: val loss 0.561907\n",
      "\n",
      "Epoch %d: train loss %f 175 0.2876089093360034\n",
      "Epoch 175: val loss 0.546217\n",
      "\n",
      "Epoch %d: train loss %f 176 0.269147436727177\n",
      "Epoch 176: val loss 0.549735\n",
      "\n",
      "Epoch %d: train loss %f 177 0.2772543051026084\n",
      "Epoch 177: val loss 0.556008\n",
      "\n",
      "Epoch %d: train loss %f 178 0.27356785806742584\n",
      "Epoch 178: val loss 0.553042\n",
      "\n",
      "Epoch %d: train loss %f 179 0.30039446326819336\n",
      "Epoch 179: val loss 0.540247\n",
      "\n",
      "Epoch %d: train loss %f 180 0.3057375184514306\n",
      "Epoch 180: val loss 0.551547\n",
      "\n",
      "Epoch %d: train loss %f 181 0.295852315696803\n",
      "Epoch 181: val loss 0.548742\n",
      "\n",
      "Epoch %d: train loss %f 182 0.2780092683705417\n",
      "Epoch 182: val loss 0.511470\n",
      "\n",
      "Epoch %d: train loss %f 183 0.2720032144676555\n",
      "Epoch 183: val loss 0.593767\n",
      "\n",
      "Epoch %d: train loss %f 184 0.29892583055929706\n",
      "Epoch 184: val loss 0.542280\n",
      "\n",
      "Epoch %d: train loss %f 185 0.31921165504238824\n",
      "Epoch 185: val loss 0.588001\n",
      "\n",
      "Epoch %d: train loss %f 186 0.2913062409921126\n",
      "Epoch 186: val loss 0.527799\n",
      "\n",
      "Epoch %d: train loss %f 187 0.32525140182538465\n",
      "Epoch 187: val loss 0.609688\n",
      "\n",
      "Epoch %d: train loss %f 188 0.3067829486998645\n",
      "Epoch 188: val loss 0.516365\n",
      "\n",
      "Epoch %d: train loss %f 189 0.2832740572365848\n",
      "Epoch 189: val loss 0.542312\n",
      "\n",
      "Epoch %d: train loss %f 190 0.3216617926955223\n",
      "Epoch 190: val loss 0.514231\n",
      "\n",
      "Epoch %d: train loss %f 191 0.33848072723908856\n",
      "Epoch 191: val loss 0.511355\n",
      "\n",
      "Epoch %d: train loss %f 192 0.26967675442045386\n",
      "Epoch 192: val loss 0.608450\n",
      "\n",
      "Epoch %d: train loss %f 193 0.2668393220413815\n",
      "Epoch 193: val loss 0.525528\n",
      "\n",
      "Epoch %d: train loss %f 194 0.2936165305701169\n",
      "Epoch 194: val loss 0.521580\n",
      "\n",
      "Epoch %d: train loss %f 195 0.27281885932792316\n",
      "Epoch 195: val loss 0.514935\n",
      "\n",
      "Epoch %d: train loss %f 196 0.28407054191285913\n",
      "Epoch 196: val loss 0.534511\n",
      "\n",
      "Epoch %d: train loss %f 197 0.2826134440573779\n",
      "Epoch 197: val loss 0.542166\n",
      "\n",
      "Epoch %d: train loss %f 198 0.30644646422429517\n",
      "Epoch 198: val loss 0.536005\n",
      "\n",
      "Epoch %d: train loss %f 199 0.2689782902598381\n",
      "Epoch 199: val loss 0.552464\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6901144764640115\n",
      "Epoch 0: val loss 0.691908\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6808930960568514\n",
      "Epoch 1: val loss 0.688848\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6696114431728016\n",
      "Epoch 2: val loss 0.682889\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6567052386023782\n",
      "Epoch 3: val loss 0.669859\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6384211670268666\n",
      "Epoch 4: val loss 0.650541\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6334011825648221\n",
      "Epoch 5: val loss 0.640252\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6142433285713196\n",
      "Epoch 6: val loss 0.624890\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5875978144732389\n",
      "Epoch 7: val loss 0.621501\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5709159699353304\n",
      "Epoch 8: val loss 0.616150\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5631899969144301\n",
      "Epoch 9: val loss 0.614493\n",
      "\n",
      "Epoch %d: train loss %f 10 0.549971035935662\n",
      "Epoch 10: val loss 0.610870\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5541180561889302\n",
      "Epoch 11: val loss 0.608531\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5355222116817128\n",
      "Epoch 12: val loss 0.600896\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5300999663092874\n",
      "Epoch 13: val loss 0.611248\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5325749137184836\n",
      "Epoch 14: val loss 0.598856\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5118275257674131\n",
      "Epoch 15: val loss 0.609040\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5259722579609264\n",
      "Epoch 16: val loss 0.599240\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5185495913028717\n",
      "Epoch 17: val loss 0.587757\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5062268267978322\n",
      "Epoch 18: val loss 0.604702\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5162937207655474\n",
      "Epoch 19: val loss 0.613618\n",
      "\n",
      "Epoch %d: train loss %f 20 0.49355811964381824\n",
      "Epoch 20: val loss 0.608994\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5153731487014077\n",
      "Epoch 21: val loss 0.616450\n",
      "\n",
      "Epoch %d: train loss %f 22 0.49783925305713306\n",
      "Epoch 22: val loss 0.617591\n",
      "\n",
      "Epoch %d: train loss %f 23 0.497152564200488\n",
      "Epoch 23: val loss 0.605800\n",
      "\n",
      "Epoch %d: train loss %f 24 0.4676412736827677\n",
      "Epoch 24: val loss 0.613454\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4883140352639285\n",
      "Epoch 25: val loss 0.603318\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4921480579809709\n",
      "Epoch 26: val loss 0.628266\n",
      "\n",
      "Epoch %d: train loss %f 27 0.49226911772381177\n",
      "Epoch 27: val loss 0.609070\n",
      "\n",
      "Epoch %d: train loss %f 28 0.47103051163933496\n",
      "Epoch 28: val loss 0.625320\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4800086427818645\n",
      "Epoch 29: val loss 0.597099\n",
      "\n",
      "Epoch %d: train loss %f 30 0.4984024708921259\n",
      "Epoch 30: val loss 0.604537\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4617424715648998\n",
      "Epoch 31: val loss 0.620539\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4427532472393729\n",
      "Epoch 32: val loss 0.577482\n",
      "\n",
      "Epoch %d: train loss %f 33 0.4494922391392968\n",
      "Epoch 33: val loss 0.610333\n",
      "\n",
      "Epoch %d: train loss %f 34 0.4381435513496399\n",
      "Epoch 34: val loss 0.592023\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4846082912249999\n",
      "Epoch 35: val loss 0.594737\n",
      "\n",
      "Epoch %d: train loss %f 36 0.44710706038908526\n",
      "Epoch 36: val loss 0.589685\n",
      "\n",
      "Epoch %d: train loss %f 37 0.46451031077991833\n",
      "Epoch 37: val loss 0.592452\n",
      "\n",
      "Epoch %d: train loss %f 38 0.43530501425266266\n",
      "Epoch 38: val loss 0.583275\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4470753751017831\n",
      "Epoch 39: val loss 0.625761\n",
      "\n",
      "Epoch %d: train loss %f 40 0.44245448437604035\n",
      "Epoch 40: val loss 0.598315\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4062823301011866\n",
      "Epoch 41: val loss 0.613144\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4096524959260767\n",
      "Epoch 42: val loss 0.606241\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4382809522477063\n",
      "Epoch 43: val loss 0.606972\n",
      "\n",
      "Epoch %d: train loss %f 44 0.415344083851034\n",
      "Epoch 44: val loss 0.612470\n",
      "\n",
      "Epoch %d: train loss %f 45 0.43673268231478607\n",
      "Epoch 45: val loss 0.620071\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4372425106438724\n",
      "Epoch 46: val loss 0.617692\n",
      "\n",
      "Epoch %d: train loss %f 47 0.41528773307800293\n",
      "Epoch 47: val loss 0.675540\n",
      "\n",
      "Epoch %d: train loss %f 48 0.4618013392795216\n",
      "Epoch 48: val loss 0.612611\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4100201292471452\n",
      "Epoch 49: val loss 0.600563\n",
      "\n",
      "Epoch %d: train loss %f 50 0.4325377914038571\n",
      "Epoch 50: val loss 0.623671\n",
      "\n",
      "Epoch %d: train loss %f 51 0.3641748360612176\n",
      "Epoch 51: val loss 0.631971\n",
      "\n",
      "Epoch %d: train loss %f 52 0.38268475099043414\n",
      "Epoch 52: val loss 0.658174\n",
      "\n",
      "Epoch %d: train loss %f 53 0.4244358078999953\n",
      "Epoch 53: val loss 0.614130\n",
      "\n",
      "Epoch %d: train loss %f 54 0.41182204810055817\n",
      "Epoch 54: val loss 0.605227\n",
      "\n",
      "Epoch %d: train loss %f 55 0.42736431008035486\n",
      "Epoch 55: val loss 0.598147\n",
      "\n",
      "Epoch %d: train loss %f 56 0.41462267528880725\n",
      "Epoch 56: val loss 0.653654\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4099689342758872\n",
      "Epoch 57: val loss 0.629783\n",
      "\n",
      "Epoch %d: train loss %f 58 0.43502255190502515\n",
      "Epoch 58: val loss 0.609695\n",
      "\n",
      "Epoch %d: train loss %f 59 0.401923806829886\n",
      "Epoch 59: val loss 0.635997\n",
      "\n",
      "Epoch %d: train loss %f 60 0.3797936656258323\n",
      "Epoch 60: val loss 0.609338\n",
      "\n",
      "Epoch %d: train loss %f 61 0.4075264578515833\n",
      "Epoch 61: val loss 0.620843\n",
      "\n",
      "Epoch %d: train loss %f 62 0.38176177035678516\n",
      "Epoch 62: val loss 0.615304\n",
      "\n",
      "Epoch %d: train loss %f 63 0.3655547946691513\n",
      "Epoch 63: val loss 0.661262\n",
      "\n",
      "Epoch %d: train loss %f 64 0.4121698249470104\n",
      "Epoch 64: val loss 0.635578\n",
      "\n",
      "Epoch %d: train loss %f 65 0.3873518705368042\n",
      "Epoch 65: val loss 0.628695\n",
      "\n",
      "Epoch %d: train loss %f 66 0.3804315518249165\n",
      "Epoch 66: val loss 0.624107\n",
      "\n",
      "Epoch %d: train loss %f 67 0.40721004524014215\n",
      "Epoch 67: val loss 0.632659\n",
      "\n",
      "Epoch %d: train loss %f 68 0.3993159275163304\n",
      "Epoch 68: val loss 0.652212\n",
      "\n",
      "Epoch %d: train loss %f 69 0.3856863704594699\n",
      "Epoch 69: val loss 0.631835\n",
      "\n",
      "Epoch %d: train loss %f 70 0.3542458035729148\n",
      "Epoch 70: val loss 0.662234\n",
      "\n",
      "Epoch %d: train loss %f 71 0.395704521374269\n",
      "Epoch 71: val loss 0.605002\n",
      "\n",
      "Epoch %d: train loss %f 72 0.4255967817523263\n",
      "Epoch 72: val loss 0.600979\n",
      "\n",
      "Epoch %d: train loss %f 73 0.37081679566340014\n",
      "Epoch 73: val loss 0.617244\n",
      "\n",
      "Epoch %d: train loss %f 74 0.36430737240747973\n",
      "Epoch 74: val loss 0.656947\n",
      "\n",
      "Epoch %d: train loss %f 75 0.40601111271164636\n",
      "Epoch 75: val loss 0.624403\n",
      "\n",
      "Epoch %d: train loss %f 76 0.36095715923742816\n",
      "Epoch 76: val loss 0.617107\n",
      "\n",
      "Epoch %d: train loss %f 77 0.37053521925752814\n",
      "Epoch 77: val loss 0.624447\n",
      "\n",
      "Epoch %d: train loss %f 78 0.3898469182578\n",
      "Epoch 78: val loss 0.599260\n",
      "\n",
      "Epoch %d: train loss %f 79 0.3814094662666321\n",
      "Epoch 79: val loss 0.605822\n",
      "\n",
      "Epoch %d: train loss %f 80 0.39083014022220264\n",
      "Epoch 80: val loss 0.632234\n",
      "\n",
      "Epoch %d: train loss %f 81 0.35443968258120795\n",
      "Epoch 81: val loss 0.624374\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3463691527193243\n",
      "Epoch 82: val loss 0.635391\n",
      "\n",
      "Epoch %d: train loss %f 83 0.3758169575171037\n",
      "Epoch 83: val loss 0.621281\n",
      "\n",
      "Epoch %d: train loss %f 84 0.37887071750380774\n",
      "Epoch 84: val loss 0.605799\n",
      "\n",
      "Epoch %d: train loss %f 85 0.34456983479586517\n",
      "Epoch 85: val loss 0.686703\n",
      "\n",
      "Epoch %d: train loss %f 86 0.39258454604582355\n",
      "Epoch 86: val loss 0.660582\n",
      "\n",
      "Epoch %d: train loss %f 87 0.35547367144714703\n",
      "Epoch 87: val loss 0.619274\n",
      "\n",
      "Epoch %d: train loss %f 88 0.360386769879948\n",
      "Epoch 88: val loss 0.652846\n",
      "\n",
      "Epoch %d: train loss %f 89 0.4023301899433136\n",
      "Epoch 89: val loss 0.612685\n",
      "\n",
      "Epoch %d: train loss %f 90 0.3403037203983827\n",
      "Epoch 90: val loss 0.604341\n",
      "\n",
      "Epoch %d: train loss %f 91 0.3671591579914093\n",
      "Epoch 91: val loss 0.673037\n",
      "\n",
      "Epoch %d: train loss %f 92 0.36022955585609784\n",
      "Epoch 92: val loss 0.650865\n",
      "\n",
      "Epoch %d: train loss %f 93 0.35433598540045996\n",
      "Epoch 93: val loss 0.622892\n",
      "\n",
      "Epoch %d: train loss %f 94 0.3556993549520319\n",
      "Epoch 94: val loss 0.686628\n",
      "\n",
      "Epoch %d: train loss %f 95 0.3257001868703149\n",
      "Epoch 95: val loss 0.671992\n",
      "\n",
      "Epoch %d: train loss %f 96 0.36995086344805633\n",
      "Epoch 96: val loss 0.619312\n",
      "\n",
      "Epoch %d: train loss %f 97 0.35165591402487323\n",
      "Epoch 97: val loss 0.720874\n",
      "\n",
      "Epoch %d: train loss %f 98 0.36118643798611383\n",
      "Epoch 98: val loss 0.582856\n",
      "\n",
      "Epoch %d: train loss %f 99 0.350350868972865\n",
      "Epoch 99: val loss 0.647915\n",
      "\n",
      "Epoch %d: train loss %f 100 0.40140319954265247\n",
      "Epoch 100: val loss 0.598763\n",
      "\n",
      "Epoch %d: train loss %f 101 0.37339936061338946\n",
      "Epoch 101: val loss 0.580824\n",
      "\n",
      "Epoch %d: train loss %f 102 0.3102997189218348\n",
      "Epoch 102: val loss 0.659680\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3333835100585764\n",
      "Epoch 103: val loss 0.628977\n",
      "\n",
      "Epoch %d: train loss %f 104 0.33757352558049286\n",
      "Epoch 104: val loss 0.629848\n",
      "\n",
      "Epoch %d: train loss %f 105 0.34305222061547364\n",
      "Epoch 105: val loss 0.644441\n",
      "\n",
      "Epoch %d: train loss %f 106 0.33014525744048034\n",
      "Epoch 106: val loss 0.648914\n",
      "\n",
      "Epoch %d: train loss %f 107 0.3509066524830731\n",
      "Epoch 107: val loss 0.689200\n",
      "\n",
      "Epoch %d: train loss %f 108 0.3469374450770291\n",
      "Epoch 108: val loss 0.645578\n",
      "\n",
      "Epoch %d: train loss %f 109 0.3698181537064639\n",
      "Epoch 109: val loss 0.656959\n",
      "\n",
      "Epoch %d: train loss %f 110 0.33603436838496814\n",
      "Epoch 110: val loss 0.650555\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3048850270834836\n",
      "Epoch 111: val loss 0.648549\n",
      "\n",
      "Epoch %d: train loss %f 112 0.3983887312087146\n",
      "Epoch 112: val loss 0.632395\n",
      "\n",
      "Epoch %d: train loss %f 113 0.3082542256875472\n",
      "Epoch 113: val loss 0.605184\n",
      "\n",
      "Epoch %d: train loss %f 114 0.35626414960080927\n",
      "Epoch 114: val loss 0.625241\n",
      "\n",
      "Epoch %d: train loss %f 115 0.3109450868584893\n",
      "Epoch 115: val loss 0.625873\n",
      "\n",
      "Epoch %d: train loss %f 116 0.32087779857895593\n",
      "Epoch 116: val loss 0.623331\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3161954757842151\n",
      "Epoch 117: val loss 0.650458\n",
      "\n",
      "Epoch %d: train loss %f 118 0.3339942829175429\n",
      "Epoch 118: val loss 0.612425\n",
      "\n",
      "Epoch %d: train loss %f 119 0.36650450121272693\n",
      "Epoch 119: val loss 0.676137\n",
      "\n",
      "Epoch %d: train loss %f 120 0.31454462761228735\n",
      "Epoch 120: val loss 0.633349\n",
      "\n",
      "Epoch %d: train loss %f 121 0.3219535865566947\n",
      "Epoch 121: val loss 0.633343\n",
      "\n",
      "Epoch %d: train loss %f 122 0.2967291763899001\n",
      "Epoch 122: val loss 0.675593\n",
      "\n",
      "Epoch %d: train loss %f 123 0.3233524737032977\n",
      "Epoch 123: val loss 0.668552\n",
      "\n",
      "Epoch %d: train loss %f 124 0.35290557958863\n",
      "Epoch 124: val loss 0.674450\n",
      "\n",
      "Epoch %d: train loss %f 125 0.33161469210277905\n",
      "Epoch 125: val loss 0.681428\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3050212101502852\n",
      "Epoch 126: val loss 0.658947\n",
      "\n",
      "Epoch %d: train loss %f 127 0.3257010199806907\n",
      "Epoch 127: val loss 0.635692\n",
      "\n",
      "Epoch %d: train loss %f 128 0.28335228291424835\n",
      "Epoch 128: val loss 0.632597\n",
      "\n",
      "Epoch %d: train loss %f 129 0.333640919490294\n",
      "Epoch 129: val loss 0.693157\n",
      "\n",
      "Epoch %d: train loss %f 130 0.3082662278955633\n",
      "Epoch 130: val loss 0.653449\n",
      "\n",
      "Epoch %d: train loss %f 131 0.3095277398824692\n",
      "Epoch 131: val loss 0.764549\n",
      "\n",
      "Epoch %d: train loss %f 132 0.32797407562082465\n",
      "Epoch 132: val loss 0.655521\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3112601488828659\n",
      "Epoch 133: val loss 0.665800\n",
      "\n",
      "Epoch %d: train loss %f 134 0.31656157970428467\n",
      "Epoch 134: val loss 0.663762\n",
      "\n",
      "Epoch %d: train loss %f 135 0.272114404223182\n",
      "Epoch 135: val loss 0.699808\n",
      "\n",
      "Epoch %d: train loss %f 136 0.34630095823244617\n",
      "Epoch 136: val loss 0.664588\n",
      "\n",
      "Epoch %d: train loss %f 137 0.28445828367363324\n",
      "Epoch 137: val loss 0.709107\n",
      "\n",
      "Epoch %d: train loss %f 138 0.27580892226912757\n",
      "Epoch 138: val loss 0.691660\n",
      "\n",
      "Epoch %d: train loss %f 139 0.3299023156816309\n",
      "Epoch 139: val loss 0.708960\n",
      "\n",
      "Epoch %d: train loss %f 140 0.30495652691884473\n",
      "Epoch 140: val loss 0.720381\n",
      "\n",
      "Epoch %d: train loss %f 141 0.3013585250486027\n",
      "Epoch 141: val loss 0.706881\n",
      "\n",
      "Epoch %d: train loss %f 142 0.28438459404490213\n",
      "Epoch 142: val loss 0.693991\n",
      "\n",
      "Epoch %d: train loss %f 143 0.2767909074371511\n",
      "Epoch 143: val loss 0.698494\n",
      "\n",
      "Epoch %d: train loss %f 144 0.31049478121779184\n",
      "Epoch 144: val loss 0.728200\n",
      "\n",
      "Epoch %d: train loss %f 145 0.30134049464355817\n",
      "Epoch 145: val loss 0.705440\n",
      "\n",
      "Epoch %d: train loss %f 146 0.2615163732658733\n",
      "Epoch 146: val loss 0.673138\n",
      "\n",
      "Epoch %d: train loss %f 147 0.269647247411988\n",
      "Epoch 147: val loss 0.712179\n",
      "\n",
      "Epoch %d: train loss %f 148 0.26874900406057184\n",
      "Epoch 148: val loss 0.667285\n",
      "\n",
      "Epoch %d: train loss %f 149 0.2838292717933655\n",
      "Epoch 149: val loss 0.629688\n",
      "\n",
      "Epoch %d: train loss %f 150 0.23949106579477136\n",
      "Epoch 150: val loss 0.696071\n",
      "\n",
      "Epoch %d: train loss %f 151 0.29413860765370453\n",
      "Epoch 151: val loss 0.698324\n",
      "\n",
      "Epoch %d: train loss %f 152 0.28559674051674927\n",
      "Epoch 152: val loss 0.710398\n",
      "\n",
      "Epoch %d: train loss %f 153 0.30696024948900397\n",
      "Epoch 153: val loss 0.642063\n",
      "\n",
      "Epoch %d: train loss %f 154 0.2814525867050344\n",
      "Epoch 154: val loss 0.669277\n",
      "\n",
      "Epoch %d: train loss %f 155 0.32516471770676697\n",
      "Epoch 155: val loss 0.657957\n",
      "\n",
      "Epoch %d: train loss %f 156 0.2642234374176372\n",
      "Epoch 156: val loss 0.655971\n",
      "\n",
      "Epoch %d: train loss %f 157 0.26143019985068927\n",
      "Epoch 157: val loss 0.658858\n",
      "\n",
      "Epoch %d: train loss %f 158 0.25619754737073724\n",
      "Epoch 158: val loss 0.697003\n",
      "\n",
      "Epoch %d: train loss %f 159 0.2946316030892459\n",
      "Epoch 159: val loss 0.767752\n",
      "\n",
      "Epoch %d: train loss %f 160 0.2419768517667597\n",
      "Epoch 160: val loss 0.707833\n",
      "\n",
      "Epoch %d: train loss %f 161 0.2903584655035626\n",
      "Epoch 161: val loss 0.773101\n",
      "\n",
      "Epoch %d: train loss %f 162 0.26483121514320374\n",
      "Epoch 162: val loss 0.657820\n",
      "\n",
      "Epoch %d: train loss %f 163 0.32875927740877325\n",
      "Epoch 163: val loss 0.804737\n",
      "\n",
      "Epoch %d: train loss %f 164 0.28398597646843304\n",
      "Epoch 164: val loss 0.715458\n",
      "\n",
      "Epoch %d: train loss %f 165 0.2990733168341897\n",
      "Epoch 165: val loss 0.801411\n",
      "\n",
      "Epoch %d: train loss %f 166 0.31016497991301795\n",
      "Epoch 166: val loss 0.687382\n",
      "\n",
      "Epoch %d: train loss %f 167 0.27959040145982395\n",
      "Epoch 167: val loss 0.727908\n",
      "\n",
      "Epoch %d: train loss %f 168 0.3356759683652358\n",
      "Epoch 168: val loss 0.707886\n",
      "\n",
      "Epoch %d: train loss %f 169 0.33582259850068524\n",
      "Epoch 169: val loss 0.707250\n",
      "\n",
      "Epoch %d: train loss %f 170 0.28521249375560065\n",
      "Epoch 170: val loss 0.695018\n",
      "\n",
      "Epoch %d: train loss %f 171 0.25716605850241403\n",
      "Epoch 171: val loss 0.746871\n",
      "\n",
      "Epoch %d: train loss %f 172 0.29580955884673377\n",
      "Epoch 172: val loss 0.677858\n",
      "\n",
      "Epoch %d: train loss %f 173 0.303377548402006\n",
      "Epoch 173: val loss 0.697366\n",
      "\n",
      "Epoch %d: train loss %f 174 0.2743583091280677\n",
      "Epoch 174: val loss 0.612641\n",
      "\n",
      "Epoch %d: train loss %f 175 0.31590416756543244\n",
      "Epoch 175: val loss 0.744643\n",
      "\n",
      "Epoch %d: train loss %f 176 0.25346673212268134\n",
      "Epoch 176: val loss 0.671018\n",
      "\n",
      "Epoch %d: train loss %f 177 0.32827704738486896\n",
      "Epoch 177: val loss 0.698065\n",
      "\n",
      "Epoch %d: train loss %f 178 0.2879317104816437\n",
      "Epoch 178: val loss 0.680907\n",
      "\n",
      "Epoch %d: train loss %f 179 0.2613177570429715\n",
      "Epoch 179: val loss 0.732290\n",
      "\n",
      "Epoch %d: train loss %f 180 0.25825097208673303\n",
      "Epoch 180: val loss 0.677741\n",
      "\n",
      "Epoch %d: train loss %f 181 0.3007486963813955\n",
      "Epoch 181: val loss 0.679499\n",
      "\n",
      "Epoch %d: train loss %f 182 0.25431678782809863\n",
      "Epoch 182: val loss 0.655460\n",
      "\n",
      "Epoch %d: train loss %f 183 0.25097235359928827\n",
      "Epoch 183: val loss 0.683183\n",
      "\n",
      "Epoch %d: train loss %f 184 0.2789471217177131\n",
      "Epoch 184: val loss 0.734341\n",
      "\n",
      "Epoch %d: train loss %f 185 0.27085699276490643\n",
      "Epoch 185: val loss 0.693983\n",
      "\n",
      "Epoch %d: train loss %f 186 0.20783623511140997\n",
      "Epoch 186: val loss 0.695607\n",
      "\n",
      "Epoch %d: train loss %f 187 0.2245562801306898\n",
      "Epoch 187: val loss 0.653913\n",
      "\n",
      "Epoch %d: train loss %f 188 0.2475339729677547\n",
      "Epoch 188: val loss 0.723251\n",
      "\n",
      "Epoch %d: train loss %f 189 0.2670741216702895\n",
      "Epoch 189: val loss 0.705834\n",
      "\n",
      "Epoch %d: train loss %f 190 0.26676702770319854\n",
      "Epoch 190: val loss 0.689667\n",
      "\n",
      "Epoch %d: train loss %f 191 0.23015488819642502\n",
      "Epoch 191: val loss 0.677768\n",
      "\n",
      "Epoch %d: train loss %f 192 0.2489738870750774\n",
      "Epoch 192: val loss 0.701804\n",
      "\n",
      "Epoch %d: train loss %f 193 0.2686711515892636\n",
      "Epoch 193: val loss 0.714590\n",
      "\n",
      "Epoch %d: train loss %f 194 0.25169039449908515\n",
      "Epoch 194: val loss 0.731043\n",
      "\n",
      "Epoch %d: train loss %f 195 0.2344197007742795\n",
      "Epoch 195: val loss 0.693473\n",
      "\n",
      "Epoch %d: train loss %f 196 0.23648320070721887\n",
      "Epoch 196: val loss 0.711449\n",
      "\n",
      "Epoch %d: train loss %f 197 0.2659692845561288\n",
      "Epoch 197: val loss 0.736501\n",
      "\n",
      "Epoch %d: train loss %f 198 0.2479465515776114\n",
      "Epoch 198: val loss 0.744124\n",
      "\n",
      "Epoch %d: train loss %f 199 0.22178375314582477\n",
      "Epoch 199: val loss 0.741111\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6901747073446002\n",
      "Epoch 0: val loss 0.691469\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6833330052239555\n",
      "Epoch 1: val loss 0.686611\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6677424481936863\n",
      "Epoch 2: val loss 0.671952\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6475235990115574\n",
      "Epoch 3: val loss 0.651132\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6238713647638049\n",
      "Epoch 4: val loss 0.634004\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6028647209916796\n",
      "Epoch 5: val loss 0.626435\n",
      "\n",
      "Epoch %d: train loss %f 6 0.5787638127803802\n",
      "Epoch 6: val loss 0.610152\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5701808312109539\n",
      "Epoch 7: val loss 0.627657\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5459092834166118\n",
      "Epoch 8: val loss 0.604106\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5449346772262028\n",
      "Epoch 9: val loss 0.612831\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5208322086504528\n",
      "Epoch 10: val loss 0.621466\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5251179316214153\n",
      "Epoch 11: val loss 0.601196\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5248481844152723\n",
      "Epoch 12: val loss 0.601706\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5229672087090356\n",
      "Epoch 13: val loss 0.597556\n",
      "\n",
      "Epoch %d: train loss %f 14 0.515356353351048\n",
      "Epoch 14: val loss 0.610845\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5008497834205627\n",
      "Epoch 15: val loss 0.590655\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5018148124217987\n",
      "Epoch 16: val loss 0.593468\n",
      "\n",
      "Epoch %d: train loss %f 17 0.49728608557156156\n",
      "Epoch 17: val loss 0.589523\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5065410073314395\n",
      "Epoch 18: val loss 0.585395\n",
      "\n",
      "Epoch %d: train loss %f 19 0.4910540559462139\n",
      "Epoch 19: val loss 0.575986\n",
      "\n",
      "Epoch %d: train loss %f 20 0.48307581671646665\n",
      "Epoch 20: val loss 0.593703\n",
      "\n",
      "Epoch %d: train loss %f 21 0.48487315646239687\n",
      "Epoch 21: val loss 0.577956\n",
      "\n",
      "Epoch %d: train loss %f 22 0.4754116045577185\n",
      "Epoch 22: val loss 0.591532\n",
      "\n",
      "Epoch %d: train loss %f 23 0.4769511009965624\n",
      "Epoch 23: val loss 0.566586\n",
      "\n",
      "Epoch %d: train loss %f 24 0.49546979793480467\n",
      "Epoch 24: val loss 0.572737\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4671350504670824\n",
      "Epoch 25: val loss 0.581205\n",
      "\n",
      "Epoch %d: train loss %f 26 0.46234738401004244\n",
      "Epoch 26: val loss 0.559093\n",
      "\n",
      "Epoch %d: train loss %f 27 0.4932295616183962\n",
      "Epoch 27: val loss 0.594726\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4620422508035387\n",
      "Epoch 28: val loss 0.565728\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4600641791309629\n",
      "Epoch 29: val loss 0.572979\n",
      "\n",
      "Epoch %d: train loss %f 30 0.44487880383219036\n",
      "Epoch 30: val loss 0.584652\n",
      "\n",
      "Epoch %d: train loss %f 31 0.46153822754110607\n",
      "Epoch 31: val loss 0.592080\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4607255756855011\n",
      "Epoch 32: val loss 0.586339\n",
      "\n",
      "Epoch %d: train loss %f 33 0.43956262724740164\n",
      "Epoch 33: val loss 0.596528\n",
      "\n",
      "Epoch %d: train loss %f 34 0.4605275584118707\n",
      "Epoch 34: val loss 0.581993\n",
      "\n",
      "Epoch %d: train loss %f 35 0.46057400746004923\n",
      "Epoch 35: val loss 0.567175\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4205295060362135\n",
      "Epoch 36: val loss 0.572477\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4204764323575156\n",
      "Epoch 37: val loss 0.569929\n",
      "\n",
      "Epoch %d: train loss %f 38 0.44283989710467203\n",
      "Epoch 38: val loss 0.577813\n",
      "\n",
      "Epoch %d: train loss %f 39 0.42897902003356386\n",
      "Epoch 39: val loss 0.575381\n",
      "\n",
      "Epoch %d: train loss %f 40 0.43679011506693705\n",
      "Epoch 40: val loss 0.558575\n",
      "\n",
      "Epoch %d: train loss %f 41 0.41545278685433523\n",
      "Epoch 41: val loss 0.559481\n",
      "\n",
      "Epoch %d: train loss %f 42 0.42377595603466034\n",
      "Epoch 42: val loss 0.558364\n",
      "\n",
      "Epoch %d: train loss %f 43 0.402271317584174\n",
      "Epoch 43: val loss 0.595926\n",
      "\n",
      "Epoch %d: train loss %f 44 0.40747684027467457\n",
      "Epoch 44: val loss 0.588373\n",
      "\n",
      "Epoch %d: train loss %f 45 0.39877439183848246\n",
      "Epoch 45: val loss 0.600324\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4154961864863123\n",
      "Epoch 46: val loss 0.577819\n",
      "\n",
      "Epoch %d: train loss %f 47 0.3871496000460216\n",
      "Epoch 47: val loss 0.564483\n",
      "\n",
      "Epoch %d: train loss %f 48 0.39261399635246824\n",
      "Epoch 48: val loss 0.610504\n",
      "\n",
      "Epoch %d: train loss %f 49 0.41806477521147045\n",
      "Epoch 49: val loss 0.564323\n",
      "\n",
      "Epoch %d: train loss %f 50 0.3799738862684795\n",
      "Epoch 50: val loss 0.620264\n",
      "\n",
      "Epoch %d: train loss %f 51 0.394901237317494\n",
      "Epoch 51: val loss 0.583287\n",
      "\n",
      "Epoch %d: train loss %f 52 0.39650560702596394\n",
      "Epoch 52: val loss 0.578029\n",
      "\n",
      "Epoch %d: train loss %f 53 0.38375514639275415\n",
      "Epoch 53: val loss 0.562652\n",
      "\n",
      "Epoch %d: train loss %f 54 0.39379921342645374\n",
      "Epoch 54: val loss 0.561852\n",
      "\n",
      "Epoch %d: train loss %f 55 0.4081461578607559\n",
      "Epoch 55: val loss 0.579013\n",
      "\n",
      "Epoch %d: train loss %f 56 0.3884084565298898\n",
      "Epoch 56: val loss 0.557978\n",
      "\n",
      "Epoch %d: train loss %f 57 0.3868572541645595\n",
      "Epoch 57: val loss 0.568596\n",
      "\n",
      "Epoch %d: train loss %f 58 0.35944936105183195\n",
      "Epoch 58: val loss 0.590858\n",
      "\n",
      "Epoch %d: train loss %f 59 0.3744460216590336\n",
      "Epoch 59: val loss 0.588891\n",
      "\n",
      "Epoch %d: train loss %f 60 0.3534704776746886\n",
      "Epoch 60: val loss 0.535772\n",
      "\n",
      "Epoch %d: train loss %f 61 0.38195740644420895\n",
      "Epoch 61: val loss 0.564128\n",
      "\n",
      "Epoch %d: train loss %f 62 0.34662469370024546\n",
      "Epoch 62: val loss 0.552471\n",
      "\n",
      "Epoch %d: train loss %f 63 0.3628265453236444\n",
      "Epoch 63: val loss 0.632459\n",
      "\n",
      "Epoch %d: train loss %f 64 0.3812874044690813\n",
      "Epoch 64: val loss 0.557051\n",
      "\n",
      "Epoch %d: train loss %f 65 0.36493709470544544\n",
      "Epoch 65: val loss 0.570789\n",
      "\n",
      "Epoch %d: train loss %f 66 0.3585939130612782\n",
      "Epoch 66: val loss 0.579922\n",
      "\n",
      "Epoch %d: train loss %f 67 0.3641358071139881\n",
      "Epoch 67: val loss 0.603884\n",
      "\n",
      "Epoch %d: train loss %f 68 0.3566605161343302\n",
      "Epoch 68: val loss 0.616204\n",
      "\n",
      "Epoch %d: train loss %f 69 0.3334213356886591\n",
      "Epoch 69: val loss 0.556887\n",
      "\n",
      "Epoch %d: train loss %f 70 0.35105657896825243\n",
      "Epoch 70: val loss 0.548855\n",
      "\n",
      "Epoch %d: train loss %f 71 0.3555142326014383\n",
      "Epoch 71: val loss 0.584804\n",
      "\n",
      "Epoch %d: train loss %f 72 0.3286530449986458\n",
      "Epoch 72: val loss 0.570700\n",
      "\n",
      "Epoch %d: train loss %f 73 0.32586086434977396\n",
      "Epoch 73: val loss 0.576556\n",
      "\n",
      "Epoch %d: train loss %f 74 0.33534309906618937\n",
      "Epoch 74: val loss 0.555202\n",
      "\n",
      "Epoch %d: train loss %f 75 0.38775043402399334\n",
      "Epoch 75: val loss 0.574329\n",
      "\n",
      "Epoch %d: train loss %f 76 0.36880477730716976\n",
      "Epoch 76: val loss 0.544802\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3411788525325911\n",
      "Epoch 77: val loss 0.543276\n",
      "\n",
      "Epoch %d: train loss %f 78 0.31960369859422955\n",
      "Epoch 78: val loss 0.577214\n",
      "\n",
      "Epoch %d: train loss %f 79 0.34775930643081665\n",
      "Epoch 79: val loss 0.572188\n",
      "\n",
      "Epoch %d: train loss %f 80 0.3485232623560088\n",
      "Epoch 80: val loss 0.542800\n",
      "\n",
      "Epoch %d: train loss %f 81 0.30618480486529215\n",
      "Epoch 81: val loss 0.594430\n",
      "\n",
      "Epoch %d: train loss %f 82 0.33688794715063913\n",
      "Epoch 82: val loss 0.582697\n",
      "\n",
      "Epoch %d: train loss %f 83 0.32197326315300806\n",
      "Epoch 83: val loss 0.597435\n",
      "\n",
      "Epoch %d: train loss %f 84 0.33788263372012545\n",
      "Epoch 84: val loss 0.545745\n",
      "\n",
      "Epoch %d: train loss %f 85 0.3318479603954724\n",
      "Epoch 85: val loss 0.598383\n",
      "\n",
      "Epoch %d: train loss %f 86 0.31844777507441385\n",
      "Epoch 86: val loss 0.529135\n",
      "\n",
      "Epoch %d: train loss %f 87 0.32629365580422537\n",
      "Epoch 87: val loss 0.554129\n",
      "\n",
      "Epoch %d: train loss %f 88 0.31996120512485504\n",
      "Epoch 88: val loss 0.522704\n",
      "\n",
      "Epoch %d: train loss %f 89 0.32167221126811846\n",
      "Epoch 89: val loss 0.536205\n",
      "\n",
      "Epoch %d: train loss %f 90 0.31451151413576944\n",
      "Epoch 90: val loss 0.526732\n",
      "\n",
      "Epoch %d: train loss %f 91 0.3136202852640833\n",
      "Epoch 91: val loss 0.541336\n",
      "\n",
      "Epoch %d: train loss %f 92 0.30403077602386475\n",
      "Epoch 92: val loss 0.590342\n",
      "\n",
      "Epoch %d: train loss %f 93 0.29557765807424274\n",
      "Epoch 93: val loss 0.552693\n",
      "\n",
      "Epoch %d: train loss %f 94 0.32856624679906027\n",
      "Epoch 94: val loss 0.593881\n",
      "\n",
      "Epoch %d: train loss %f 95 0.3012136657323156\n",
      "Epoch 95: val loss 0.558099\n",
      "\n",
      "Epoch %d: train loss %f 96 0.30322632672531263\n",
      "Epoch 96: val loss 0.585918\n",
      "\n",
      "Epoch %d: train loss %f 97 0.2852969297340938\n",
      "Epoch 97: val loss 0.566027\n",
      "\n",
      "Epoch %d: train loss %f 98 0.2797497862151691\n",
      "Epoch 98: val loss 0.542136\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3161207139492035\n",
      "Epoch 99: val loss 0.517643\n",
      "\n",
      "Epoch %d: train loss %f 100 0.301225839981011\n",
      "Epoch 100: val loss 0.559695\n",
      "\n",
      "Epoch %d: train loss %f 101 0.2761594983083861\n",
      "Epoch 101: val loss 0.535341\n",
      "\n",
      "Epoch %d: train loss %f 102 0.3184957749077252\n",
      "Epoch 102: val loss 0.570001\n",
      "\n",
      "Epoch %d: train loss %f 103 0.2838892074567931\n",
      "Epoch 103: val loss 0.591617\n",
      "\n",
      "Epoch %d: train loss %f 104 0.31311829281704767\n",
      "Epoch 104: val loss 0.593529\n",
      "\n",
      "Epoch %d: train loss %f 105 0.33284796455076765\n",
      "Epoch 105: val loss 0.544762\n",
      "\n",
      "Epoch %d: train loss %f 106 0.3052183432238443\n",
      "Epoch 106: val loss 0.563049\n",
      "\n",
      "Epoch %d: train loss %f 107 0.26198323709624155\n",
      "Epoch 107: val loss 0.579429\n",
      "\n",
      "Epoch %d: train loss %f 108 0.3230496362916061\n",
      "Epoch 108: val loss 0.549537\n",
      "\n",
      "Epoch %d: train loss %f 109 0.311613991856575\n",
      "Epoch 109: val loss 0.553220\n",
      "\n",
      "Epoch %d: train loss %f 110 0.3253791800567082\n",
      "Epoch 110: val loss 0.585757\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3286867471677916\n",
      "Epoch 111: val loss 0.572809\n",
      "\n",
      "Epoch %d: train loss %f 112 0.34225786477327347\n",
      "Epoch 112: val loss 0.562996\n",
      "\n",
      "Epoch %d: train loss %f 113 0.30769488854067667\n",
      "Epoch 113: val loss 0.604184\n",
      "\n",
      "Epoch %d: train loss %f 114 0.26167778724006246\n",
      "Epoch 114: val loss 0.560339\n",
      "\n",
      "Epoch %d: train loss %f 115 0.29333391998495373\n",
      "Epoch 115: val loss 0.547877\n",
      "\n",
      "Epoch %d: train loss %f 116 0.30953807490212576\n",
      "Epoch 116: val loss 0.538891\n",
      "\n",
      "Epoch %d: train loss %f 117 0.2821114936045238\n",
      "Epoch 117: val loss 0.580485\n",
      "\n",
      "Epoch %d: train loss %f 118 0.2730399285043989\n",
      "Epoch 118: val loss 0.564942\n",
      "\n",
      "Epoch %d: train loss %f 119 0.2627573258110455\n",
      "Epoch 119: val loss 0.545962\n",
      "\n",
      "Epoch %d: train loss %f 120 0.2828297753419195\n",
      "Epoch 120: val loss 0.552858\n",
      "\n",
      "Epoch %d: train loss %f 121 0.27784310813461033\n",
      "Epoch 121: val loss 0.531644\n",
      "\n",
      "Epoch %d: train loss %f 122 0.26956250624997274\n",
      "Epoch 122: val loss 0.547425\n",
      "\n",
      "Epoch %d: train loss %f 123 0.2706075459718704\n",
      "Epoch 123: val loss 0.582808\n",
      "\n",
      "Epoch %d: train loss %f 124 0.27981824800372124\n",
      "Epoch 124: val loss 0.564109\n",
      "\n",
      "Epoch %d: train loss %f 125 0.31297234339373453\n",
      "Epoch 125: val loss 0.540932\n",
      "\n",
      "Epoch %d: train loss %f 126 0.2869336200611932\n",
      "Epoch 126: val loss 0.538265\n",
      "\n",
      "Epoch %d: train loss %f 127 0.27279746319566456\n",
      "Epoch 127: val loss 0.570207\n",
      "\n",
      "Epoch %d: train loss %f 128 0.25848523314510075\n",
      "Epoch 128: val loss 0.563146\n",
      "\n",
      "Epoch %d: train loss %f 129 0.27993571758270264\n",
      "Epoch 129: val loss 0.595534\n",
      "\n",
      "Epoch %d: train loss %f 130 0.275154852441379\n",
      "Epoch 130: val loss 0.581873\n",
      "\n",
      "Epoch %d: train loss %f 131 0.24394717918975012\n",
      "Epoch 131: val loss 0.557492\n",
      "\n",
      "Epoch %d: train loss %f 132 0.31876442368541447\n",
      "Epoch 132: val loss 0.539957\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3119155487843922\n",
      "Epoch 133: val loss 0.604593\n",
      "\n",
      "Epoch %d: train loss %f 134 0.2817752329366548\n",
      "Epoch 134: val loss 0.561242\n",
      "\n",
      "Epoch %d: train loss %f 135 0.24449420241372927\n",
      "Epoch 135: val loss 0.557923\n",
      "\n",
      "Epoch %d: train loss %f 136 0.27449561123337063\n",
      "Epoch 136: val loss 0.556349\n",
      "\n",
      "Epoch %d: train loss %f 137 0.2588653564453125\n",
      "Epoch 137: val loss 0.571411\n",
      "\n",
      "Epoch %d: train loss %f 138 0.275315451834883\n",
      "Epoch 138: val loss 0.583451\n",
      "\n",
      "Epoch %d: train loss %f 139 0.26257214163030895\n",
      "Epoch 139: val loss 0.575131\n",
      "\n",
      "Epoch %d: train loss %f 140 0.2668285183608532\n",
      "Epoch 140: val loss 0.572771\n",
      "\n",
      "Epoch %d: train loss %f 141 0.26761756784149576\n",
      "Epoch 141: val loss 0.555376\n",
      "\n",
      "Epoch %d: train loss %f 142 0.2744783620749201\n",
      "Epoch 142: val loss 0.603512\n",
      "\n",
      "Epoch %d: train loss %f 143 0.27844082564115524\n",
      "Epoch 143: val loss 0.597217\n",
      "\n",
      "Epoch %d: train loss %f 144 0.2601798634443964\n",
      "Epoch 144: val loss 0.560384\n",
      "\n",
      "Epoch %d: train loss %f 145 0.2838012384516852\n",
      "Epoch 145: val loss 0.557284\n",
      "\n",
      "Epoch %d: train loss %f 146 0.2524429698075567\n",
      "Epoch 146: val loss 0.600003\n",
      "\n",
      "Epoch %d: train loss %f 147 0.2542063796094486\n",
      "Epoch 147: val loss 0.603760\n",
      "\n",
      "Epoch %d: train loss %f 148 0.26270880709801403\n",
      "Epoch 148: val loss 0.579082\n",
      "\n",
      "Epoch %d: train loss %f 149 0.25205737139497486\n",
      "Epoch 149: val loss 0.551242\n",
      "\n",
      "Epoch %d: train loss %f 150 0.28150544315576553\n",
      "Epoch 150: val loss 0.541781\n",
      "\n",
      "Epoch %d: train loss %f 151 0.24499335139989853\n",
      "Epoch 151: val loss 0.548209\n",
      "\n",
      "Epoch %d: train loss %f 152 0.2137942441872188\n",
      "Epoch 152: val loss 0.606041\n",
      "\n",
      "Epoch %d: train loss %f 153 0.23143140652350017\n",
      "Epoch 153: val loss 0.596243\n",
      "\n",
      "Epoch %d: train loss %f 154 0.28275023081472944\n",
      "Epoch 154: val loss 0.627494\n",
      "\n",
      "Epoch %d: train loss %f 155 0.2398336785180228\n",
      "Epoch 155: val loss 0.527839\n",
      "\n",
      "Epoch %d: train loss %f 156 0.26323823630809784\n",
      "Epoch 156: val loss 0.537795\n",
      "\n",
      "Epoch %d: train loss %f 157 0.2753260156938008\n",
      "Epoch 157: val loss 0.585517\n",
      "\n",
      "Epoch %d: train loss %f 158 0.23552219729338372\n",
      "Epoch 158: val loss 0.562642\n",
      "\n",
      "Epoch %d: train loss %f 159 0.23618395732981817\n",
      "Epoch 159: val loss 0.582507\n",
      "\n",
      "Epoch %d: train loss %f 160 0.24368047820670263\n",
      "Epoch 160: val loss 0.641175\n",
      "\n",
      "Epoch %d: train loss %f 161 0.22618304139801435\n",
      "Epoch 161: val loss 0.583526\n",
      "\n",
      "Epoch %d: train loss %f 162 0.2304044089147023\n",
      "Epoch 162: val loss 0.556672\n",
      "\n",
      "Epoch %d: train loss %f 163 0.30555678052561625\n",
      "Epoch 163: val loss 0.684310\n",
      "\n",
      "Epoch %d: train loss %f 164 0.28380746075085234\n",
      "Epoch 164: val loss 0.627696\n",
      "\n",
      "Epoch %d: train loss %f 165 0.2779007149594171\n",
      "Epoch 165: val loss 0.587537\n",
      "\n",
      "Epoch %d: train loss %f 166 0.22356564604810306\n",
      "Epoch 166: val loss 0.599779\n",
      "\n",
      "Epoch %d: train loss %f 167 0.2267375492623874\n",
      "Epoch 167: val loss 0.586323\n",
      "\n",
      "Epoch %d: train loss %f 168 0.2736571026699884\n",
      "Epoch 168: val loss 0.605009\n",
      "\n",
      "Epoch %d: train loss %f 169 0.2636870028717177\n",
      "Epoch 169: val loss 0.610557\n",
      "\n",
      "Epoch %d: train loss %f 170 0.2331154133592333\n",
      "Epoch 170: val loss 0.574749\n",
      "\n",
      "Epoch %d: train loss %f 171 0.21992935133831842\n",
      "Epoch 171: val loss 0.544156\n",
      "\n",
      "Epoch %d: train loss %f 172 0.2267793087022645\n",
      "Epoch 172: val loss 0.613758\n",
      "\n",
      "Epoch %d: train loss %f 173 0.2796672891293253\n",
      "Epoch 173: val loss 0.605576\n",
      "\n",
      "Epoch %d: train loss %f 174 0.22784283331462316\n",
      "Epoch 174: val loss 0.617742\n",
      "\n",
      "Epoch %d: train loss %f 175 0.21424020613942826\n",
      "Epoch 175: val loss 0.549014\n",
      "\n",
      "Epoch %d: train loss %f 176 0.2600032942635672\n",
      "Epoch 176: val loss 0.617035\n",
      "\n",
      "Epoch %d: train loss %f 177 0.21730426166738784\n",
      "Epoch 177: val loss 0.569617\n",
      "\n",
      "Epoch %d: train loss %f 178 0.23108873409884317\n",
      "Epoch 178: val loss 0.591616\n",
      "\n",
      "Epoch %d: train loss %f 179 0.25091747088091715\n",
      "Epoch 179: val loss 0.608129\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6939512491226196\n",
      "Epoch 0: val loss 0.693308\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6906289127137926\n",
      "Epoch 1: val loss 0.690665\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6857311526934305\n",
      "Epoch 2: val loss 0.686753\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6816413601239523\n",
      "Epoch 3: val loss 0.680632\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6758434706264072\n",
      "Epoch 4: val loss 0.673597\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6610605385568407\n",
      "Epoch 5: val loss 0.667046\n",
      "\n",
      "Epoch %d: train loss %f 6 0.650289966000451\n",
      "Epoch 6: val loss 0.660501\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6439866820971171\n",
      "Epoch 7: val loss 0.654111\n",
      "\n",
      "Epoch %d: train loss %f 8 0.6317412985695733\n",
      "Epoch 8: val loss 0.650089\n",
      "\n",
      "Epoch %d: train loss %f 9 0.6033649113443162\n",
      "Epoch 9: val loss 0.652407\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5891863372590807\n",
      "Epoch 10: val loss 0.660756\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5757307873831855\n",
      "Epoch 11: val loss 0.677818\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5632846156756083\n",
      "Epoch 12: val loss 0.686846\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5411456724007925\n",
      "Epoch 13: val loss 0.688918\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5380434493223826\n",
      "Epoch 14: val loss 0.682738\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5687472787168291\n",
      "Epoch 15: val loss 0.692469\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5387893815835317\n",
      "Epoch 16: val loss 0.688673\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5171935326523251\n",
      "Epoch 17: val loss 0.682938\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5298842423492007\n",
      "Epoch 18: val loss 0.684216\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5260502066877153\n",
      "Epoch 19: val loss 0.696635\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5035284956296285\n",
      "Epoch 20: val loss 0.690118\n",
      "\n",
      "Epoch %d: train loss %f 21 0.4967567026615143\n",
      "Epoch 21: val loss 0.714593\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5256114403406779\n",
      "Epoch 22: val loss 0.760762\n",
      "\n",
      "Epoch %d: train loss %f 23 0.5266852842436897\n",
      "Epoch 23: val loss 0.733445\n",
      "\n",
      "Epoch %d: train loss %f 24 0.5268576973014407\n",
      "Epoch 24: val loss 0.708220\n",
      "\n",
      "Epoch %d: train loss %f 25 0.50145623087883\n",
      "Epoch 25: val loss 0.690241\n",
      "\n",
      "Epoch %d: train loss %f 26 0.5075071156024933\n",
      "Epoch 26: val loss 0.705198\n",
      "\n",
      "Epoch %d: train loss %f 27 0.48521172338061863\n",
      "Epoch 27: val loss 0.727534\n",
      "\n",
      "Epoch %d: train loss %f 28 0.523850119776196\n",
      "Epoch 28: val loss 0.742705\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4739809003141191\n",
      "Epoch 29: val loss 0.710526\n",
      "\n",
      "Epoch %d: train loss %f 30 0.5017139083809323\n",
      "Epoch 30: val loss 0.689843\n",
      "\n",
      "Epoch %d: train loss %f 31 0.49998774462276036\n",
      "Epoch 31: val loss 0.707502\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4889520870314704\n",
      "Epoch 32: val loss 0.729698\n",
      "\n",
      "Epoch %d: train loss %f 33 0.48589852783415055\n",
      "Epoch 33: val loss 0.736300\n",
      "\n",
      "Epoch %d: train loss %f 34 0.49162774946954513\n",
      "Epoch 34: val loss 0.738440\n",
      "\n",
      "Epoch %d: train loss %f 35 0.5195270677407583\n",
      "Epoch 35: val loss 0.710681\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4778902630011241\n",
      "Epoch 36: val loss 0.725120\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4772379199663798\n",
      "Epoch 37: val loss 0.734269\n",
      "\n",
      "Epoch %d: train loss %f 38 0.4837687446011437\n",
      "Epoch 38: val loss 0.732324\n",
      "\n",
      "Epoch %d: train loss %f 39 0.5072659088505639\n",
      "Epoch 39: val loss 0.720516\n",
      "\n",
      "Epoch %d: train loss %f 40 0.46691305107540554\n",
      "Epoch 40: val loss 0.719244\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4813235070970323\n",
      "Epoch 41: val loss 0.724091\n",
      "\n",
      "Epoch %d: train loss %f 42 0.47918527987268233\n",
      "Epoch 42: val loss 0.698088\n",
      "\n",
      "Epoch %d: train loss %f 43 0.47028094198968673\n",
      "Epoch 43: val loss 0.713386\n",
      "\n",
      "Epoch %d: train loss %f 44 0.4620954626136356\n",
      "Epoch 44: val loss 0.743916\n",
      "\n",
      "Epoch %d: train loss %f 45 0.4960917830467224\n",
      "Epoch 45: val loss 0.742500\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4879903495311737\n",
      "Epoch 46: val loss 0.722057\n",
      "\n",
      "Epoch %d: train loss %f 47 0.4603552520275116\n",
      "Epoch 47: val loss 0.693009\n",
      "\n",
      "Epoch %d: train loss %f 48 0.4609183536635505\n",
      "Epoch 48: val loss 0.727950\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4764459894763099\n",
      "Epoch 49: val loss 0.758549\n",
      "\n",
      "Epoch %d: train loss %f 50 0.44910014669100445\n",
      "Epoch 50: val loss 0.759017\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4541342457135518\n",
      "Epoch 51: val loss 0.755681\n",
      "\n",
      "Epoch %d: train loss %f 52 0.46625156866179573\n",
      "Epoch 52: val loss 0.738830\n",
      "\n",
      "Epoch %d: train loss %f 53 0.46187908119625515\n",
      "Epoch 53: val loss 0.754451\n",
      "\n",
      "Epoch %d: train loss %f 54 0.4570688870218065\n",
      "Epoch 54: val loss 0.761801\n",
      "\n",
      "Epoch %d: train loss %f 55 0.45291707581943935\n",
      "Epoch 55: val loss 0.745781\n",
      "\n",
      "Epoch %d: train loss %f 56 0.46254468626446194\n",
      "Epoch 56: val loss 0.748959\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4725945227675968\n",
      "Epoch 57: val loss 0.736146\n",
      "\n",
      "Epoch %d: train loss %f 58 0.45170759161313373\n",
      "Epoch 58: val loss 0.759679\n",
      "\n",
      "Epoch %d: train loss %f 59 0.44838828179571366\n",
      "Epoch 59: val loss 0.754686\n",
      "\n",
      "Epoch %d: train loss %f 60 0.46936673588222927\n",
      "Epoch 60: val loss 0.745791\n",
      "\n",
      "Epoch %d: train loss %f 61 0.45780205726623535\n",
      "Epoch 61: val loss 0.779870\n",
      "\n",
      "Epoch %d: train loss %f 62 0.4473108649253845\n",
      "Epoch 62: val loss 0.773533\n",
      "\n",
      "Epoch %d: train loss %f 63 0.455403874317805\n",
      "Epoch 63: val loss 0.785359\n",
      "\n",
      "Epoch %d: train loss %f 64 0.46080313788519967\n",
      "Epoch 64: val loss 0.772167\n",
      "\n",
      "Epoch %d: train loss %f 65 0.44965550634596085\n",
      "Epoch 65: val loss 0.754494\n",
      "\n",
      "Epoch %d: train loss %f 66 0.4416944748825497\n",
      "Epoch 66: val loss 0.699025\n",
      "\n",
      "Epoch %d: train loss %f 67 0.4318439198864831\n",
      "Epoch 67: val loss 0.747561\n",
      "\n",
      "Epoch %d: train loss %f 68 0.4599624143706428\n",
      "Epoch 68: val loss 0.758720\n",
      "\n",
      "Epoch %d: train loss %f 69 0.47553032636642456\n",
      "Epoch 69: val loss 0.742810\n",
      "\n",
      "Epoch %d: train loss %f 70 0.43885398242208695\n",
      "Epoch 70: val loss 0.740479\n",
      "\n",
      "Epoch %d: train loss %f 71 0.43783767686949837\n",
      "Epoch 71: val loss 0.753731\n",
      "\n",
      "Epoch %d: train loss %f 72 0.4373319016562568\n",
      "Epoch 72: val loss 0.756101\n",
      "\n",
      "Epoch %d: train loss %f 73 0.44895761211713153\n",
      "Epoch 73: val loss 0.749385\n",
      "\n",
      "Epoch %d: train loss %f 74 0.45514549646112656\n",
      "Epoch 74: val loss 0.758397\n",
      "\n",
      "Epoch %d: train loss %f 75 0.4392685691515605\n",
      "Epoch 75: val loss 0.775415\n",
      "\n",
      "Epoch %d: train loss %f 76 0.41204553180270725\n",
      "Epoch 76: val loss 0.782652\n",
      "\n",
      "Epoch %d: train loss %f 77 0.427303949991862\n",
      "Epoch 77: val loss 0.784251\n",
      "\n",
      "Epoch %d: train loss %f 78 0.44267106387350297\n",
      "Epoch 78: val loss 0.788596\n",
      "\n",
      "Epoch %d: train loss %f 79 0.43650397327211166\n",
      "Epoch 79: val loss 0.810269\n",
      "\n",
      "Epoch %d: train loss %f 80 0.4373414185312059\n",
      "Epoch 80: val loss 0.782394\n",
      "\n",
      "Epoch %d: train loss %f 81 0.4641689525710212\n",
      "Epoch 81: val loss 0.776445\n",
      "\n",
      "Epoch %d: train loss %f 82 0.42586279577679104\n",
      "Epoch 82: val loss 0.766381\n",
      "\n",
      "Epoch %d: train loss %f 83 0.44259007109536064\n",
      "Epoch 83: val loss 0.727606\n",
      "\n",
      "Epoch %d: train loss %f 84 0.44369670417573714\n",
      "Epoch 84: val loss 0.768780\n",
      "\n",
      "Epoch %d: train loss %f 85 0.4481540819009145\n",
      "Epoch 85: val loss 0.777895\n",
      "\n",
      "Epoch %d: train loss %f 86 0.4366399281554752\n",
      "Epoch 86: val loss 0.756478\n",
      "\n",
      "Epoch %d: train loss %f 87 0.4202088514963786\n",
      "Epoch 87: val loss 0.771049\n",
      "\n",
      "Epoch %d: train loss %f 88 0.4483480585945977\n",
      "Epoch 88: val loss 0.748901\n",
      "\n",
      "Epoch %d: train loss %f 89 0.45085014237297905\n",
      "Epoch 89: val loss 0.763992\n",
      "\n",
      "Epoch %d: train loss %f 90 0.44016923838191563\n",
      "Epoch 90: val loss 0.767663\n",
      "\n",
      "Epoch %d: train loss %f 91 0.4365134470992618\n",
      "Epoch 91: val loss 0.769974\n",
      "\n",
      "Epoch %d: train loss %f 92 0.4081711636649238\n",
      "Epoch 92: val loss 0.800887\n",
      "\n",
      "Epoch %d: train loss %f 93 0.4502733018663194\n",
      "Epoch 93: val loss 0.813771\n",
      "\n",
      "Epoch %d: train loss %f 94 0.4009539286295573\n",
      "Epoch 94: val loss 0.785772\n",
      "\n",
      "Epoch %d: train loss %f 95 0.41926927367846173\n",
      "Epoch 95: val loss 0.795616\n",
      "\n",
      "Epoch %d: train loss %f 96 0.39058920906649697\n",
      "Epoch 96: val loss 0.825891\n",
      "\n",
      "Epoch %d: train loss %f 97 0.47232407993740505\n",
      "Epoch 97: val loss 0.792890\n",
      "\n",
      "Epoch %d: train loss %f 98 0.4742286271519131\n",
      "Epoch 98: val loss 0.805124\n",
      "\n",
      "Epoch %d: train loss %f 99 0.41354473763042027\n",
      "Epoch 99: val loss 0.824132\n",
      "\n",
      "Epoch %d: train loss %f 100 0.3975727988613976\n",
      "Epoch 100: val loss 0.825171\n",
      "\n",
      "Epoch %d: train loss %f 101 0.4054935872554779\n",
      "Epoch 101: val loss 0.769668\n",
      "\n",
      "Epoch %d: train loss %f 102 0.417063123650021\n",
      "Epoch 102: val loss 0.782342\n",
      "\n",
      "Epoch %d: train loss %f 103 0.42107879784372115\n",
      "Epoch 103: val loss 0.806739\n",
      "\n",
      "Epoch %d: train loss %f 104 0.41896137595176697\n",
      "Epoch 104: val loss 0.780044\n",
      "\n",
      "Epoch %d: train loss %f 105 0.4171164631843567\n",
      "Epoch 105: val loss 0.845449\n",
      "\n",
      "Epoch %d: train loss %f 106 0.39741983347468907\n",
      "Epoch 106: val loss 0.790916\n",
      "\n",
      "Epoch %d: train loss %f 107 0.4323540959093306\n",
      "Epoch 107: val loss 0.826518\n",
      "\n",
      "Epoch %d: train loss %f 108 0.3741169058614307\n",
      "Epoch 108: val loss 0.832243\n",
      "\n",
      "Epoch %d: train loss %f 109 0.40896210736698574\n",
      "Epoch 109: val loss 0.807928\n",
      "\n",
      "Epoch %d: train loss %f 110 0.3964923272530238\n",
      "Epoch 110: val loss 0.779696\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3815149466196696\n",
      "Epoch 111: val loss 0.806522\n",
      "\n",
      "Epoch %d: train loss %f 112 0.42459403143988717\n",
      "Epoch 112: val loss 0.857136\n",
      "\n",
      "Epoch %d: train loss %f 113 0.39501606424649555\n",
      "Epoch 113: val loss 0.816952\n",
      "\n",
      "Epoch %d: train loss %f 114 0.3996758908033371\n",
      "Epoch 114: val loss 0.782843\n",
      "\n",
      "Epoch %d: train loss %f 115 0.405311503344112\n",
      "Epoch 115: val loss 0.789958\n",
      "\n",
      "Epoch %d: train loss %f 116 0.41339886519644\n",
      "Epoch 116: val loss 0.839899\n",
      "\n",
      "Epoch %d: train loss %f 117 0.4314934147728814\n",
      "Epoch 117: val loss 0.828514\n",
      "\n",
      "Epoch %d: train loss %f 118 0.44315779871410793\n",
      "Epoch 118: val loss 0.784602\n",
      "\n",
      "Epoch %d: train loss %f 119 0.3926764345831341\n",
      "Epoch 119: val loss 0.781541\n",
      "\n",
      "Epoch %d: train loss %f 120 0.3853149248494042\n",
      "Epoch 120: val loss 0.803406\n",
      "\n",
      "Epoch %d: train loss %f 121 0.4292269680235121\n",
      "Epoch 121: val loss 0.795473\n",
      "\n",
      "Epoch %d: train loss %f 122 0.40892844398816425\n",
      "Epoch 122: val loss 0.797803\n",
      "\n",
      "Epoch %d: train loss %f 123 0.4041447573237949\n",
      "Epoch 123: val loss 0.790023\n",
      "\n",
      "Epoch %d: train loss %f 124 0.40708837740951115\n",
      "Epoch 124: val loss 0.811276\n",
      "\n",
      "Epoch %d: train loss %f 125 0.381521870692571\n",
      "Epoch 125: val loss 0.804806\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3843758735391829\n",
      "Epoch 126: val loss 0.807074\n",
      "\n",
      "Epoch %d: train loss %f 127 0.3793085813522339\n",
      "Epoch 127: val loss 0.841986\n",
      "\n",
      "Epoch %d: train loss %f 128 0.4194108843803406\n",
      "Epoch 128: val loss 0.820104\n",
      "\n",
      "Epoch %d: train loss %f 129 0.4048124286863539\n",
      "Epoch 129: val loss 0.843165\n",
      "\n",
      "Epoch %d: train loss %f 130 0.3950428581900067\n",
      "Epoch 130: val loss 0.839374\n",
      "\n",
      "Epoch %d: train loss %f 131 0.3770636386341519\n",
      "Epoch 131: val loss 0.862511\n",
      "\n",
      "Epoch %d: train loss %f 132 0.41697665717866683\n",
      "Epoch 132: val loss 0.839106\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3856288227770064\n",
      "Epoch 133: val loss 0.831849\n",
      "\n",
      "Epoch %d: train loss %f 134 0.38490146895249683\n",
      "Epoch 134: val loss 0.776018\n",
      "\n",
      "Epoch %d: train loss %f 135 0.39270305302408004\n",
      "Epoch 135: val loss 0.781639\n",
      "\n",
      "Epoch %d: train loss %f 136 0.38630250758594936\n",
      "Epoch 136: val loss 0.806283\n",
      "\n",
      "Epoch %d: train loss %f 137 0.4047868814733293\n",
      "Epoch 137: val loss 0.865351\n",
      "\n",
      "Epoch %d: train loss %f 138 0.3778535823027293\n",
      "Epoch 138: val loss 0.815104\n",
      "\n",
      "Epoch %d: train loss %f 139 0.39127117064264083\n",
      "Epoch 139: val loss 0.776939\n",
      "\n",
      "Epoch %d: train loss %f 140 0.37062610189119977\n",
      "Epoch 140: val loss 0.846791\n",
      "\n",
      "Epoch %d: train loss %f 141 0.39907455775472855\n",
      "Epoch 141: val loss 0.840388\n",
      "\n",
      "Epoch %d: train loss %f 142 0.38767942123942906\n",
      "Epoch 142: val loss 0.806772\n",
      "\n",
      "Epoch %d: train loss %f 143 0.3794841319322586\n",
      "Epoch 143: val loss 0.815875\n",
      "\n",
      "Epoch %d: train loss %f 144 0.41223201486799455\n",
      "Epoch 144: val loss 0.827982\n",
      "\n",
      "Epoch %d: train loss %f 145 0.35254043340682983\n",
      "Epoch 145: val loss 0.871854\n",
      "\n",
      "Epoch %d: train loss %f 146 0.41349557042121887\n",
      "Epoch 146: val loss 0.839558\n",
      "\n",
      "Epoch %d: train loss %f 147 0.3820144186417262\n",
      "Epoch 147: val loss 0.836481\n",
      "\n",
      "Epoch %d: train loss %f 148 0.4131370683511098\n",
      "Epoch 148: val loss 0.904837\n",
      "\n",
      "Epoch %d: train loss %f 149 0.3549938317802217\n",
      "Epoch 149: val loss 0.858846\n",
      "\n",
      "Epoch %d: train loss %f 150 0.3528359747595257\n",
      "Epoch 150: val loss 0.866937\n",
      "\n",
      "Epoch %d: train loss %f 151 0.39184292819764877\n",
      "Epoch 151: val loss 0.861269\n",
      "\n",
      "Epoch %d: train loss %f 152 0.39367953439553577\n",
      "Epoch 152: val loss 0.815602\n",
      "\n",
      "Epoch %d: train loss %f 153 0.3653574486573537\n",
      "Epoch 153: val loss 0.899182\n",
      "\n",
      "Epoch %d: train loss %f 154 0.3865505274799135\n",
      "Epoch 154: val loss 0.902426\n",
      "\n",
      "Epoch %d: train loss %f 155 0.38358596960703534\n",
      "Epoch 155: val loss 0.885388\n",
      "\n",
      "Epoch %d: train loss %f 156 0.3586897585127089\n",
      "Epoch 156: val loss 0.889307\n",
      "\n",
      "Epoch %d: train loss %f 157 0.40234171350797016\n",
      "Epoch 157: val loss 0.902475\n",
      "\n",
      "Epoch %d: train loss %f 158 0.37077393796708846\n",
      "Epoch 158: val loss 0.920149\n",
      "\n",
      "Epoch %d: train loss %f 159 0.37676927116182113\n",
      "Epoch 159: val loss 0.877311\n",
      "\n",
      "Epoch %d: train loss %f 160 0.3799510614739524\n",
      "Epoch 160: val loss 0.889018\n",
      "\n",
      "Epoch %d: train loss %f 161 0.3872671922047933\n",
      "Epoch 161: val loss 0.865682\n",
      "\n",
      "Epoch %d: train loss %f 162 0.38094184133741593\n",
      "Epoch 162: val loss 0.859576\n",
      "\n",
      "Epoch %d: train loss %f 163 0.3898450434207916\n",
      "Epoch 163: val loss 0.899892\n",
      "\n",
      "Epoch %d: train loss %f 164 0.3618211481306288\n",
      "Epoch 164: val loss 0.908984\n",
      "\n",
      "Epoch %d: train loss %f 165 0.37433012657695347\n",
      "Epoch 165: val loss 0.816420\n",
      "\n",
      "Epoch %d: train loss %f 166 0.3703066209952037\n",
      "Epoch 166: val loss 0.833360\n",
      "\n",
      "Epoch %d: train loss %f 167 0.3900898214843538\n",
      "Epoch 167: val loss 0.890669\n",
      "\n",
      "Epoch %d: train loss %f 168 0.4116923047436608\n",
      "Epoch 168: val loss 0.828199\n",
      "\n",
      "Epoch %d: train loss %f 169 0.34560473759969074\n",
      "Epoch 169: val loss 0.813439\n",
      "\n",
      "Epoch %d: train loss %f 170 0.36086221701569027\n",
      "Epoch 170: val loss 0.816550\n",
      "\n",
      "Epoch %d: train loss %f 171 0.34833183222346836\n",
      "Epoch 171: val loss 0.894687\n",
      "\n",
      "Epoch %d: train loss %f 172 0.3285683062341478\n",
      "Epoch 172: val loss 0.902296\n",
      "\n",
      "Epoch %d: train loss %f 173 0.36138607064882916\n",
      "Epoch 173: val loss 0.891034\n",
      "\n",
      "Epoch %d: train loss %f 174 0.3404771354463365\n",
      "Epoch 174: val loss 0.893387\n",
      "\n",
      "Epoch %d: train loss %f 175 0.378069669008255\n",
      "Epoch 175: val loss 0.885531\n",
      "\n",
      "Epoch %d: train loss %f 176 0.3736457559797499\n",
      "Epoch 176: val loss 0.890389\n",
      "\n",
      "Epoch %d: train loss %f 177 0.3657364812162187\n",
      "Epoch 177: val loss 0.882418\n",
      "\n",
      "Epoch %d: train loss %f 178 0.3771646320819855\n",
      "Epoch 178: val loss 0.916414\n",
      "\n",
      "Epoch %d: train loss %f 179 0.3589368727472093\n",
      "Epoch 179: val loss 0.917410\n",
      "\n",
      "Epoch %d: train loss %f 180 0.3473302606079314\n",
      "Epoch 180: val loss 0.862620\n",
      "\n",
      "Epoch %d: train loss %f 181 0.34108126329051125\n",
      "Epoch 181: val loss 0.897624\n",
      "\n",
      "Epoch %d: train loss %f 182 0.31928801867696976\n",
      "Epoch 182: val loss 0.915175\n",
      "\n",
      "Epoch %d: train loss %f 183 0.36308034757773083\n",
      "Epoch 183: val loss 0.896116\n",
      "\n",
      "Epoch %d: train loss %f 184 0.369430070122083\n",
      "Epoch 184: val loss 0.867181\n",
      "\n",
      "Epoch %d: train loss %f 185 0.33379537529415554\n",
      "Epoch 185: val loss 0.863436\n",
      "\n",
      "Epoch %d: train loss %f 186 0.3503931396537357\n",
      "Epoch 186: val loss 0.906599\n",
      "\n",
      "Epoch %d: train loss %f 187 0.3329542345470852\n",
      "Epoch 187: val loss 0.929950\n",
      "\n",
      "Epoch %d: train loss %f 188 0.3519185715251499\n",
      "Epoch 188: val loss 0.888976\n",
      "\n",
      "Epoch %d: train loss %f 189 0.35047808041175205\n",
      "Epoch 189: val loss 0.890986\n",
      "\n",
      "Epoch %d: train loss %f 190 0.34527530272801715\n",
      "Epoch 190: val loss 0.919393\n",
      "\n",
      "Epoch %d: train loss %f 191 0.3992935104502572\n",
      "Epoch 191: val loss 0.889874\n",
      "\n",
      "Epoch %d: train loss %f 192 0.3595003883043925\n",
      "Epoch 192: val loss 0.935968\n",
      "\n",
      "Epoch %d: train loss %f 193 0.38622867067654926\n",
      "Epoch 193: val loss 0.883737\n",
      "\n",
      "Epoch %d: train loss %f 194 0.3408127427101135\n",
      "Epoch 194: val loss 0.921378\n",
      "\n",
      "Epoch %d: train loss %f 195 0.3397194941838582\n",
      "Epoch 195: val loss 0.910980\n",
      "\n",
      "Epoch %d: train loss %f 196 0.41946812636322445\n",
      "Epoch 196: val loss 0.843598\n",
      "\n",
      "Epoch %d: train loss %f 197 0.38306517402331036\n",
      "Epoch 197: val loss 0.943818\n",
      "\n",
      "Epoch %d: train loss %f 198 0.3836134374141693\n",
      "Epoch 198: val loss 0.844347\n",
      "\n",
      "Epoch %d: train loss %f 199 0.3306487484110726\n",
      "Epoch 199: val loss 0.859360\n",
      "\n",
      "Epoch %d: train loss %f 0 0.7041883601082696\n",
      "Epoch 0: val loss 0.704515\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6991329458024766\n",
      "Epoch 1: val loss 0.701045\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6938511861695184\n",
      "Epoch 2: val loss 0.695593\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6879893077744378\n",
      "Epoch 3: val loss 0.688008\n",
      "\n",
      "Epoch %d: train loss %f 4 0.674618124961853\n",
      "Epoch 4: val loss 0.678002\n",
      "\n",
      "Epoch %d: train loss %f 5 0.661832140551673\n",
      "Epoch 5: val loss 0.659540\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6520953708224826\n",
      "Epoch 6: val loss 0.639928\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6305391059981452\n",
      "Epoch 7: val loss 0.621247\n",
      "\n",
      "Epoch %d: train loss %f 8 0.6236138277583652\n",
      "Epoch 8: val loss 0.605957\n",
      "\n",
      "Epoch %d: train loss %f 9 0.6249212357732985\n",
      "Epoch 9: val loss 0.601163\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5956190427144369\n",
      "Epoch 10: val loss 0.590508\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5754765669504801\n",
      "Epoch 11: val loss 0.568047\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5809725291199155\n",
      "Epoch 12: val loss 0.565751\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5644922190242343\n",
      "Epoch 13: val loss 0.565093\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5453031427330441\n",
      "Epoch 14: val loss 0.563376\n",
      "\n",
      "Epoch %d: train loss %f 15 0.549623837073644\n",
      "Epoch 15: val loss 0.570142\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5620816780461205\n",
      "Epoch 16: val loss 0.577082\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5548534260855781\n",
      "Epoch 17: val loss 0.562868\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5522415108150907\n",
      "Epoch 18: val loss 0.559015\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5282107823424869\n",
      "Epoch 19: val loss 0.554496\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5337097280555301\n",
      "Epoch 20: val loss 0.568648\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5105259087350633\n",
      "Epoch 21: val loss 0.576950\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5260868271191915\n",
      "Epoch 22: val loss 0.572227\n",
      "\n",
      "Epoch %d: train loss %f 23 0.5164450638824039\n",
      "Epoch 23: val loss 0.568723\n",
      "\n",
      "Epoch %d: train loss %f 24 0.5115160180462731\n",
      "Epoch 24: val loss 0.580268\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4940711193614536\n",
      "Epoch 25: val loss 0.574064\n",
      "\n",
      "Epoch %d: train loss %f 26 0.528289688958062\n",
      "Epoch 26: val loss 0.562567\n",
      "\n",
      "Epoch %d: train loss %f 27 0.5410756170749664\n",
      "Epoch 27: val loss 0.586696\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4916843507024977\n",
      "Epoch 28: val loss 0.575737\n",
      "\n",
      "Epoch %d: train loss %f 29 0.5205947160720825\n",
      "Epoch 29: val loss 0.585574\n",
      "\n",
      "Epoch %d: train loss %f 30 0.49952833520041573\n",
      "Epoch 30: val loss 0.583440\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4752707713180118\n",
      "Epoch 31: val loss 0.582149\n",
      "\n",
      "Epoch %d: train loss %f 32 0.5060469375716315\n",
      "Epoch 32: val loss 0.574926\n",
      "\n",
      "Epoch %d: train loss %f 33 0.5083337426185608\n",
      "Epoch 33: val loss 0.610484\n",
      "\n",
      "Epoch %d: train loss %f 34 0.47376543283462524\n",
      "Epoch 34: val loss 0.586957\n",
      "\n",
      "Epoch %d: train loss %f 35 0.45575308468606734\n",
      "Epoch 35: val loss 0.584303\n",
      "\n",
      "Epoch %d: train loss %f 36 0.504467225737042\n",
      "Epoch 36: val loss 0.584625\n",
      "\n",
      "Epoch %d: train loss %f 37 0.5190607474909888\n",
      "Epoch 37: val loss 0.616779\n",
      "\n",
      "Epoch %d: train loss %f 38 0.4692660901281569\n",
      "Epoch 38: val loss 0.621917\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4681805504692925\n",
      "Epoch 39: val loss 0.605138\n",
      "\n",
      "Epoch %d: train loss %f 40 0.4650508032904731\n",
      "Epoch 40: val loss 0.628458\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4879131250911289\n",
      "Epoch 41: val loss 0.633504\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4749416510264079\n",
      "Epoch 42: val loss 0.632814\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4657101266913944\n",
      "Epoch 43: val loss 0.619742\n",
      "\n",
      "Epoch %d: train loss %f 44 0.48557674553659225\n",
      "Epoch 44: val loss 0.629591\n",
      "\n",
      "Epoch %d: train loss %f 45 0.46758588155110675\n",
      "Epoch 45: val loss 0.607008\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4993792374928792\n",
      "Epoch 46: val loss 0.614657\n",
      "\n",
      "Epoch %d: train loss %f 47 0.4631369047694736\n",
      "Epoch 47: val loss 0.646890\n",
      "\n",
      "Epoch %d: train loss %f 48 0.4843216836452484\n",
      "Epoch 48: val loss 0.630071\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4814908504486084\n",
      "Epoch 49: val loss 0.643237\n",
      "\n",
      "Epoch %d: train loss %f 50 0.43710408939255607\n",
      "Epoch 50: val loss 0.641778\n",
      "\n",
      "Epoch %d: train loss %f 51 0.42436840799119735\n",
      "Epoch 51: val loss 0.633178\n",
      "\n",
      "Epoch %d: train loss %f 52 0.47530915670924717\n",
      "Epoch 52: val loss 0.655663\n",
      "\n",
      "Epoch %d: train loss %f 53 0.4398451778623793\n",
      "Epoch 53: val loss 0.638485\n",
      "\n",
      "Epoch %d: train loss %f 54 0.4306129349602593\n",
      "Epoch 54: val loss 0.648052\n",
      "\n",
      "Epoch %d: train loss %f 55 0.44526570041974384\n",
      "Epoch 55: val loss 0.650897\n",
      "\n",
      "Epoch %d: train loss %f 56 0.4445439510875278\n",
      "Epoch 56: val loss 0.650468\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4362577663527595\n",
      "Epoch 57: val loss 0.672696\n",
      "\n",
      "Epoch %d: train loss %f 58 0.45079360405604046\n",
      "Epoch 58: val loss 0.651753\n",
      "\n",
      "Epoch %d: train loss %f 59 0.44960365030500626\n",
      "Epoch 59: val loss 0.649982\n",
      "\n",
      "Epoch %d: train loss %f 60 0.47676510612169903\n",
      "Epoch 60: val loss 0.656314\n",
      "\n",
      "Epoch %d: train loss %f 61 0.4425133466720581\n",
      "Epoch 61: val loss 0.682788\n",
      "\n",
      "Epoch %d: train loss %f 62 0.4581898616419898\n",
      "Epoch 62: val loss 0.634543\n",
      "\n",
      "Epoch %d: train loss %f 63 0.4167790181106991\n",
      "Epoch 63: val loss 0.692031\n",
      "\n",
      "Epoch %d: train loss %f 64 0.44923551546202767\n",
      "Epoch 64: val loss 0.620737\n",
      "\n",
      "Epoch %d: train loss %f 65 0.4433338940143585\n",
      "Epoch 65: val loss 0.665569\n",
      "\n",
      "Epoch %d: train loss %f 66 0.424960196018219\n",
      "Epoch 66: val loss 0.707090\n",
      "\n",
      "Epoch %d: train loss %f 67 0.40028433005015057\n",
      "Epoch 67: val loss 0.666269\n",
      "\n",
      "Epoch %d: train loss %f 68 0.4328918506701787\n",
      "Epoch 68: val loss 0.675441\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4038238293594784\n",
      "Epoch 69: val loss 0.654670\n",
      "\n",
      "Epoch %d: train loss %f 70 0.4227691723240746\n",
      "Epoch 70: val loss 0.640108\n",
      "\n",
      "Epoch %d: train loss %f 71 0.41580436958207023\n",
      "Epoch 71: val loss 0.692024\n",
      "\n",
      "Epoch %d: train loss %f 72 0.443382634056939\n",
      "Epoch 72: val loss 0.680407\n",
      "\n",
      "Epoch %d: train loss %f 73 0.39274979962242973\n",
      "Epoch 73: val loss 0.700977\n",
      "\n",
      "Epoch %d: train loss %f 74 0.39568128519588047\n",
      "Epoch 74: val loss 0.693399\n",
      "\n",
      "Epoch %d: train loss %f 75 0.4100400507450104\n",
      "Epoch 75: val loss 0.706056\n",
      "\n",
      "Epoch %d: train loss %f 76 0.38778779407342273\n",
      "Epoch 76: val loss 0.706551\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3901912768681844\n",
      "Epoch 77: val loss 0.697735\n",
      "\n",
      "Epoch %d: train loss %f 78 0.4000007559855779\n",
      "Epoch 78: val loss 0.697551\n",
      "\n",
      "Epoch %d: train loss %f 79 0.40978486670388115\n",
      "Epoch 79: val loss 0.689199\n",
      "\n",
      "Epoch %d: train loss %f 80 0.4415091441737281\n",
      "Epoch 80: val loss 0.744360\n",
      "\n",
      "Epoch %d: train loss %f 81 0.41582799288961625\n",
      "Epoch 81: val loss 0.726826\n",
      "\n",
      "Epoch %d: train loss %f 82 0.34724561704529655\n",
      "Epoch 82: val loss 0.794669\n",
      "\n",
      "Epoch %d: train loss %f 83 0.39071863889694214\n",
      "Epoch 83: val loss 0.725465\n",
      "\n",
      "Epoch %d: train loss %f 84 0.4322119156519572\n",
      "Epoch 84: val loss 0.709192\n",
      "\n",
      "Epoch %d: train loss %f 85 0.4010411434703403\n",
      "Epoch 85: val loss 0.755506\n",
      "\n",
      "Epoch %d: train loss %f 86 0.36689332458708024\n",
      "Epoch 86: val loss 0.723060\n",
      "\n",
      "Epoch %d: train loss %f 87 0.42660914527045357\n",
      "Epoch 87: val loss 0.758978\n",
      "\n",
      "Epoch %d: train loss %f 88 0.37031804853015476\n",
      "Epoch 88: val loss 0.834788\n",
      "\n",
      "Epoch %d: train loss %f 89 0.43032747507095337\n",
      "Epoch 89: val loss 0.735380\n",
      "\n",
      "Epoch %d: train loss %f 90 0.40723766883214313\n",
      "Epoch 90: val loss 0.724116\n",
      "\n",
      "Epoch %d: train loss %f 91 0.39288678103023106\n",
      "Epoch 91: val loss 0.743881\n",
      "\n",
      "Epoch %d: train loss %f 92 0.37259439792897964\n",
      "Epoch 92: val loss 0.732184\n",
      "\n",
      "Epoch %d: train loss %f 93 0.39562463760375977\n",
      "Epoch 93: val loss 0.757495\n",
      "\n",
      "Epoch %d: train loss %f 94 0.41576410002178615\n",
      "Epoch 94: val loss 0.766172\n",
      "\n",
      "Epoch %d: train loss %f 95 0.3856861823134952\n",
      "Epoch 95: val loss 0.746906\n",
      "\n",
      "Epoch %d: train loss %f 96 0.4019920378923416\n",
      "Epoch 96: val loss 0.758709\n",
      "\n",
      "Epoch %d: train loss %f 97 0.35451747642623055\n",
      "Epoch 97: val loss 0.719860\n",
      "\n",
      "Epoch %d: train loss %f 98 0.3887018710374832\n",
      "Epoch 98: val loss 0.758472\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3906950056552887\n",
      "Epoch 99: val loss 0.751917\n",
      "\n",
      "Epoch %d: train loss %f 100 0.38952524463335675\n",
      "Epoch 100: val loss 0.754889\n",
      "\n",
      "Epoch %d: train loss %f 101 0.36704761783281964\n",
      "Epoch 101: val loss 0.749449\n",
      "\n",
      "Epoch %d: train loss %f 102 0.360548992951711\n",
      "Epoch 102: val loss 0.767258\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3490796486536662\n",
      "Epoch 103: val loss 0.753174\n",
      "\n",
      "Epoch %d: train loss %f 104 0.3834346781174342\n",
      "Epoch 104: val loss 0.801889\n",
      "\n",
      "Epoch %d: train loss %f 105 0.36696580549081165\n",
      "Epoch 105: val loss 0.796100\n",
      "\n",
      "Epoch %d: train loss %f 106 0.3726239949464798\n",
      "Epoch 106: val loss 0.811579\n",
      "\n",
      "Epoch %d: train loss %f 107 0.4166885283258226\n",
      "Epoch 107: val loss 0.786247\n",
      "\n",
      "Epoch %d: train loss %f 108 0.40474555889765423\n",
      "Epoch 108: val loss 0.795222\n",
      "\n",
      "Epoch %d: train loss %f 109 0.34858080248037976\n",
      "Epoch 109: val loss 0.795787\n",
      "\n",
      "Epoch %d: train loss %f 110 0.39426371786329484\n",
      "Epoch 110: val loss 0.776591\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3980088233947754\n",
      "Epoch 111: val loss 0.819360\n",
      "\n",
      "Epoch %d: train loss %f 112 0.37311399314138627\n",
      "Epoch 112: val loss 0.802212\n",
      "\n",
      "Epoch %d: train loss %f 113 0.3582726866006851\n",
      "Epoch 113: val loss 0.780212\n",
      "\n",
      "Epoch %d: train loss %f 114 0.3252967960304684\n",
      "Epoch 114: val loss 0.766480\n",
      "\n",
      "Epoch %d: train loss %f 115 0.3526490181684494\n",
      "Epoch 115: val loss 0.851573\n",
      "\n",
      "Epoch %d: train loss %f 116 0.34384587075975204\n",
      "Epoch 116: val loss 0.798953\n",
      "\n",
      "Epoch %d: train loss %f 117 0.36335058013598126\n",
      "Epoch 117: val loss 0.775930\n",
      "\n",
      "Epoch %d: train loss %f 118 0.34185966352621716\n",
      "Epoch 118: val loss 0.827543\n",
      "\n",
      "Epoch %d: train loss %f 119 0.3618561973174413\n",
      "Epoch 119: val loss 0.835846\n",
      "\n",
      "Epoch %d: train loss %f 120 0.3329357686969969\n",
      "Epoch 120: val loss 0.815722\n",
      "\n",
      "Epoch %d: train loss %f 121 0.35979394283559585\n",
      "Epoch 121: val loss 0.847386\n",
      "\n",
      "Epoch %d: train loss %f 122 0.39944827887747025\n",
      "Epoch 122: val loss 0.853673\n",
      "\n",
      "Epoch %d: train loss %f 123 0.3608504749006695\n",
      "Epoch 123: val loss 0.859291\n",
      "\n",
      "Epoch %d: train loss %f 124 0.37593618366453385\n",
      "Epoch 124: val loss 0.807189\n",
      "\n",
      "Epoch %d: train loss %f 125 0.32986665931012893\n",
      "Epoch 125: val loss 0.833594\n",
      "\n",
      "Epoch %d: train loss %f 126 0.33258670651250416\n",
      "Epoch 126: val loss 0.836612\n",
      "\n",
      "Epoch %d: train loss %f 127 0.3360893991258409\n",
      "Epoch 127: val loss 0.840485\n",
      "\n",
      "Epoch %d: train loss %f 128 0.32231440477901036\n",
      "Epoch 128: val loss 0.883750\n",
      "\n",
      "Epoch %d: train loss %f 129 0.3263850327995088\n",
      "Epoch 129: val loss 0.813373\n",
      "\n",
      "Epoch %d: train loss %f 130 0.33729560010963017\n",
      "Epoch 130: val loss 0.899590\n",
      "\n",
      "Epoch %d: train loss %f 131 0.36692765686247086\n",
      "Epoch 131: val loss 0.821027\n",
      "\n",
      "Epoch %d: train loss %f 132 0.363600997461213\n",
      "Epoch 132: val loss 0.870035\n",
      "\n",
      "Epoch %d: train loss %f 133 0.35970813698238796\n",
      "Epoch 133: val loss 0.863644\n",
      "\n",
      "Epoch %d: train loss %f 134 0.3694208761056264\n",
      "Epoch 134: val loss 0.894771\n",
      "\n",
      "Epoch %d: train loss %f 135 0.2961461941401164\n",
      "Epoch 135: val loss 0.833723\n",
      "\n",
      "Epoch %d: train loss %f 136 0.34786077340443927\n",
      "Epoch 136: val loss 0.887912\n",
      "\n",
      "Epoch %d: train loss %f 137 0.3067050162288878\n",
      "Epoch 137: val loss 0.873441\n",
      "\n",
      "Epoch %d: train loss %f 138 0.3399403426382277\n",
      "Epoch 138: val loss 0.878061\n",
      "\n",
      "Epoch %d: train loss %f 139 0.3429698298374812\n",
      "Epoch 139: val loss 0.922298\n",
      "\n",
      "Epoch %d: train loss %f 140 0.3399396273824904\n",
      "Epoch 140: val loss 0.840828\n",
      "\n",
      "Epoch %d: train loss %f 141 0.3568685402472814\n",
      "Epoch 141: val loss 0.947238\n",
      "\n",
      "Epoch %d: train loss %f 142 0.33423790170086753\n",
      "Epoch 142: val loss 0.865010\n",
      "\n",
      "Epoch %d: train loss %f 143 0.37682347496350604\n",
      "Epoch 143: val loss 0.831106\n",
      "\n",
      "Epoch %d: train loss %f 144 0.31946760912736255\n",
      "Epoch 144: val loss 0.906518\n",
      "\n",
      "Epoch %d: train loss %f 145 0.31960465345117783\n",
      "Epoch 145: val loss 0.901590\n",
      "\n",
      "Epoch %d: train loss %f 146 0.35066841708289254\n",
      "Epoch 146: val loss 0.888423\n",
      "\n",
      "Epoch %d: train loss %f 147 0.30266964104440475\n",
      "Epoch 147: val loss 0.835979\n",
      "\n",
      "Epoch %d: train loss %f 148 0.32758613924185437\n",
      "Epoch 148: val loss 0.897621\n",
      "\n",
      "Epoch %d: train loss %f 149 0.2869930581914054\n",
      "Epoch 149: val loss 0.894707\n",
      "\n",
      "Epoch %d: train loss %f 150 0.32511206302377915\n",
      "Epoch 150: val loss 0.854458\n",
      "\n",
      "Epoch %d: train loss %f 151 0.3455381128523085\n",
      "Epoch 151: val loss 0.886339\n",
      "\n",
      "Epoch %d: train loss %f 152 0.31993403202957577\n",
      "Epoch 152: val loss 0.924151\n",
      "\n",
      "Epoch %d: train loss %f 153 0.35868411097261643\n",
      "Epoch 153: val loss 0.904066\n",
      "\n",
      "Epoch %d: train loss %f 154 0.31643058525191414\n",
      "Epoch 154: val loss 0.908443\n",
      "\n",
      "Epoch %d: train loss %f 155 0.3173985580603282\n",
      "Epoch 155: val loss 0.885484\n",
      "\n",
      "Epoch %d: train loss %f 156 0.3205756064918306\n",
      "Epoch 156: val loss 0.942333\n",
      "\n",
      "Epoch %d: train loss %f 157 0.3140755610333549\n",
      "Epoch 157: val loss 0.880591\n",
      "\n",
      "Epoch %d: train loss %f 158 0.29731209410561454\n",
      "Epoch 158: val loss 0.911978\n",
      "\n",
      "Epoch %d: train loss %f 159 0.32643797165817684\n",
      "Epoch 159: val loss 0.957251\n",
      "\n",
      "Epoch %d: train loss %f 160 0.29579997062683105\n",
      "Epoch 160: val loss 0.907451\n",
      "\n",
      "Epoch %d: train loss %f 161 0.29970131980048287\n",
      "Epoch 161: val loss 0.930104\n",
      "\n",
      "Epoch %d: train loss %f 162 0.31289058261447483\n",
      "Epoch 162: val loss 0.932791\n",
      "\n",
      "Epoch %d: train loss %f 163 0.3268913726011912\n",
      "Epoch 163: val loss 1.000042\n",
      "\n",
      "Epoch %d: train loss %f 164 0.32077814473046196\n",
      "Epoch 164: val loss 0.960568\n",
      "\n",
      "Epoch %d: train loss %f 165 0.31426315506299335\n",
      "Epoch 165: val loss 0.925453\n",
      "\n",
      "Epoch %d: train loss %f 166 0.30909216072824264\n",
      "Epoch 166: val loss 0.923824\n",
      "\n",
      "Epoch %d: train loss %f 167 0.29020017551051247\n",
      "Epoch 167: val loss 0.984957\n",
      "\n",
      "Epoch %d: train loss %f 168 0.36930691368050045\n",
      "Epoch 168: val loss 0.867279\n",
      "\n",
      "Epoch %d: train loss %f 169 0.3442518247498406\n",
      "Epoch 169: val loss 0.966012\n",
      "\n",
      "Epoch %d: train loss %f 170 0.2804141193628311\n",
      "Epoch 170: val loss 0.914822\n",
      "\n",
      "Epoch %d: train loss %f 171 0.29402654949161744\n",
      "Epoch 171: val loss 0.892103\n",
      "\n",
      "Epoch %d: train loss %f 172 0.32410188515981037\n",
      "Epoch 172: val loss 0.926415\n",
      "\n",
      "Epoch %d: train loss %f 173 0.32790958550241256\n",
      "Epoch 173: val loss 0.932171\n",
      "\n",
      "Epoch %d: train loss %f 174 0.30541350113021004\n",
      "Epoch 174: val loss 0.967030\n",
      "\n",
      "Epoch %d: train loss %f 175 0.3109521766503652\n",
      "Epoch 175: val loss 0.954250\n",
      "\n",
      "Epoch %d: train loss %f 176 0.30278581546412575\n",
      "Epoch 176: val loss 0.914626\n",
      "\n",
      "Epoch %d: train loss %f 177 0.2983349793487125\n",
      "Epoch 177: val loss 0.934135\n",
      "\n",
      "Epoch %d: train loss %f 178 0.27169911232259536\n",
      "Epoch 178: val loss 0.922985\n",
      "\n",
      "Epoch %d: train loss %f 179 0.32521288262473214\n",
      "Epoch 179: val loss 0.890074\n",
      "\n",
      "Epoch %d: train loss %f 180 0.33591967821121216\n",
      "Epoch 180: val loss 0.940128\n",
      "\n",
      "Epoch %d: train loss %f 181 0.264957446191046\n",
      "Epoch 181: val loss 1.002537\n",
      "\n",
      "Epoch %d: train loss %f 182 0.3145390252272288\n",
      "Epoch 182: val loss 0.976403\n",
      "\n",
      "Epoch %d: train loss %f 183 0.2430687662627962\n",
      "Epoch 183: val loss 0.934257\n",
      "\n",
      "Epoch %d: train loss %f 184 0.2977552115917206\n",
      "Epoch 184: val loss 1.027159\n",
      "\n",
      "Epoch %d: train loss %f 185 0.3328793661461936\n",
      "Epoch 185: val loss 0.950162\n",
      "\n",
      "Epoch %d: train loss %f 186 0.2752769754992591\n",
      "Epoch 186: val loss 0.911128\n",
      "\n",
      "Epoch %d: train loss %f 187 0.27920736703607774\n",
      "Epoch 187: val loss 1.014801\n",
      "\n",
      "Epoch %d: train loss %f 188 0.28840791516833836\n",
      "Epoch 188: val loss 1.005739\n",
      "\n",
      "Epoch %d: train loss %f 189 0.33620034489366746\n",
      "Epoch 189: val loss 0.953894\n",
      "\n",
      "Epoch %d: train loss %f 190 0.28025117847654557\n",
      "Epoch 190: val loss 1.003013\n",
      "\n",
      "Epoch %d: train loss %f 191 0.28560220532947117\n",
      "Epoch 191: val loss 1.013432\n",
      "\n",
      "Epoch %d: train loss %f 192 0.2887367457151413\n",
      "Epoch 192: val loss 0.992491\n",
      "\n",
      "Epoch %d: train loss %f 193 0.2864132672548294\n",
      "Epoch 193: val loss 0.930802\n",
      "\n",
      "Epoch %d: train loss %f 194 0.286228198144171\n",
      "Epoch 194: val loss 0.980986\n",
      "\n",
      "Epoch %d: train loss %f 195 0.30354144672552746\n",
      "Epoch 195: val loss 0.957026\n",
      "\n",
      "Epoch %d: train loss %f 196 0.246485758986738\n",
      "Epoch 196: val loss 1.012478\n",
      "\n",
      "Epoch %d: train loss %f 197 0.33845659097035724\n",
      "Epoch 197: val loss 0.978898\n",
      "\n",
      "Epoch %d: train loss %f 198 0.31276767949263257\n",
      "Epoch 198: val loss 0.922743\n",
      "\n",
      "Epoch %d: train loss %f 199 0.3055208358499739\n",
      "Epoch 199: val loss 1.005283\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6966273585955302\n",
      "Epoch 0: val loss 0.699337\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6925746003786722\n",
      "Epoch 1: val loss 0.697331\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6811107330852084\n",
      "Epoch 2: val loss 0.694859\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6779960460133023\n",
      "Epoch 3: val loss 0.691192\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6575208107630411\n",
      "Epoch 4: val loss 0.687306\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6480533613098992\n",
      "Epoch 5: val loss 0.681038\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6341120534472995\n",
      "Epoch 6: val loss 0.666003\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6156322492493523\n",
      "Epoch 7: val loss 0.653574\n",
      "\n",
      "Epoch %d: train loss %f 8 0.6099011699358622\n",
      "Epoch 8: val loss 0.627283\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5981378356615702\n",
      "Epoch 9: val loss 0.599662\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5931715501679314\n",
      "Epoch 10: val loss 0.600855\n",
      "\n",
      "Epoch %d: train loss %f 11 0.6007131801711189\n",
      "Epoch 11: val loss 0.597566\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5691961910989549\n",
      "Epoch 12: val loss 0.621044\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5741385916868845\n",
      "Epoch 13: val loss 0.610753\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5689065588845147\n",
      "Epoch 14: val loss 0.579009\n",
      "\n",
      "Epoch %d: train loss %f 15 0.547565930419498\n",
      "Epoch 15: val loss 0.651736\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5748169157240126\n",
      "Epoch 16: val loss 0.600300\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5580904616249932\n",
      "Epoch 17: val loss 0.586383\n",
      "\n",
      "Epoch %d: train loss %f 18 0.545585165421168\n",
      "Epoch 18: val loss 0.655095\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5705656276808845\n",
      "Epoch 19: val loss 0.637823\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5520649055639902\n",
      "Epoch 20: val loss 0.580786\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5410228007369571\n",
      "Epoch 21: val loss 0.638501\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5437435309092203\n",
      "Epoch 22: val loss 0.617159\n",
      "\n",
      "Epoch %d: train loss %f 23 0.5371939573023055\n",
      "Epoch 23: val loss 0.589646\n",
      "\n",
      "Epoch %d: train loss %f 24 0.5408025019698672\n",
      "Epoch 24: val loss 0.626301\n",
      "\n",
      "Epoch %d: train loss %f 25 0.5437218646208445\n",
      "Epoch 25: val loss 0.603336\n",
      "\n",
      "Epoch %d: train loss %f 26 0.535841123925315\n",
      "Epoch 26: val loss 0.603829\n",
      "\n",
      "Epoch %d: train loss %f 27 0.5058275858561198\n",
      "Epoch 27: val loss 0.618009\n",
      "\n",
      "Epoch %d: train loss %f 28 0.5293349756134881\n",
      "Epoch 28: val loss 0.605080\n",
      "\n",
      "Epoch %d: train loss %f 29 0.5243938863277435\n",
      "Epoch 29: val loss 0.631928\n",
      "\n",
      "Epoch %d: train loss %f 30 0.5315406653616164\n",
      "Epoch 30: val loss 0.573089\n",
      "\n",
      "Epoch %d: train loss %f 31 0.5311510894033644\n",
      "Epoch 31: val loss 0.579667\n",
      "\n",
      "Epoch %d: train loss %f 32 0.5247315731313493\n",
      "Epoch 32: val loss 0.596395\n",
      "\n",
      "Epoch %d: train loss %f 33 0.5242836541599698\n",
      "Epoch 33: val loss 0.606741\n",
      "\n",
      "Epoch %d: train loss %f 34 0.5279412766297659\n",
      "Epoch 34: val loss 0.586054\n",
      "\n",
      "Epoch %d: train loss %f 35 0.5282294286621941\n",
      "Epoch 35: val loss 0.616914\n",
      "\n",
      "Epoch %d: train loss %f 36 0.5170479085710313\n",
      "Epoch 36: val loss 0.589032\n",
      "\n",
      "Epoch %d: train loss %f 37 0.5183877713150449\n",
      "Epoch 37: val loss 0.630919\n",
      "\n",
      "Epoch %d: train loss %f 38 0.5208826164404551\n",
      "Epoch 38: val loss 0.633013\n",
      "\n",
      "Epoch %d: train loss %f 39 0.503027511967553\n",
      "Epoch 39: val loss 0.592655\n",
      "\n",
      "Epoch %d: train loss %f 40 0.5319763653808169\n",
      "Epoch 40: val loss 0.593997\n",
      "\n",
      "Epoch %d: train loss %f 41 0.5213397675090365\n",
      "Epoch 41: val loss 0.593846\n",
      "\n",
      "Epoch %d: train loss %f 42 0.510481466849645\n",
      "Epoch 42: val loss 0.574395\n",
      "\n",
      "Epoch %d: train loss %f 43 0.5332333114412096\n",
      "Epoch 43: val loss 0.590811\n",
      "\n",
      "Epoch %d: train loss %f 44 0.48458171221945023\n",
      "Epoch 44: val loss 0.595642\n",
      "\n",
      "Epoch %d: train loss %f 45 0.4697071380085415\n",
      "Epoch 45: val loss 0.619300\n",
      "\n",
      "Epoch %d: train loss %f 46 0.500460320048862\n",
      "Epoch 46: val loss 0.588748\n",
      "\n",
      "Epoch %d: train loss %f 47 0.5039225021998087\n",
      "Epoch 47: val loss 0.583156\n",
      "\n",
      "Epoch %d: train loss %f 48 0.5088334580262502\n",
      "Epoch 48: val loss 0.593062\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4882900317509969\n",
      "Epoch 49: val loss 0.579055\n",
      "\n",
      "Epoch %d: train loss %f 50 0.4636206030845642\n",
      "Epoch 50: val loss 0.566318\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4914134111669328\n",
      "Epoch 51: val loss 0.575114\n",
      "\n",
      "Epoch %d: train loss %f 52 0.4968536463048723\n",
      "Epoch 52: val loss 0.585733\n",
      "\n",
      "Epoch %d: train loss %f 53 0.4797240032090081\n",
      "Epoch 53: val loss 0.583003\n",
      "\n",
      "Epoch %d: train loss %f 54 0.4882762200302548\n",
      "Epoch 54: val loss 0.585533\n",
      "\n",
      "Epoch %d: train loss %f 55 0.5029813249905905\n",
      "Epoch 55: val loss 0.579641\n",
      "\n",
      "Epoch %d: train loss %f 56 0.47792049911287093\n",
      "Epoch 56: val loss 0.581095\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4806061089038849\n",
      "Epoch 57: val loss 0.582887\n",
      "\n",
      "Epoch %d: train loss %f 58 0.4592415127489302\n",
      "Epoch 58: val loss 0.575214\n",
      "\n",
      "Epoch %d: train loss %f 59 0.4893929594092899\n",
      "Epoch 59: val loss 0.655927\n",
      "\n",
      "Epoch %d: train loss %f 60 0.45571230683061814\n",
      "Epoch 60: val loss 0.571169\n",
      "\n",
      "Epoch %d: train loss %f 61 0.5025711125797696\n",
      "Epoch 61: val loss 0.626636\n",
      "\n",
      "Epoch %d: train loss %f 62 0.4819645815425449\n",
      "Epoch 62: val loss 0.592898\n",
      "\n",
      "Epoch %d: train loss %f 63 0.4808942543135749\n",
      "Epoch 63: val loss 0.573459\n",
      "\n",
      "Epoch %d: train loss %f 64 0.5276848243342506\n",
      "Epoch 64: val loss 0.588918\n",
      "\n",
      "Epoch %d: train loss %f 65 0.4484728342956967\n",
      "Epoch 65: val loss 0.575823\n",
      "\n",
      "Epoch %d: train loss %f 66 0.4576346410645379\n",
      "Epoch 66: val loss 0.580686\n",
      "\n",
      "Epoch %d: train loss %f 67 0.46948255432976616\n",
      "Epoch 67: val loss 0.605827\n",
      "\n",
      "Epoch %d: train loss %f 68 0.47641726666026646\n",
      "Epoch 68: val loss 0.589317\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4679607351620992\n",
      "Epoch 69: val loss 0.558069\n",
      "\n",
      "Epoch %d: train loss %f 70 0.4628738893402947\n",
      "Epoch 70: val loss 0.601573\n",
      "\n",
      "Epoch %d: train loss %f 71 0.4634115133020613\n",
      "Epoch 71: val loss 0.580408\n",
      "\n",
      "Epoch %d: train loss %f 72 0.44416653778817916\n",
      "Epoch 72: val loss 0.645560\n",
      "\n",
      "Epoch %d: train loss %f 73 0.4537736276785533\n",
      "Epoch 73: val loss 0.582922\n",
      "\n",
      "Epoch %d: train loss %f 74 0.4555578960312737\n",
      "Epoch 74: val loss 0.615045\n",
      "\n",
      "Epoch %d: train loss %f 75 0.46091584033436245\n",
      "Epoch 75: val loss 0.594653\n",
      "\n",
      "Epoch %d: train loss %f 76 0.42037059863408405\n",
      "Epoch 76: val loss 0.592002\n",
      "\n",
      "Epoch %d: train loss %f 77 0.43477657106187606\n",
      "Epoch 77: val loss 0.585318\n",
      "\n",
      "Epoch %d: train loss %f 78 0.41471516754892135\n",
      "Epoch 78: val loss 0.613672\n",
      "\n",
      "Epoch %d: train loss %f 79 0.48439845111634994\n",
      "Epoch 79: val loss 0.605659\n",
      "\n",
      "Epoch %d: train loss %f 80 0.4780407514837053\n",
      "Epoch 80: val loss 0.621550\n",
      "\n",
      "Epoch %d: train loss %f 81 0.4537729885843065\n",
      "Epoch 81: val loss 0.604700\n",
      "\n",
      "Epoch %d: train loss %f 82 0.422481420967314\n",
      "Epoch 82: val loss 0.578514\n",
      "\n",
      "Epoch %d: train loss %f 83 0.4287820557753245\n",
      "Epoch 83: val loss 0.589772\n",
      "\n",
      "Epoch %d: train loss %f 84 0.45126113957828945\n",
      "Epoch 84: val loss 0.629741\n",
      "\n",
      "Epoch %d: train loss %f 85 0.4664482639895545\n",
      "Epoch 85: val loss 0.618457\n",
      "\n",
      "Epoch %d: train loss %f 86 0.4221569961971707\n",
      "Epoch 86: val loss 0.571920\n",
      "\n",
      "Epoch %d: train loss %f 87 0.45516616106033325\n",
      "Epoch 87: val loss 0.612425\n",
      "\n",
      "Epoch %d: train loss %f 88 0.4278424945142534\n",
      "Epoch 88: val loss 0.604462\n",
      "\n",
      "Epoch %d: train loss %f 89 0.41473083363638985\n",
      "Epoch 89: val loss 0.607841\n",
      "\n",
      "Epoch %d: train loss %f 90 0.4678329957856072\n",
      "Epoch 90: val loss 0.587864\n",
      "\n",
      "Epoch %d: train loss %f 91 0.4230121456914478\n",
      "Epoch 91: val loss 0.575964\n",
      "\n",
      "Epoch %d: train loss %f 92 0.4289603696929084\n",
      "Epoch 92: val loss 0.608879\n",
      "\n",
      "Epoch %d: train loss %f 93 0.4126058253977034\n",
      "Epoch 93: val loss 0.653964\n",
      "\n",
      "Epoch %d: train loss %f 94 0.4165107078022427\n",
      "Epoch 94: val loss 0.614182\n",
      "\n",
      "Epoch %d: train loss %f 95 0.4810352192984687\n",
      "Epoch 95: val loss 0.636710\n",
      "\n",
      "Epoch %d: train loss %f 96 0.4665765364964803\n",
      "Epoch 96: val loss 0.612070\n",
      "\n",
      "Epoch %d: train loss %f 97 0.41877948575549656\n",
      "Epoch 97: val loss 0.641804\n",
      "\n",
      "Epoch %d: train loss %f 98 0.42726285921202767\n",
      "Epoch 98: val loss 0.581279\n",
      "\n",
      "Epoch %d: train loss %f 99 0.4472503496540917\n",
      "Epoch 99: val loss 0.547269\n",
      "\n",
      "Epoch %d: train loss %f 100 0.41208676166004604\n",
      "Epoch 100: val loss 0.599860\n",
      "\n",
      "Epoch %d: train loss %f 101 0.41844939688841504\n",
      "Epoch 101: val loss 0.643379\n",
      "\n",
      "Epoch %d: train loss %f 102 0.4038138853179084\n",
      "Epoch 102: val loss 0.600751\n",
      "\n",
      "Epoch %d: train loss %f 103 0.42615678906440735\n",
      "Epoch 103: val loss 0.597024\n",
      "\n",
      "Epoch %d: train loss %f 104 0.41913721296522355\n",
      "Epoch 104: val loss 0.599267\n",
      "\n",
      "Epoch %d: train loss %f 105 0.4126342833042145\n",
      "Epoch 105: val loss 0.580137\n",
      "\n",
      "Epoch %d: train loss %f 106 0.38053343031141496\n",
      "Epoch 106: val loss 0.581292\n",
      "\n",
      "Epoch %d: train loss %f 107 0.4403841859764523\n",
      "Epoch 107: val loss 0.656601\n",
      "\n",
      "Epoch %d: train loss %f 108 0.4028009921312332\n",
      "Epoch 108: val loss 0.596689\n",
      "\n",
      "Epoch %d: train loss %f 109 0.40189574162165326\n",
      "Epoch 109: val loss 0.611040\n",
      "\n",
      "Epoch %d: train loss %f 110 0.427700721555286\n",
      "Epoch 110: val loss 0.629145\n",
      "\n",
      "Epoch %d: train loss %f 111 0.4053956965605418\n",
      "Epoch 111: val loss 0.595325\n",
      "\n",
      "Epoch %d: train loss %f 112 0.4179710977607303\n",
      "Epoch 112: val loss 0.655502\n",
      "\n",
      "Epoch %d: train loss %f 113 0.39797653092278373\n",
      "Epoch 113: val loss 0.585648\n",
      "\n",
      "Epoch %d: train loss %f 114 0.40189681119389004\n",
      "Epoch 114: val loss 0.662381\n",
      "\n",
      "Epoch %d: train loss %f 115 0.411011709107293\n",
      "Epoch 115: val loss 0.602070\n",
      "\n",
      "Epoch %d: train loss %f 116 0.39178549581103855\n",
      "Epoch 116: val loss 0.626745\n",
      "\n",
      "Epoch %d: train loss %f 117 0.4346104429827796\n",
      "Epoch 117: val loss 0.597591\n",
      "\n",
      "Epoch %d: train loss %f 118 0.4107403986983829\n",
      "Epoch 118: val loss 0.623849\n",
      "\n",
      "Epoch %d: train loss %f 119 0.39788320495022667\n",
      "Epoch 119: val loss 0.636511\n",
      "\n",
      "Epoch %d: train loss %f 120 0.39088081154558396\n",
      "Epoch 120: val loss 0.637551\n",
      "\n",
      "Epoch %d: train loss %f 121 0.394407053788503\n",
      "Epoch 121: val loss 0.621127\n",
      "\n",
      "Epoch %d: train loss %f 122 0.3890367845694224\n",
      "Epoch 122: val loss 0.685280\n",
      "\n",
      "Epoch %d: train loss %f 123 0.38963737421565586\n",
      "Epoch 123: val loss 0.589651\n",
      "\n",
      "Epoch %d: train loss %f 124 0.4206295659144719\n",
      "Epoch 124: val loss 0.593479\n",
      "\n",
      "Epoch %d: train loss %f 125 0.38598810301886666\n",
      "Epoch 125: val loss 0.637140\n",
      "\n",
      "Epoch %d: train loss %f 126 0.36584333413177067\n",
      "Epoch 126: val loss 0.629456\n",
      "\n",
      "Epoch %d: train loss %f 127 0.38929104142718846\n",
      "Epoch 127: val loss 0.639046\n",
      "\n",
      "Epoch %d: train loss %f 128 0.3929666578769684\n",
      "Epoch 128: val loss 0.619878\n",
      "\n",
      "Epoch %d: train loss %f 129 0.3891012867291768\n",
      "Epoch 129: val loss 0.628105\n",
      "\n",
      "Epoch %d: train loss %f 130 0.3434556970993678\n",
      "Epoch 130: val loss 0.607831\n",
      "\n",
      "Epoch %d: train loss %f 131 0.38410311109489864\n",
      "Epoch 131: val loss 0.613462\n",
      "\n",
      "Epoch %d: train loss %f 132 0.36665388610627914\n",
      "Epoch 132: val loss 0.637737\n",
      "\n",
      "Epoch %d: train loss %f 133 0.39033536116282147\n",
      "Epoch 133: val loss 0.653063\n",
      "\n",
      "Epoch %d: train loss %f 134 0.379669177863333\n",
      "Epoch 134: val loss 0.649514\n",
      "\n",
      "Epoch %d: train loss %f 135 0.3497479491763645\n",
      "Epoch 135: val loss 0.634999\n",
      "\n",
      "Epoch %d: train loss %f 136 0.34181488553682965\n",
      "Epoch 136: val loss 0.638171\n",
      "\n",
      "Epoch %d: train loss %f 137 0.36533647278944653\n",
      "Epoch 137: val loss 0.633700\n",
      "\n",
      "Epoch %d: train loss %f 138 0.3569929748773575\n",
      "Epoch 138: val loss 0.646768\n",
      "\n",
      "Epoch %d: train loss %f 139 0.33334484365251327\n",
      "Epoch 139: val loss 0.680910\n",
      "\n",
      "Epoch %d: train loss %f 140 0.3646348880396949\n",
      "Epoch 140: val loss 0.626498\n",
      "\n",
      "Epoch %d: train loss %f 141 0.36382944881916046\n",
      "Epoch 141: val loss 0.621207\n",
      "\n",
      "Epoch %d: train loss %f 142 0.38780221343040466\n",
      "Epoch 142: val loss 0.628423\n",
      "\n",
      "Epoch %d: train loss %f 143 0.39396043287383187\n",
      "Epoch 143: val loss 0.680804\n",
      "\n",
      "Epoch %d: train loss %f 144 0.3910085956255595\n",
      "Epoch 144: val loss 0.667013\n",
      "\n",
      "Epoch %d: train loss %f 145 0.3469695912467109\n",
      "Epoch 145: val loss 0.682688\n",
      "\n",
      "Epoch %d: train loss %f 146 0.392206781440311\n",
      "Epoch 146: val loss 0.646949\n",
      "\n",
      "Epoch %d: train loss %f 147 0.36076802180873024\n",
      "Epoch 147: val loss 0.662814\n",
      "\n",
      "Epoch %d: train loss %f 148 0.3539893726507823\n",
      "Epoch 148: val loss 0.600927\n",
      "\n",
      "Epoch %d: train loss %f 149 0.4095024996333652\n",
      "Epoch 149: val loss 0.734287\n",
      "\n",
      "Epoch %d: train loss %f 150 0.376287783185641\n",
      "Epoch 150: val loss 0.649832\n",
      "\n",
      "Epoch %d: train loss %f 151 0.3829776843388875\n",
      "Epoch 151: val loss 0.687164\n",
      "\n",
      "Epoch %d: train loss %f 152 0.38653795255555046\n",
      "Epoch 152: val loss 0.671974\n",
      "\n",
      "Epoch %d: train loss %f 153 0.3592605276240243\n",
      "Epoch 153: val loss 0.655101\n",
      "\n",
      "Epoch %d: train loss %f 154 0.36956370373566944\n",
      "Epoch 154: val loss 0.674748\n",
      "\n",
      "Epoch %d: train loss %f 155 0.3658503443002701\n",
      "Epoch 155: val loss 0.704997\n",
      "\n",
      "Epoch %d: train loss %f 156 0.3349754313627879\n",
      "Epoch 156: val loss 0.664062\n",
      "\n",
      "Epoch %d: train loss %f 157 0.32376693354712593\n",
      "Epoch 157: val loss 0.724513\n",
      "\n",
      "Epoch %d: train loss %f 158 0.36041293210453457\n",
      "Epoch 158: val loss 0.681234\n",
      "\n",
      "Epoch %d: train loss %f 159 0.3603383418586519\n",
      "Epoch 159: val loss 0.673416\n",
      "\n",
      "Epoch %d: train loss %f 160 0.35941770010524327\n",
      "Epoch 160: val loss 0.625309\n",
      "\n",
      "Epoch %d: train loss %f 161 0.3691467179192437\n",
      "Epoch 161: val loss 0.738355\n",
      "\n",
      "Epoch %d: train loss %f 162 0.3808680127064387\n",
      "Epoch 162: val loss 0.680925\n",
      "\n",
      "Epoch %d: train loss %f 163 0.341383684012625\n",
      "Epoch 163: val loss 0.766718\n",
      "\n",
      "Epoch %d: train loss %f 164 0.36249810953934986\n",
      "Epoch 164: val loss 0.671915\n",
      "\n",
      "Epoch %d: train loss %f 165 0.36557303534613717\n",
      "Epoch 165: val loss 0.701525\n",
      "\n",
      "Epoch %d: train loss %f 166 0.3293360455168618\n",
      "Epoch 166: val loss 0.676439\n",
      "\n",
      "Epoch %d: train loss %f 167 0.3738395637936062\n",
      "Epoch 167: val loss 0.686788\n",
      "\n",
      "Epoch %d: train loss %f 168 0.36833852529525757\n",
      "Epoch 168: val loss 0.717301\n",
      "\n",
      "Epoch %d: train loss %f 169 0.3332766940196355\n",
      "Epoch 169: val loss 0.649878\n",
      "\n",
      "Epoch %d: train loss %f 170 0.36212291154596543\n",
      "Epoch 170: val loss 0.715928\n",
      "\n",
      "Epoch %d: train loss %f 171 0.34379928807417554\n",
      "Epoch 171: val loss 0.681515\n",
      "\n",
      "Epoch %d: train loss %f 172 0.32724932332833606\n",
      "Epoch 172: val loss 0.655274\n",
      "\n",
      "Epoch %d: train loss %f 173 0.3518499467107985\n",
      "Epoch 173: val loss 0.727551\n",
      "\n",
      "Epoch %d: train loss %f 174 0.376356561978658\n",
      "Epoch 174: val loss 0.720150\n",
      "\n",
      "Epoch %d: train loss %f 175 0.3234095291958915\n",
      "Epoch 175: val loss 0.749616\n",
      "\n",
      "Epoch %d: train loss %f 176 0.35068939129511517\n",
      "Epoch 176: val loss 0.666962\n",
      "\n",
      "Epoch %d: train loss %f 177 0.37862444420655567\n",
      "Epoch 177: val loss 0.652764\n",
      "\n",
      "Epoch %d: train loss %f 178 0.3719698074791167\n",
      "Epoch 178: val loss 0.687845\n",
      "\n",
      "Epoch %d: train loss %f 179 0.34630165497461957\n",
      "Epoch 179: val loss 0.695942\n",
      "\n",
      "Epoch %d: train loss %f 180 0.3146432406372494\n",
      "Epoch 180: val loss 0.708944\n",
      "\n",
      "Epoch %d: train loss %f 181 0.36619864569769967\n",
      "Epoch 181: val loss 0.739857\n",
      "\n",
      "Epoch %d: train loss %f 182 0.34064484139283496\n",
      "Epoch 182: val loss 0.665132\n",
      "\n",
      "Epoch %d: train loss %f 183 0.3631465501255459\n",
      "Epoch 183: val loss 0.731792\n",
      "\n",
      "Epoch %d: train loss %f 184 0.3574255059162776\n",
      "Epoch 184: val loss 0.735946\n",
      "\n",
      "Epoch %d: train loss %f 185 0.37091205683019424\n",
      "Epoch 185: val loss 0.711678\n",
      "\n",
      "Epoch %d: train loss %f 186 0.3127633167637719\n",
      "Epoch 186: val loss 0.707053\n",
      "\n",
      "Epoch %d: train loss %f 187 0.3077353239059448\n",
      "Epoch 187: val loss 0.691243\n",
      "\n",
      "Epoch %d: train loss %f 188 0.3519898189438714\n",
      "Epoch 188: val loss 0.651447\n",
      "\n",
      "Epoch %d: train loss %f 189 0.33890989422798157\n",
      "Epoch 189: val loss 0.678243\n",
      "\n",
      "Epoch %d: train loss %f 190 0.3528745240635342\n",
      "Epoch 190: val loss 0.711187\n",
      "\n",
      "Epoch %d: train loss %f 191 0.39683152900801766\n",
      "Epoch 191: val loss 0.766456\n",
      "\n",
      "Epoch %d: train loss %f 192 0.2957381506760915\n",
      "Epoch 192: val loss 0.714298\n",
      "\n",
      "Epoch %d: train loss %f 193 0.3750797212123871\n",
      "Epoch 193: val loss 0.709970\n",
      "\n",
      "Epoch %d: train loss %f 194 0.3717828061845567\n",
      "Epoch 194: val loss 0.720425\n",
      "\n",
      "Epoch %d: train loss %f 195 0.3459711770216624\n",
      "Epoch 195: val loss 0.720700\n",
      "\n",
      "Epoch %d: train loss %f 196 0.328913829392857\n",
      "Epoch 196: val loss 0.725865\n",
      "\n",
      "Epoch %d: train loss %f 197 0.368640778793229\n",
      "Epoch 197: val loss 0.716033\n",
      "\n",
      "Epoch %d: train loss %f 198 0.32592008511225384\n",
      "Epoch 198: val loss 0.797765\n",
      "\n",
      "Epoch %d: train loss %f 199 0.34116024937894607\n",
      "Epoch 199: val loss 0.756705\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6892189118597243\n",
      "Epoch 0: val loss 0.689956\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6859876447253757\n",
      "Epoch 1: val loss 0.688005\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6828268104129367\n",
      "Epoch 2: val loss 0.684964\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6694031291537814\n",
      "Epoch 3: val loss 0.680771\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6618408030933804\n",
      "Epoch 4: val loss 0.676192\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6476535929573907\n",
      "Epoch 5: val loss 0.674986\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6340071890089247\n",
      "Epoch 6: val loss 0.672419\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6273376080724928\n",
      "Epoch 7: val loss 0.668976\n",
      "\n",
      "Epoch %d: train loss %f 8 0.612799863020579\n",
      "Epoch 8: val loss 0.657602\n",
      "\n",
      "Epoch %d: train loss %f 9 0.6094562278853523\n",
      "Epoch 9: val loss 0.638777\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5942460099856058\n",
      "Epoch 10: val loss 0.636603\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5692912770642174\n",
      "Epoch 11: val loss 0.626656\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5750975476370918\n",
      "Epoch 12: val loss 0.662751\n",
      "\n",
      "Epoch %d: train loss %f 13 0.559799505604638\n",
      "Epoch 13: val loss 0.624554\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5361269546879662\n",
      "Epoch 14: val loss 0.634375\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5487533840868208\n",
      "Epoch 15: val loss 0.634826\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5224762194686465\n",
      "Epoch 16: val loss 0.638840\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5280145704746246\n",
      "Epoch 17: val loss 0.654020\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5137356552812788\n",
      "Epoch 18: val loss 0.633798\n",
      "\n",
      "Epoch %d: train loss %f 19 0.4893852273623149\n",
      "Epoch 19: val loss 0.648159\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5065750082333883\n",
      "Epoch 20: val loss 0.660290\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5237946576542325\n",
      "Epoch 21: val loss 0.675018\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5347131954299079\n",
      "Epoch 22: val loss 0.634308\n",
      "\n",
      "Epoch %d: train loss %f 23 0.5010347730583615\n",
      "Epoch 23: val loss 0.688274\n",
      "\n",
      "Epoch %d: train loss %f 24 0.5228935049639808\n",
      "Epoch 24: val loss 0.641307\n",
      "\n",
      "Epoch %d: train loss %f 25 0.48264744877815247\n",
      "Epoch 25: val loss 0.660218\n",
      "\n",
      "Epoch %d: train loss %f 26 0.49644915593994987\n",
      "Epoch 26: val loss 0.686421\n",
      "\n",
      "Epoch %d: train loss %f 27 0.4957520564397176\n",
      "Epoch 27: val loss 0.647352\n",
      "\n",
      "Epoch %d: train loss %f 28 0.5121244390805563\n",
      "Epoch 28: val loss 0.676375\n",
      "\n",
      "Epoch %d: train loss %f 29 0.5032597184181213\n",
      "Epoch 29: val loss 0.656078\n",
      "\n",
      "Epoch %d: train loss %f 30 0.512793607181973\n",
      "Epoch 30: val loss 0.649120\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4756939974096086\n",
      "Epoch 31: val loss 0.660779\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4758513967196147\n",
      "Epoch 32: val loss 0.670838\n",
      "\n",
      "Epoch %d: train loss %f 33 0.4768654571639167\n",
      "Epoch 33: val loss 0.682569\n",
      "\n",
      "Epoch %d: train loss %f 34 0.5001550979084439\n",
      "Epoch 34: val loss 0.660393\n",
      "\n",
      "Epoch %d: train loss %f 35 0.48627350727717084\n",
      "Epoch 35: val loss 0.692171\n",
      "\n",
      "Epoch %d: train loss %f 36 0.47778547472423977\n",
      "Epoch 36: val loss 0.674780\n",
      "\n",
      "Epoch %d: train loss %f 37 0.5106728010707431\n",
      "Epoch 37: val loss 0.656914\n",
      "\n",
      "Epoch %d: train loss %f 38 0.4694484836525387\n",
      "Epoch 38: val loss 0.668631\n",
      "\n",
      "Epoch %d: train loss %f 39 0.49090975522994995\n",
      "Epoch 39: val loss 0.680912\n",
      "\n",
      "Epoch %d: train loss %f 40 0.460571782456504\n",
      "Epoch 40: val loss 0.664233\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4836117625236511\n",
      "Epoch 41: val loss 0.674928\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4659733639823066\n",
      "Epoch 42: val loss 0.692413\n",
      "\n",
      "Epoch %d: train loss %f 43 0.45816337400012547\n",
      "Epoch 43: val loss 0.726519\n",
      "\n",
      "Epoch %d: train loss %f 44 0.477792802784178\n",
      "Epoch 44: val loss 0.703257\n",
      "\n",
      "Epoch %d: train loss %f 45 0.467685600121816\n",
      "Epoch 45: val loss 0.707088\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4493604534202152\n",
      "Epoch 46: val loss 0.695975\n",
      "\n",
      "Epoch %d: train loss %f 47 0.46834687723053825\n",
      "Epoch 47: val loss 0.688038\n",
      "\n",
      "Epoch %d: train loss %f 48 0.46250121461020577\n",
      "Epoch 48: val loss 0.716081\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4697442054748535\n",
      "Epoch 49: val loss 0.706266\n",
      "\n",
      "Epoch %d: train loss %f 50 0.44216299388143754\n",
      "Epoch 50: val loss 0.711092\n",
      "\n",
      "Epoch %d: train loss %f 51 0.44682692488034564\n",
      "Epoch 51: val loss 0.716761\n",
      "\n",
      "Epoch %d: train loss %f 52 0.44267095790969\n",
      "Epoch 52: val loss 0.734833\n",
      "\n",
      "Epoch %d: train loss %f 53 0.4457702222797606\n",
      "Epoch 53: val loss 0.720381\n",
      "\n",
      "Epoch %d: train loss %f 54 0.4349035885598924\n",
      "Epoch 54: val loss 0.737232\n",
      "\n",
      "Epoch %d: train loss %f 55 0.4519726534684499\n",
      "Epoch 55: val loss 0.730342\n",
      "\n",
      "Epoch %d: train loss %f 56 0.45859156714545357\n",
      "Epoch 56: val loss 0.704761\n",
      "\n",
      "Epoch %d: train loss %f 57 0.43438514404826695\n",
      "Epoch 57: val loss 0.703889\n",
      "\n",
      "Epoch %d: train loss %f 58 0.4389892750316196\n",
      "Epoch 58: val loss 0.716231\n",
      "\n",
      "Epoch %d: train loss %f 59 0.4515331983566284\n",
      "Epoch 59: val loss 0.712004\n",
      "\n",
      "Epoch %d: train loss %f 60 0.4168711122539308\n",
      "Epoch 60: val loss 0.771019\n",
      "\n",
      "Epoch %d: train loss %f 61 0.46741106112798053\n",
      "Epoch 61: val loss 0.772559\n",
      "\n",
      "Epoch %d: train loss %f 62 0.41689764459927875\n",
      "Epoch 62: val loss 0.737951\n",
      "\n",
      "Epoch %d: train loss %f 63 0.4621165162987179\n",
      "Epoch 63: val loss 0.729548\n",
      "\n",
      "Epoch %d: train loss %f 64 0.42949318554666305\n",
      "Epoch 64: val loss 0.709470\n",
      "\n",
      "Epoch %d: train loss %f 65 0.43941815031899345\n",
      "Epoch 65: val loss 0.751579\n",
      "\n",
      "Epoch %d: train loss %f 66 0.4092659486664666\n",
      "Epoch 66: val loss 0.720016\n",
      "\n",
      "Epoch %d: train loss %f 67 0.4282177686691284\n",
      "Epoch 67: val loss 0.804905\n",
      "\n",
      "Epoch %d: train loss %f 68 0.4242935942278968\n",
      "Epoch 68: val loss 0.789641\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4315107564131419\n",
      "Epoch 69: val loss 0.786737\n",
      "\n",
      "Epoch %d: train loss %f 70 0.39438987109396195\n",
      "Epoch 70: val loss 0.764452\n",
      "\n",
      "Epoch %d: train loss %f 71 0.40435384379492867\n",
      "Epoch 71: val loss 0.753350\n",
      "\n",
      "Epoch %d: train loss %f 72 0.42496639490127563\n",
      "Epoch 72: val loss 0.774547\n",
      "\n",
      "Epoch %d: train loss %f 73 0.44553954071468777\n",
      "Epoch 73: val loss 0.816992\n",
      "\n",
      "Epoch %d: train loss %f 74 0.39519648253917694\n",
      "Epoch 74: val loss 0.749426\n",
      "\n",
      "Epoch %d: train loss %f 75 0.453170958492491\n",
      "Epoch 75: val loss 0.799477\n",
      "\n",
      "Epoch %d: train loss %f 76 0.40516769223743015\n",
      "Epoch 76: val loss 0.744251\n",
      "\n",
      "Epoch %d: train loss %f 77 0.40048397580782574\n",
      "Epoch 77: val loss 0.789789\n",
      "\n",
      "Epoch %d: train loss %f 78 0.4319785171084934\n",
      "Epoch 78: val loss 0.755983\n",
      "\n",
      "Epoch %d: train loss %f 79 0.39964718123277027\n",
      "Epoch 79: val loss 0.805871\n",
      "\n",
      "Epoch %d: train loss %f 80 0.40035779939757454\n",
      "Epoch 80: val loss 0.780864\n",
      "\n",
      "Epoch %d: train loss %f 81 0.4498053193092346\n",
      "Epoch 81: val loss 0.786356\n",
      "\n",
      "Epoch %d: train loss %f 82 0.43263160188992816\n",
      "Epoch 82: val loss 0.794914\n",
      "\n",
      "Epoch %d: train loss %f 83 0.41329021586312187\n",
      "Epoch 83: val loss 0.798910\n",
      "\n",
      "Epoch %d: train loss %f 84 0.42183276679780746\n",
      "Epoch 84: val loss 0.754254\n",
      "\n",
      "Epoch %d: train loss %f 85 0.40989115834236145\n",
      "Epoch 85: val loss 0.786526\n",
      "\n",
      "Epoch %d: train loss %f 86 0.44932616915967727\n",
      "Epoch 86: val loss 0.759876\n",
      "\n",
      "Epoch %d: train loss %f 87 0.4204988678296407\n",
      "Epoch 87: val loss 0.776982\n",
      "\n",
      "Epoch %d: train loss %f 88 0.3609492811891768\n",
      "Epoch 88: val loss 0.732998\n",
      "\n",
      "Epoch %d: train loss %f 89 0.42086145612928605\n",
      "Epoch 89: val loss 0.823295\n",
      "\n",
      "Epoch %d: train loss %f 90 0.47568290763431126\n",
      "Epoch 90: val loss 0.756483\n",
      "\n",
      "Epoch %d: train loss %f 91 0.41988195644484627\n",
      "Epoch 91: val loss 0.766058\n",
      "\n",
      "Epoch %d: train loss %f 92 0.43791773584153915\n",
      "Epoch 92: val loss 0.782836\n",
      "\n",
      "Epoch %d: train loss %f 93 0.36902733974986607\n",
      "Epoch 93: val loss 0.727113\n",
      "\n",
      "Epoch %d: train loss %f 94 0.4097917477289836\n",
      "Epoch 94: val loss 0.778050\n",
      "\n",
      "Epoch %d: train loss %f 95 0.3634597526656257\n",
      "Epoch 95: val loss 0.810010\n",
      "\n",
      "Epoch %d: train loss %f 96 0.3937890695201026\n",
      "Epoch 96: val loss 0.784688\n",
      "\n",
      "Epoch %d: train loss %f 97 0.3946281174818675\n",
      "Epoch 97: val loss 0.802616\n",
      "\n",
      "Epoch %d: train loss %f 98 0.4070322877830929\n",
      "Epoch 98: val loss 0.806671\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3707880609565311\n",
      "Epoch 99: val loss 0.819932\n",
      "\n",
      "Epoch %d: train loss %f 100 0.37380000286632115\n",
      "Epoch 100: val loss 0.798797\n",
      "\n",
      "Epoch %d: train loss %f 101 0.41149523191981846\n",
      "Epoch 101: val loss 0.819222\n",
      "\n",
      "Epoch %d: train loss %f 102 0.40983950429492527\n",
      "Epoch 102: val loss 0.738693\n",
      "\n",
      "Epoch %d: train loss %f 103 0.43190038204193115\n",
      "Epoch 103: val loss 0.747598\n",
      "\n",
      "Epoch %d: train loss %f 104 0.3928302725156148\n",
      "Epoch 104: val loss 0.871069\n",
      "\n",
      "Epoch %d: train loss %f 105 0.38328904906908673\n",
      "Epoch 105: val loss 0.795767\n",
      "\n",
      "Epoch %d: train loss %f 106 0.3682549562719133\n",
      "Epoch 106: val loss 0.825202\n",
      "\n",
      "Epoch %d: train loss %f 107 0.38225525948736405\n",
      "Epoch 107: val loss 0.819002\n",
      "\n",
      "Epoch %d: train loss %f 108 0.39614691999223495\n",
      "Epoch 108: val loss 0.813634\n",
      "\n",
      "Epoch %d: train loss %f 109 0.3659655683570438\n",
      "Epoch 109: val loss 0.842406\n",
      "\n",
      "Epoch %d: train loss %f 110 0.35960765679677326\n",
      "Epoch 110: val loss 0.810830\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3806691699557834\n",
      "Epoch 111: val loss 0.852801\n",
      "\n",
      "Epoch %d: train loss %f 112 0.3526960478888618\n",
      "Epoch 112: val loss 0.870663\n",
      "\n",
      "Epoch %d: train loss %f 113 0.3598955687549379\n",
      "Epoch 113: val loss 0.836885\n",
      "\n",
      "Epoch %d: train loss %f 114 0.40058565470907426\n",
      "Epoch 114: val loss 0.849152\n",
      "\n",
      "Epoch %d: train loss %f 115 0.34548166228665245\n",
      "Epoch 115: val loss 0.861966\n",
      "\n",
      "Epoch %d: train loss %f 116 0.34377479553222656\n",
      "Epoch 116: val loss 0.914500\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3691704935497708\n",
      "Epoch 117: val loss 0.827655\n",
      "\n",
      "Epoch %d: train loss %f 118 0.3747697522242864\n",
      "Epoch 118: val loss 0.876330\n",
      "\n",
      "Epoch %d: train loss %f 119 0.33366257283422684\n",
      "Epoch 119: val loss 0.863821\n",
      "\n",
      "Epoch %d: train loss %f 120 0.3215523229704963\n",
      "Epoch 120: val loss 0.856079\n",
      "\n",
      "Epoch %d: train loss %f 121 0.35073360469606185\n",
      "Epoch 121: val loss 0.860305\n",
      "\n",
      "Epoch %d: train loss %f 122 0.3928884036011166\n",
      "Epoch 122: val loss 0.887923\n",
      "\n",
      "Epoch %d: train loss %f 123 0.36149246493975323\n",
      "Epoch 123: val loss 0.923235\n",
      "\n",
      "Epoch %d: train loss %f 124 0.3780171722173691\n",
      "Epoch 124: val loss 0.861989\n",
      "\n",
      "Epoch %d: train loss %f 125 0.3341519518031014\n",
      "Epoch 125: val loss 0.914224\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3552170793215434\n",
      "Epoch 126: val loss 0.881739\n",
      "\n",
      "Epoch %d: train loss %f 127 0.34090110825167763\n",
      "Epoch 127: val loss 0.899381\n",
      "\n",
      "Epoch %d: train loss %f 128 0.33792004320356583\n",
      "Epoch 128: val loss 0.874962\n",
      "\n",
      "Epoch %d: train loss %f 129 0.31746190455224776\n",
      "Epoch 129: val loss 0.956304\n",
      "\n",
      "Epoch %d: train loss %f 130 0.3559908601972792\n",
      "Epoch 130: val loss 0.931660\n",
      "\n",
      "Epoch %d: train loss %f 131 0.32315364480018616\n",
      "Epoch 131: val loss 0.919764\n",
      "\n",
      "Epoch %d: train loss %f 132 0.3332914113998413\n",
      "Epoch 132: val loss 0.879828\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3613766133785248\n",
      "Epoch 133: val loss 0.911525\n",
      "\n",
      "Epoch %d: train loss %f 134 0.3414597130484051\n",
      "Epoch 134: val loss 0.929619\n",
      "\n",
      "Epoch %d: train loss %f 135 0.3787257538901435\n",
      "Epoch 135: val loss 0.853217\n",
      "\n",
      "Epoch %d: train loss %f 136 0.346087967356046\n",
      "Epoch 136: val loss 0.990946\n",
      "\n",
      "Epoch %d: train loss %f 137 0.34996722473038566\n",
      "Epoch 137: val loss 0.853997\n",
      "\n",
      "Epoch %d: train loss %f 138 0.3979233271545834\n",
      "Epoch 138: val loss 0.873624\n",
      "\n",
      "Epoch %d: train loss %f 139 0.33696385390228695\n",
      "Epoch 139: val loss 0.952432\n",
      "\n",
      "Epoch %d: train loss %f 140 0.33324359854062396\n",
      "Epoch 140: val loss 0.941209\n",
      "\n",
      "Epoch %d: train loss %f 141 0.3497769667042626\n",
      "Epoch 141: val loss 0.940724\n",
      "\n",
      "Epoch %d: train loss %f 142 0.314420234825876\n",
      "Epoch 142: val loss 0.907940\n",
      "\n",
      "Epoch %d: train loss %f 143 0.4108925693564945\n",
      "Epoch 143: val loss 0.882807\n",
      "\n",
      "Epoch %d: train loss %f 144 0.33360100123617387\n",
      "Epoch 144: val loss 0.922931\n",
      "\n",
      "Epoch %d: train loss %f 145 0.36465836895836723\n",
      "Epoch 145: val loss 0.869587\n",
      "\n",
      "Epoch %d: train loss %f 146 0.3687013354566362\n",
      "Epoch 146: val loss 0.980521\n",
      "\n",
      "Epoch %d: train loss %f 147 0.3729478021462758\n",
      "Epoch 147: val loss 0.893939\n",
      "\n",
      "Epoch %d: train loss %f 148 0.31707146432664657\n",
      "Epoch 148: val loss 0.886319\n",
      "\n",
      "Epoch %d: train loss %f 149 0.3057318967249658\n",
      "Epoch 149: val loss 0.913473\n",
      "\n",
      "Epoch %d: train loss %f 150 0.34686204459932113\n",
      "Epoch 150: val loss 0.911286\n",
      "\n",
      "Epoch %d: train loss %f 151 0.3414411147435506\n",
      "Epoch 151: val loss 0.981920\n",
      "\n",
      "Epoch %d: train loss %f 152 0.3851940996117062\n",
      "Epoch 152: val loss 0.912986\n",
      "\n",
      "Epoch %d: train loss %f 153 0.3498347004254659\n",
      "Epoch 153: val loss 0.926880\n",
      "\n",
      "Epoch %d: train loss %f 154 0.3232897337940004\n",
      "Epoch 154: val loss 0.870004\n",
      "\n",
      "Epoch %d: train loss %f 155 0.30837559203306836\n",
      "Epoch 155: val loss 0.955636\n",
      "\n",
      "Epoch %d: train loss %f 156 0.2750808596611023\n",
      "Epoch 156: val loss 0.941780\n",
      "\n",
      "Epoch %d: train loss %f 157 0.33103245000044507\n",
      "Epoch 157: val loss 0.937273\n",
      "\n",
      "Epoch %d: train loss %f 158 0.31421131392319995\n",
      "Epoch 158: val loss 0.958444\n",
      "\n",
      "Epoch %d: train loss %f 159 0.29065781169467503\n",
      "Epoch 159: val loss 0.960057\n",
      "\n",
      "Epoch %d: train loss %f 160 0.2922752367125617\n",
      "Epoch 160: val loss 0.949315\n",
      "\n",
      "Epoch %d: train loss %f 161 0.3422880354854796\n",
      "Epoch 161: val loss 0.967502\n",
      "\n",
      "Epoch %d: train loss %f 162 0.28999927391608554\n",
      "Epoch 162: val loss 0.957817\n",
      "\n",
      "Epoch %d: train loss %f 163 0.2878352022833294\n",
      "Epoch 163: val loss 0.972070\n",
      "\n",
      "Epoch %d: train loss %f 164 0.2886886199315389\n",
      "Epoch 164: val loss 0.952206\n",
      "\n",
      "Epoch %d: train loss %f 165 0.2719853421052297\n",
      "Epoch 165: val loss 0.981242\n",
      "\n",
      "Epoch %d: train loss %f 166 0.27384883496496415\n",
      "Epoch 166: val loss 0.960145\n",
      "\n",
      "Epoch %d: train loss %f 167 0.27198132541444564\n",
      "Epoch 167: val loss 0.988877\n",
      "\n",
      "Epoch %d: train loss %f 168 0.297650534245703\n",
      "Epoch 168: val loss 1.046564\n",
      "\n",
      "Epoch %d: train loss %f 169 0.355378872818417\n",
      "Epoch 169: val loss 0.969100\n",
      "\n",
      "Epoch %d: train loss %f 170 0.34228112465805477\n",
      "Epoch 170: val loss 0.972255\n",
      "\n",
      "Epoch %d: train loss %f 171 0.29167557011048\n",
      "Epoch 171: val loss 0.993372\n",
      "\n",
      "Epoch %d: train loss %f 172 0.290158008535703\n",
      "Epoch 172: val loss 1.006889\n",
      "\n",
      "Epoch %d: train loss %f 173 0.2829226247138447\n",
      "Epoch 173: val loss 1.033301\n",
      "\n",
      "Epoch %d: train loss %f 174 0.3330638077523973\n",
      "Epoch 174: val loss 0.994084\n",
      "\n",
      "Epoch %d: train loss %f 175 0.31023196710480583\n",
      "Epoch 175: val loss 1.014555\n",
      "\n",
      "Epoch %d: train loss %f 176 0.31879324383205837\n",
      "Epoch 176: val loss 1.007548\n",
      "\n",
      "Epoch %d: train loss %f 177 0.29056959682040745\n",
      "Epoch 177: val loss 1.064671\n",
      "\n",
      "Epoch %d: train loss %f 178 0.2746313181188371\n",
      "Epoch 178: val loss 1.023666\n",
      "\n",
      "Epoch %d: train loss %f 179 0.27595899833573234\n",
      "Epoch 179: val loss 1.044236\n",
      "\n",
      "Epoch %d: train loss %f 180 0.2996608068545659\n",
      "Epoch 180: val loss 0.986511\n",
      "\n",
      "Epoch %d: train loss %f 181 0.2844388865762287\n",
      "Epoch 181: val loss 0.982919\n",
      "\n",
      "Epoch %d: train loss %f 182 0.31607777376969654\n",
      "Epoch 182: val loss 0.982441\n",
      "\n",
      "Epoch %d: train loss %f 183 0.2916366143359078\n",
      "Epoch 183: val loss 1.035089\n",
      "\n",
      "Epoch %d: train loss %f 184 0.2738182958629396\n",
      "Epoch 184: val loss 1.088783\n",
      "\n",
      "Epoch %d: train loss %f 185 0.33254935840765637\n",
      "Epoch 185: val loss 1.080755\n",
      "\n",
      "Epoch %d: train loss %f 186 0.30987419270806843\n",
      "Epoch 186: val loss 1.033846\n",
      "\n",
      "Epoch %d: train loss %f 187 0.28731730911466813\n",
      "Epoch 187: val loss 1.008290\n",
      "\n",
      "Epoch %d: train loss %f 188 0.33078568677107495\n",
      "Epoch 188: val loss 1.035764\n",
      "\n",
      "Epoch %d: train loss %f 189 0.2996361404657364\n",
      "Epoch 189: val loss 1.061625\n",
      "\n",
      "Epoch %d: train loss %f 190 0.28917844593524933\n",
      "Epoch 190: val loss 1.013996\n",
      "\n",
      "Epoch %d: train loss %f 191 0.26909055809179944\n",
      "Epoch 191: val loss 1.022056\n",
      "\n",
      "Epoch %d: train loss %f 192 0.3139164100090663\n",
      "Epoch 192: val loss 1.084908\n",
      "\n",
      "Epoch %d: train loss %f 193 0.30191388063960606\n",
      "Epoch 193: val loss 0.994621\n",
      "\n",
      "Epoch %d: train loss %f 194 0.3303381883435779\n",
      "Epoch 194: val loss 1.028744\n",
      "\n",
      "Epoch %d: train loss %f 195 0.2628801266352336\n",
      "Epoch 195: val loss 1.012206\n",
      "\n",
      "Epoch %d: train loss %f 196 0.29710044629044\n",
      "Epoch 196: val loss 1.069871\n",
      "\n",
      "Epoch %d: train loss %f 197 0.2962875117858251\n",
      "Epoch 197: val loss 1.064968\n",
      "\n",
      "Epoch %d: train loss %f 198 0.30963297684987384\n",
      "Epoch 198: val loss 1.051610\n",
      "\n",
      "Epoch %d: train loss %f 199 0.26916901270548504\n",
      "Epoch 199: val loss 1.056738\n",
      "\n",
      "Epoch %d: train loss %f 0 0.699773500363032\n",
      "Epoch 0: val loss 0.699064\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6947375535964966\n",
      "Epoch 1: val loss 0.696090\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6864708065986633\n",
      "Epoch 2: val loss 0.689779\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6792111943165461\n",
      "Epoch 3: val loss 0.678463\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6576631317536036\n",
      "Epoch 4: val loss 0.667876\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6545920819044113\n",
      "Epoch 5: val loss 0.648879\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6403718541065851\n",
      "Epoch 6: val loss 0.640900\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6303008695443472\n",
      "Epoch 7: val loss 0.634307\n",
      "\n",
      "Epoch %d: train loss %f 8 0.6037544707457224\n",
      "Epoch 8: val loss 0.634464\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5878623326619467\n",
      "Epoch 9: val loss 0.644712\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5872244089841843\n",
      "Epoch 10: val loss 0.640010\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5839258953928947\n",
      "Epoch 11: val loss 0.626666\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5633863086501757\n",
      "Epoch 12: val loss 0.610422\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5549114222327868\n",
      "Epoch 13: val loss 0.611447\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5515100757280985\n",
      "Epoch 14: val loss 0.663360\n",
      "\n",
      "Epoch %d: train loss %f 15 0.53712331255277\n",
      "Epoch 15: val loss 0.611221\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5412400439381599\n",
      "Epoch 16: val loss 0.646706\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5426962822675705\n",
      "Epoch 17: val loss 0.616375\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5262476752201716\n",
      "Epoch 18: val loss 0.656077\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5203165387113889\n",
      "Epoch 19: val loss 0.593729\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5268534148732821\n",
      "Epoch 20: val loss 0.600976\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5375743210315704\n",
      "Epoch 21: val loss 0.596077\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5497428228457769\n",
      "Epoch 22: val loss 0.621933\n",
      "\n",
      "Epoch %d: train loss %f 23 0.5105922222137451\n",
      "Epoch 23: val loss 0.631289\n",
      "\n",
      "Epoch %d: train loss %f 24 0.5285695344209671\n",
      "Epoch 24: val loss 0.631221\n",
      "\n",
      "Epoch %d: train loss %f 25 0.5348434423406919\n",
      "Epoch 25: val loss 0.620156\n",
      "\n",
      "Epoch %d: train loss %f 26 0.5044984022776285\n",
      "Epoch 26: val loss 0.617025\n",
      "\n",
      "Epoch %d: train loss %f 27 0.511337528626124\n",
      "Epoch 27: val loss 0.610454\n",
      "\n",
      "Epoch %d: train loss %f 28 0.49237827708323795\n",
      "Epoch 28: val loss 0.641170\n",
      "\n",
      "Epoch %d: train loss %f 29 0.5192825794219971\n",
      "Epoch 29: val loss 0.625298\n",
      "\n",
      "Epoch %d: train loss %f 30 0.4894217550754547\n",
      "Epoch 30: val loss 0.624297\n",
      "\n",
      "Epoch %d: train loss %f 31 0.5051753247777621\n",
      "Epoch 31: val loss 0.626127\n",
      "\n",
      "Epoch %d: train loss %f 32 0.524580808977286\n",
      "Epoch 32: val loss 0.620619\n",
      "\n",
      "Epoch %d: train loss %f 33 0.4907948225736618\n",
      "Epoch 33: val loss 0.600331\n",
      "\n",
      "Epoch %d: train loss %f 34 0.49739057073990506\n",
      "Epoch 34: val loss 0.624696\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4805711433291435\n",
      "Epoch 35: val loss 0.627637\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4948986967404683\n",
      "Epoch 36: val loss 0.643257\n",
      "\n",
      "Epoch %d: train loss %f 37 0.5099001005291939\n",
      "Epoch 37: val loss 0.614776\n",
      "\n",
      "Epoch %d: train loss %f 38 0.49275091538826626\n",
      "Epoch 38: val loss 0.621808\n",
      "\n",
      "Epoch %d: train loss %f 39 0.5140067984660467\n",
      "Epoch 39: val loss 0.611411\n",
      "\n",
      "Epoch %d: train loss %f 40 0.5190770129362742\n",
      "Epoch 40: val loss 0.630129\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4917222360769908\n",
      "Epoch 41: val loss 0.611782\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4783192152778308\n",
      "Epoch 42: val loss 0.619676\n",
      "\n",
      "Epoch %d: train loss %f 43 0.5075375859936079\n",
      "Epoch 43: val loss 0.600510\n",
      "\n",
      "Epoch %d: train loss %f 44 0.5033345421155294\n",
      "Epoch 44: val loss 0.605838\n",
      "\n",
      "Epoch %d: train loss %f 45 0.5056188007195791\n",
      "Epoch 45: val loss 0.612391\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4964334964752197\n",
      "Epoch 46: val loss 0.606187\n",
      "\n",
      "Epoch %d: train loss %f 47 0.5099121679862341\n",
      "Epoch 47: val loss 0.609170\n",
      "\n",
      "Epoch %d: train loss %f 48 0.4698907385269801\n",
      "Epoch 48: val loss 0.602167\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4945799832542737\n",
      "Epoch 49: val loss 0.605935\n",
      "\n",
      "Epoch %d: train loss %f 50 0.4839066192507744\n",
      "Epoch 50: val loss 0.590605\n",
      "\n",
      "Epoch %d: train loss %f 51 0.49606693536043167\n",
      "Epoch 51: val loss 0.587905\n",
      "\n",
      "Epoch %d: train loss %f 52 0.46923639873663586\n",
      "Epoch 52: val loss 0.612321\n",
      "\n",
      "Epoch %d: train loss %f 53 0.502858929336071\n",
      "Epoch 53: val loss 0.604526\n",
      "\n",
      "Epoch %d: train loss %f 54 0.47685400148232776\n",
      "Epoch 54: val loss 0.589877\n",
      "\n",
      "Epoch %d: train loss %f 55 0.49303452173868817\n",
      "Epoch 55: val loss 0.579018\n",
      "\n",
      "Epoch %d: train loss %f 56 0.4919852589567502\n",
      "Epoch 56: val loss 0.580464\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4896243487795194\n",
      "Epoch 57: val loss 0.600481\n",
      "\n",
      "Epoch %d: train loss %f 58 0.4669669084250927\n",
      "Epoch 58: val loss 0.655577\n",
      "\n",
      "Epoch %d: train loss %f 59 0.4926403984427452\n",
      "Epoch 59: val loss 0.625646\n",
      "\n",
      "Epoch %d: train loss %f 60 0.49432551860809326\n",
      "Epoch 60: val loss 0.609785\n",
      "\n",
      "Epoch %d: train loss %f 61 0.45937484006086987\n",
      "Epoch 61: val loss 0.599054\n",
      "\n",
      "Epoch %d: train loss %f 62 0.486207236846288\n",
      "Epoch 62: val loss 0.583400\n",
      "\n",
      "Epoch %d: train loss %f 63 0.49532824754714966\n",
      "Epoch 63: val loss 0.594605\n",
      "\n",
      "Epoch %d: train loss %f 64 0.4913577362895012\n",
      "Epoch 64: val loss 0.622339\n",
      "\n",
      "Epoch %d: train loss %f 65 0.47089939067761105\n",
      "Epoch 65: val loss 0.587088\n",
      "\n",
      "Epoch %d: train loss %f 66 0.46613068133592606\n",
      "Epoch 66: val loss 0.620411\n",
      "\n",
      "Epoch %d: train loss %f 67 0.4756070524454117\n",
      "Epoch 67: val loss 0.605168\n",
      "\n",
      "Epoch %d: train loss %f 68 0.4865589141845703\n",
      "Epoch 68: val loss 0.612847\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4615028401215871\n",
      "Epoch 69: val loss 0.608552\n",
      "\n",
      "Epoch %d: train loss %f 70 0.4732646370927493\n",
      "Epoch 70: val loss 0.601715\n",
      "\n",
      "Epoch %d: train loss %f 71 0.4748099346955617\n",
      "Epoch 71: val loss 0.607619\n",
      "\n",
      "Epoch %d: train loss %f 72 0.4648596942424774\n",
      "Epoch 72: val loss 0.608617\n",
      "\n",
      "Epoch %d: train loss %f 73 0.4466829299926758\n",
      "Epoch 73: val loss 0.579271\n",
      "\n",
      "Epoch %d: train loss %f 74 0.46705948809782666\n",
      "Epoch 74: val loss 0.593243\n",
      "\n",
      "Epoch %d: train loss %f 75 0.45937080681324005\n",
      "Epoch 75: val loss 0.616971\n",
      "\n",
      "Epoch %d: train loss %f 76 0.44891004761060077\n",
      "Epoch 76: val loss 0.609006\n",
      "\n",
      "Epoch %d: train loss %f 77 0.4785892094175021\n",
      "Epoch 77: val loss 0.626652\n",
      "\n",
      "Epoch %d: train loss %f 78 0.4640362535913785\n",
      "Epoch 78: val loss 0.611356\n",
      "\n",
      "Epoch %d: train loss %f 79 0.4385485053062439\n",
      "Epoch 79: val loss 0.627680\n",
      "\n",
      "Epoch %d: train loss %f 80 0.4568333824475606\n",
      "Epoch 80: val loss 0.590963\n",
      "\n",
      "Epoch %d: train loss %f 81 0.4447868913412094\n",
      "Epoch 81: val loss 0.596801\n",
      "\n",
      "Epoch %d: train loss %f 82 0.46745838473240536\n",
      "Epoch 82: val loss 0.613782\n",
      "\n",
      "Epoch %d: train loss %f 83 0.4402904734015465\n",
      "Epoch 83: val loss 0.584946\n",
      "\n",
      "Epoch %d: train loss %f 84 0.4295570527513822\n",
      "Epoch 84: val loss 0.591557\n",
      "\n",
      "Epoch %d: train loss %f 85 0.44813356051842373\n",
      "Epoch 85: val loss 0.617929\n",
      "\n",
      "Epoch %d: train loss %f 86 0.4362620810667674\n",
      "Epoch 86: val loss 0.609329\n",
      "\n",
      "Epoch %d: train loss %f 87 0.4345162933071454\n",
      "Epoch 87: val loss 0.614836\n",
      "\n",
      "Epoch %d: train loss %f 88 0.4723404844601949\n",
      "Epoch 88: val loss 0.629467\n",
      "\n",
      "Epoch %d: train loss %f 89 0.4485271175702413\n",
      "Epoch 89: val loss 0.621933\n",
      "\n",
      "Epoch %d: train loss %f 90 0.44927814851204556\n",
      "Epoch 90: val loss 0.646497\n",
      "\n",
      "Epoch %d: train loss %f 91 0.4635528450210889\n",
      "Epoch 91: val loss 0.647299\n",
      "\n",
      "Epoch %d: train loss %f 92 0.492778018116951\n",
      "Epoch 92: val loss 0.630501\n",
      "\n",
      "Epoch %d: train loss %f 93 0.44184215118487674\n",
      "Epoch 93: val loss 0.608108\n",
      "\n",
      "Epoch %d: train loss %f 94 0.4233694275220235\n",
      "Epoch 94: val loss 0.600064\n",
      "\n",
      "Epoch %d: train loss %f 95 0.4596612900495529\n",
      "Epoch 95: val loss 0.623491\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6559258759021759\n",
      "Epoch 0: val loss 0.655026\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6387633562088013\n",
      "Epoch 1: val loss 0.647242\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6168463230133057\n",
      "Epoch 2: val loss 0.633245\n",
      "\n",
      "Epoch %d: train loss %f 3 0.5860192358493805\n",
      "Epoch 3: val loss 0.603939\n",
      "\n",
      "Epoch %d: train loss %f 4 0.5695935785770416\n",
      "Epoch 4: val loss 0.555417\n",
      "\n",
      "Epoch %d: train loss %f 5 0.4988780081272125\n",
      "Epoch 5: val loss 0.501879\n",
      "\n",
      "Epoch %d: train loss %f 6 0.46564138531684873\n",
      "Epoch 6: val loss 0.460189\n",
      "\n",
      "Epoch %d: train loss %f 7 0.4187818914651871\n",
      "Epoch 7: val loss 0.444369\n",
      "\n",
      "Epoch %d: train loss %f 8 0.4298313736915588\n",
      "Epoch 8: val loss 0.436158\n",
      "\n",
      "Epoch %d: train loss %f 9 0.3857042744755745\n",
      "Epoch 9: val loss 0.434588\n",
      "\n",
      "Epoch %d: train loss %f 10 0.39091660976409914\n",
      "Epoch 10: val loss 0.432391\n",
      "\n",
      "Epoch %d: train loss %f 11 0.41532617807388306\n",
      "Epoch 11: val loss 0.431450\n",
      "\n",
      "Epoch %d: train loss %f 12 0.38834346532821656\n",
      "Epoch 12: val loss 0.440186\n",
      "\n",
      "Epoch %d: train loss %f 13 0.37328170388937\n",
      "Epoch 13: val loss 0.433361\n",
      "\n",
      "Epoch %d: train loss %f 14 0.37747235894203185\n",
      "Epoch 14: val loss 0.436478\n",
      "\n",
      "Epoch %d: train loss %f 15 0.36599545180797577\n",
      "Epoch 15: val loss 0.432921\n",
      "\n",
      "Epoch %d: train loss %f 16 0.36386880129575727\n",
      "Epoch 16: val loss 0.434324\n",
      "\n",
      "Epoch %d: train loss %f 17 0.44085000455379486\n",
      "Epoch 17: val loss 0.434824\n",
      "\n",
      "Epoch %d: train loss %f 18 0.39185082018375395\n",
      "Epoch 18: val loss 0.438342\n",
      "\n",
      "Epoch %d: train loss %f 19 0.3614792227745056\n",
      "Epoch 19: val loss 0.436911\n",
      "\n",
      "Epoch %d: train loss %f 20 0.3701004534959793\n",
      "Epoch 20: val loss 0.438969\n",
      "\n",
      "Epoch %d: train loss %f 21 0.3420750290155411\n",
      "Epoch 21: val loss 0.438831\n",
      "\n",
      "Epoch %d: train loss %f 22 0.3534928098320961\n",
      "Epoch 22: val loss 0.435800\n",
      "\n",
      "Epoch %d: train loss %f 23 0.4443373531103134\n",
      "Epoch 23: val loss 0.439610\n",
      "\n",
      "Epoch %d: train loss %f 24 0.390018031001091\n",
      "Epoch 24: val loss 0.456020\n",
      "\n",
      "Epoch %d: train loss %f 25 0.3466819614171982\n",
      "Epoch 25: val loss 0.464872\n",
      "\n",
      "Epoch %d: train loss %f 26 0.3496666520833969\n",
      "Epoch 26: val loss 0.462656\n",
      "\n",
      "Epoch %d: train loss %f 27 0.34342746138572694\n",
      "Epoch 27: val loss 0.468599\n",
      "\n",
      "Epoch %d: train loss %f 28 0.3677279591560364\n",
      "Epoch 28: val loss 0.459666\n",
      "\n",
      "Epoch %d: train loss %f 29 0.3695300430059433\n",
      "Epoch 29: val loss 0.450262\n",
      "\n",
      "Epoch %d: train loss %f 30 0.40108631253242494\n",
      "Epoch 30: val loss 0.455720\n",
      "\n",
      "Epoch %d: train loss %f 31 0.34140313416719437\n",
      "Epoch 31: val loss 0.452112\n",
      "\n",
      "Epoch %d: train loss %f 32 0.3471002072095871\n",
      "Epoch 32: val loss 0.450320\n",
      "\n",
      "Epoch %d: train loss %f 33 0.32100473940372465\n",
      "Epoch 33: val loss 0.450403\n",
      "\n",
      "Epoch %d: train loss %f 34 0.3674062922596931\n",
      "Epoch 34: val loss 0.461474\n",
      "\n",
      "Epoch %d: train loss %f 35 0.3525702625513077\n",
      "Epoch 35: val loss 0.461995\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4264411926269531\n",
      "Epoch 36: val loss 0.454529\n",
      "\n",
      "Epoch %d: train loss %f 37 0.31486582159996035\n",
      "Epoch 37: val loss 0.466519\n",
      "\n",
      "Epoch %d: train loss %f 38 0.3765977442264557\n",
      "Epoch 38: val loss 0.469165\n",
      "\n",
      "Epoch %d: train loss %f 39 0.3246577501296997\n",
      "Epoch 39: val loss 0.478583\n",
      "\n",
      "Epoch %d: train loss %f 40 0.30926083475351335\n",
      "Epoch 40: val loss 0.472030\n",
      "\n",
      "Epoch %d: train loss %f 41 0.3143390104174614\n",
      "Epoch 41: val loss 0.467859\n",
      "\n",
      "Epoch %d: train loss %f 42 0.3221250995993614\n",
      "Epoch 42: val loss 0.474460\n",
      "\n",
      "Epoch %d: train loss %f 43 0.3242451474070549\n",
      "Epoch 43: val loss 0.479477\n",
      "\n",
      "Epoch %d: train loss %f 44 0.34255462437868117\n",
      "Epoch 44: val loss 0.484205\n",
      "\n",
      "Epoch %d: train loss %f 45 0.3399138778448105\n",
      "Epoch 45: val loss 0.485328\n",
      "\n",
      "Epoch %d: train loss %f 46 0.3403976738452911\n",
      "Epoch 46: val loss 0.477817\n",
      "\n",
      "Epoch %d: train loss %f 47 0.2948560953140259\n",
      "Epoch 47: val loss 0.477790\n",
      "\n",
      "Epoch %d: train loss %f 48 0.29337691217660905\n",
      "Epoch 48: val loss 0.480166\n",
      "\n",
      "Epoch %d: train loss %f 49 0.33132197707891464\n",
      "Epoch 49: val loss 0.482290\n",
      "\n",
      "Epoch %d: train loss %f 50 0.32468904107809066\n",
      "Epoch 50: val loss 0.499979\n",
      "\n",
      "Epoch %d: train loss %f 51 0.2975440815091133\n",
      "Epoch 51: val loss 0.516526\n",
      "\n",
      "Epoch %d: train loss %f 52 0.3375708296895027\n",
      "Epoch 52: val loss 0.506444\n",
      "\n",
      "Epoch %d: train loss %f 53 0.3396392151713371\n",
      "Epoch 53: val loss 0.511083\n",
      "\n",
      "Epoch %d: train loss %f 54 0.3494140788912773\n",
      "Epoch 54: val loss 0.519441\n",
      "\n",
      "Epoch %d: train loss %f 55 0.3295181319117546\n",
      "Epoch 55: val loss 0.515535\n",
      "\n",
      "Epoch %d: train loss %f 56 0.3393720328807831\n",
      "Epoch 56: val loss 0.489426\n",
      "\n",
      "Epoch %d: train loss %f 57 0.3133267492055893\n",
      "Epoch 57: val loss 0.474859\n",
      "\n",
      "Epoch %d: train loss %f 58 0.30513829588890073\n",
      "Epoch 58: val loss 0.508956\n",
      "\n",
      "Epoch %d: train loss %f 59 0.29648119062185285\n",
      "Epoch 59: val loss 0.515261\n",
      "\n",
      "Epoch %d: train loss %f 60 0.3077153906226158\n",
      "Epoch 60: val loss 0.509198\n",
      "\n",
      "Epoch %d: train loss %f 61 0.27446396052837374\n",
      "Epoch 61: val loss 0.512079\n",
      "\n",
      "Epoch %d: train loss %f 62 0.3093199849128723\n",
      "Epoch 62: val loss 0.508894\n",
      "\n",
      "Epoch %d: train loss %f 63 0.30781989395618437\n",
      "Epoch 63: val loss 0.506878\n",
      "\n",
      "Epoch %d: train loss %f 64 0.3233420833945274\n",
      "Epoch 64: val loss 0.520340\n",
      "\n",
      "Epoch %d: train loss %f 65 0.346673221886158\n",
      "Epoch 65: val loss 0.543013\n",
      "\n",
      "Epoch %d: train loss %f 66 0.26318511590361593\n",
      "Epoch 66: val loss 0.540491\n",
      "\n",
      "Epoch %d: train loss %f 67 0.3096435546875\n",
      "Epoch 67: val loss 0.524408\n",
      "\n",
      "Epoch %d: train loss %f 68 0.2644504323601723\n",
      "Epoch 68: val loss 0.521109\n",
      "\n",
      "Epoch %d: train loss %f 69 0.2890878960490227\n",
      "Epoch 69: val loss 0.522416\n",
      "\n",
      "Epoch %d: train loss %f 70 0.3025559261441231\n",
      "Epoch 70: val loss 0.512426\n",
      "\n",
      "Epoch %d: train loss %f 71 0.26845902651548387\n",
      "Epoch 71: val loss 0.524144\n",
      "\n",
      "Epoch %d: train loss %f 72 0.27378845661878587\n",
      "Epoch 72: val loss 0.513132\n",
      "\n",
      "Epoch %d: train loss %f 73 0.2963399261236191\n",
      "Epoch 73: val loss 0.508695\n",
      "\n",
      "Epoch %d: train loss %f 74 0.30169589519500734\n",
      "Epoch 74: val loss 0.498383\n",
      "\n",
      "Epoch %d: train loss %f 75 0.28563558161258695\n",
      "Epoch 75: val loss 0.529917\n",
      "\n",
      "Epoch %d: train loss %f 76 0.30638937950134276\n",
      "Epoch 76: val loss 0.562613\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3342465728521347\n",
      "Epoch 77: val loss 0.561836\n",
      "\n",
      "Epoch %d: train loss %f 78 0.2812893882393837\n",
      "Epoch 78: val loss 0.535304\n",
      "\n",
      "Epoch %d: train loss %f 79 0.3172364667057991\n",
      "Epoch 79: val loss 0.515376\n",
      "\n",
      "Epoch %d: train loss %f 80 0.2734523206949234\n",
      "Epoch 80: val loss 0.533063\n",
      "\n",
      "Epoch %d: train loss %f 81 0.29524414241313934\n",
      "Epoch 81: val loss 0.552268\n",
      "\n",
      "Epoch %d: train loss %f 82 0.27956332117319105\n",
      "Epoch 82: val loss 0.565794\n",
      "\n",
      "Epoch %d: train loss %f 83 0.24436765909194946\n",
      "Epoch 83: val loss 0.555678\n",
      "\n",
      "Epoch %d: train loss %f 84 0.2533815510571003\n",
      "Epoch 84: val loss 0.536151\n",
      "\n",
      "Epoch %d: train loss %f 85 0.25765414983034135\n",
      "Epoch 85: val loss 0.560137\n",
      "\n",
      "Epoch %d: train loss %f 86 0.2624592274427414\n",
      "Epoch 86: val loss 0.564609\n",
      "\n",
      "Epoch %d: train loss %f 87 0.24807484000921248\n",
      "Epoch 87: val loss 0.583380\n",
      "\n",
      "Epoch %d: train loss %f 88 0.3296753615140915\n",
      "Epoch 88: val loss 0.592764\n",
      "\n",
      "Epoch %d: train loss %f 89 0.3131686121225357\n",
      "Epoch 89: val loss 0.590098\n",
      "\n",
      "Epoch %d: train loss %f 90 0.37682918161153794\n",
      "Epoch 90: val loss 0.542977\n",
      "\n",
      "Epoch %d: train loss %f 91 0.3117973625659943\n",
      "Epoch 91: val loss 0.589310\n",
      "\n",
      "Epoch %d: train loss %f 92 0.27350480407476424\n",
      "Epoch 92: val loss 0.579135\n",
      "\n",
      "Epoch %d: train loss %f 93 0.2510242581367493\n",
      "Epoch 93: val loss 0.555709\n",
      "\n",
      "Epoch %d: train loss %f 94 0.24516970664262772\n",
      "Epoch 94: val loss 0.550992\n",
      "\n",
      "Epoch %d: train loss %f 95 0.254974901676178\n",
      "Epoch 95: val loss 0.550109\n",
      "\n",
      "Epoch %d: train loss %f 96 0.27028565779328345\n",
      "Epoch 96: val loss 0.558687\n",
      "\n",
      "Epoch %d: train loss %f 97 0.27292980402708056\n",
      "Epoch 97: val loss 0.584076\n",
      "\n",
      "Epoch %d: train loss %f 98 0.28347679823637006\n",
      "Epoch 98: val loss 0.603535\n",
      "\n",
      "Epoch %d: train loss %f 99 0.27417934909462927\n",
      "Epoch 99: val loss 0.590261\n",
      "\n",
      "Epoch %d: train loss %f 100 0.3042569264769554\n",
      "Epoch 100: val loss 0.574596\n",
      "\n",
      "Epoch %d: train loss %f 101 0.3018516480922699\n",
      "Epoch 101: val loss 0.559905\n",
      "\n",
      "Epoch %d: train loss %f 102 0.2555987149477005\n",
      "Epoch 102: val loss 0.564940\n",
      "\n",
      "Epoch %d: train loss %f 103 0.31909964233636856\n",
      "Epoch 103: val loss 0.552167\n",
      "\n",
      "Epoch %d: train loss %f 104 0.28709267526865007\n",
      "Epoch 104: val loss 0.590314\n",
      "\n",
      "Epoch %d: train loss %f 105 0.30495610237121584\n",
      "Epoch 105: val loss 0.565741\n",
      "\n",
      "Epoch %d: train loss %f 106 0.26139372736215594\n",
      "Epoch 106: val loss 0.553208\n",
      "\n",
      "Epoch %d: train loss %f 107 0.2600858107209206\n",
      "Epoch 107: val loss 0.563578\n",
      "\n",
      "Epoch %d: train loss %f 108 0.25011017024517057\n",
      "Epoch 108: val loss 0.558053\n",
      "\n",
      "Epoch %d: train loss %f 109 0.26590311229228974\n",
      "Epoch 109: val loss 0.559466\n",
      "\n",
      "Epoch %d: train loss %f 110 0.28377422988414763\n",
      "Epoch 110: val loss 0.572260\n",
      "\n",
      "Epoch %d: train loss %f 111 0.23375531733036042\n",
      "Epoch 111: val loss 0.593023\n",
      "\n",
      "Epoch %d: train loss %f 112 0.24204276651144027\n",
      "Epoch 112: val loss 0.579311\n",
      "\n",
      "Epoch %d: train loss %f 113 0.23508713021874428\n",
      "Epoch 113: val loss 0.592291\n",
      "\n",
      "Epoch %d: train loss %f 114 0.2330151192843914\n",
      "Epoch 114: val loss 0.615271\n",
      "\n",
      "Epoch %d: train loss %f 115 0.2318059980869293\n",
      "Epoch 115: val loss 0.623877\n",
      "\n",
      "Epoch %d: train loss %f 116 0.21404697820544244\n",
      "Epoch 116: val loss 0.596651\n",
      "\n",
      "Epoch %d: train loss %f 117 0.2142307050526142\n",
      "Epoch 117: val loss 0.591740\n",
      "\n",
      "Epoch %d: train loss %f 118 0.2805106475949287\n",
      "Epoch 118: val loss 0.615263\n",
      "\n",
      "Epoch %d: train loss %f 119 0.2360808163881302\n",
      "Epoch 119: val loss 0.615729\n",
      "\n",
      "Epoch %d: train loss %f 120 0.2616854190826416\n",
      "Epoch 120: val loss 0.648243\n",
      "\n",
      "Epoch %d: train loss %f 121 0.25186648815870283\n",
      "Epoch 121: val loss 0.666504\n",
      "\n",
      "Epoch %d: train loss %f 122 0.2406972572207451\n",
      "Epoch 122: val loss 0.642810\n",
      "\n",
      "Epoch %d: train loss %f 123 0.24164208993315697\n",
      "Epoch 123: val loss 0.636541\n",
      "\n",
      "Epoch %d: train loss %f 124 0.2291430950164795\n",
      "Epoch 124: val loss 0.635443\n",
      "\n",
      "Epoch %d: train loss %f 125 0.23265520259737968\n",
      "Epoch 125: val loss 0.630147\n",
      "\n",
      "Epoch %d: train loss %f 126 0.26137821823358537\n",
      "Epoch 126: val loss 0.644640\n",
      "\n",
      "Epoch %d: train loss %f 127 0.2526328772306442\n",
      "Epoch 127: val loss 0.657684\n",
      "\n",
      "Epoch %d: train loss %f 128 0.2101254403591156\n",
      "Epoch 128: val loss 0.594875\n",
      "\n",
      "Epoch %d: train loss %f 129 0.23235471099615096\n",
      "Epoch 129: val loss 0.612558\n",
      "\n",
      "Epoch %d: train loss %f 130 0.22585539221763612\n",
      "Epoch 130: val loss 0.629036\n",
      "\n",
      "Epoch %d: train loss %f 131 0.21188217774033546\n",
      "Epoch 131: val loss 0.659201\n",
      "\n",
      "Epoch %d: train loss %f 132 0.20685808807611467\n",
      "Epoch 132: val loss 0.662813\n",
      "\n",
      "Epoch %d: train loss %f 133 0.2068931542336941\n",
      "Epoch 133: val loss 0.674066\n",
      "\n",
      "Epoch %d: train loss %f 134 0.2130172312259674\n",
      "Epoch 134: val loss 0.659304\n",
      "\n",
      "Epoch %d: train loss %f 135 0.21453984901309014\n",
      "Epoch 135: val loss 0.651371\n",
      "\n",
      "Epoch %d: train loss %f 136 0.22970198094844818\n",
      "Epoch 136: val loss 0.663248\n",
      "\n",
      "Epoch %d: train loss %f 137 0.21526076942682265\n",
      "Epoch 137: val loss 0.672059\n",
      "\n",
      "Epoch %d: train loss %f 138 0.21571141183376313\n",
      "Epoch 138: val loss 0.703656\n",
      "\n",
      "Epoch %d: train loss %f 139 0.21588341742753983\n",
      "Epoch 139: val loss 0.671771\n",
      "\n",
      "Epoch %d: train loss %f 140 0.1982169270515442\n",
      "Epoch 140: val loss 0.680529\n",
      "\n",
      "Epoch %d: train loss %f 141 0.19982924088835716\n",
      "Epoch 141: val loss 0.665017\n",
      "\n",
      "Epoch %d: train loss %f 142 0.23590401895344257\n",
      "Epoch 142: val loss 0.665259\n",
      "\n",
      "Epoch %d: train loss %f 143 0.2209768235683441\n",
      "Epoch 143: val loss 0.678496\n",
      "\n",
      "Epoch %d: train loss %f 144 0.2557341903448105\n",
      "Epoch 144: val loss 0.653042\n",
      "\n",
      "Epoch %d: train loss %f 145 0.2069607935845852\n",
      "Epoch 145: val loss 0.660587\n",
      "\n",
      "Epoch %d: train loss %f 146 0.20795041620731353\n",
      "Epoch 146: val loss 0.699633\n",
      "\n",
      "Epoch %d: train loss %f 147 0.1817840777337551\n",
      "Epoch 147: val loss 0.674425\n",
      "\n",
      "Epoch %d: train loss %f 148 0.2247749775648117\n",
      "Epoch 148: val loss 0.731058\n",
      "\n",
      "Epoch %d: train loss %f 149 0.3093580476939678\n",
      "Epoch 149: val loss 0.714664\n",
      "\n",
      "Epoch %d: train loss %f 150 0.18822240680456162\n",
      "Epoch 150: val loss 0.597390\n",
      "\n",
      "Epoch %d: train loss %f 151 0.22647651880979539\n",
      "Epoch 151: val loss 0.592145\n",
      "\n",
      "Epoch %d: train loss %f 152 0.2500690907239914\n",
      "Epoch 152: val loss 0.646057\n",
      "\n",
      "Epoch %d: train loss %f 153 0.2407078340649605\n",
      "Epoch 153: val loss 0.684482\n",
      "\n",
      "Epoch %d: train loss %f 154 0.2096627563238144\n",
      "Epoch 154: val loss 0.728067\n",
      "\n",
      "Epoch %d: train loss %f 155 0.2266857333481312\n",
      "Epoch 155: val loss 0.666123\n",
      "\n",
      "Epoch %d: train loss %f 156 0.20399586707353592\n",
      "Epoch 156: val loss 0.646932\n",
      "\n",
      "Epoch %d: train loss %f 157 0.21939327865839003\n",
      "Epoch 157: val loss 0.677699\n",
      "\n",
      "Epoch %d: train loss %f 158 0.19094162210822105\n",
      "Epoch 158: val loss 0.735915\n",
      "\n",
      "Epoch %d: train loss %f 159 0.20187093168497086\n",
      "Epoch 159: val loss 0.691147\n",
      "\n",
      "Epoch %d: train loss %f 160 0.19373072162270547\n",
      "Epoch 160: val loss 0.679056\n",
      "\n",
      "Epoch %d: train loss %f 161 0.1905817374587059\n",
      "Epoch 161: val loss 0.703529\n",
      "\n",
      "Epoch %d: train loss %f 162 0.1994242399930954\n",
      "Epoch 162: val loss 0.696526\n",
      "\n",
      "Epoch %d: train loss %f 163 0.19675310105085372\n",
      "Epoch 163: val loss 0.649903\n",
      "\n",
      "Epoch %d: train loss %f 164 0.204255760461092\n",
      "Epoch 164: val loss 0.674102\n",
      "\n",
      "Epoch %d: train loss %f 165 0.21480011194944382\n",
      "Epoch 165: val loss 0.693787\n",
      "\n",
      "Epoch %d: train loss %f 166 0.2052740126848221\n",
      "Epoch 166: val loss 0.678662\n",
      "\n",
      "Epoch %d: train loss %f 167 0.1870898887515068\n",
      "Epoch 167: val loss 0.723326\n",
      "\n",
      "Epoch %d: train loss %f 168 0.20561685748398303\n",
      "Epoch 168: val loss 0.723927\n",
      "\n",
      "Epoch %d: train loss %f 169 0.2229852370917797\n",
      "Epoch 169: val loss 0.739997\n",
      "\n",
      "Epoch %d: train loss %f 170 0.18217724859714507\n",
      "Epoch 170: val loss 0.730108\n",
      "\n",
      "Epoch %d: train loss %f 171 0.20570555701851845\n",
      "Epoch 171: val loss 0.742996\n",
      "\n",
      "Epoch %d: train loss %f 172 0.18278553783893586\n",
      "Epoch 172: val loss 0.771197\n",
      "\n",
      "Epoch %d: train loss %f 173 0.18381863608956336\n",
      "Epoch 173: val loss 0.724515\n",
      "\n",
      "Epoch %d: train loss %f 174 0.17136430442333223\n",
      "Epoch 174: val loss 0.679648\n",
      "\n",
      "Epoch %d: train loss %f 175 0.19317706376314164\n",
      "Epoch 175: val loss 0.693319\n",
      "\n",
      "Epoch %d: train loss %f 176 0.17827717661857606\n",
      "Epoch 176: val loss 0.651196\n",
      "\n",
      "Epoch %d: train loss %f 177 0.22727555334568023\n",
      "Epoch 177: val loss 0.700703\n",
      "\n",
      "Epoch %d: train loss %f 178 0.17102305293083192\n",
      "Epoch 178: val loss 0.741161\n",
      "\n",
      "Epoch %d: train loss %f 179 0.15568978413939477\n",
      "Epoch 179: val loss 0.712861\n",
      "\n",
      "Epoch %d: train loss %f 180 0.1638172447681427\n",
      "Epoch 180: val loss 0.730726\n",
      "\n",
      "Epoch %d: train loss %f 181 0.1617753691971302\n",
      "Epoch 181: val loss 0.760619\n",
      "\n",
      "Epoch %d: train loss %f 182 0.1769024297595024\n",
      "Epoch 182: val loss 0.757719\n",
      "\n",
      "Epoch %d: train loss %f 183 0.2642602000385523\n",
      "Epoch 183: val loss 0.748080\n",
      "\n",
      "Epoch %d: train loss %f 184 0.24192763790488242\n",
      "Epoch 184: val loss 0.756718\n",
      "\n",
      "Epoch %d: train loss %f 185 0.1896752692759037\n",
      "Epoch 185: val loss 0.823273\n",
      "\n",
      "Epoch %d: train loss %f 186 0.1996149092912674\n",
      "Epoch 186: val loss 0.791077\n",
      "\n",
      "Epoch %d: train loss %f 187 0.20771932750940322\n",
      "Epoch 187: val loss 0.777016\n",
      "\n",
      "Epoch %d: train loss %f 188 0.18454643189907075\n",
      "Epoch 188: val loss 0.774933\n",
      "\n",
      "Epoch %d: train loss %f 189 0.20483596324920655\n",
      "Epoch 189: val loss 0.745710\n",
      "\n",
      "Epoch %d: train loss %f 190 0.27204797863960267\n",
      "Epoch 190: val loss 0.748885\n",
      "\n",
      "Epoch %d: train loss %f 191 0.18907586336135865\n",
      "Epoch 191: val loss 0.718819\n",
      "\n",
      "Epoch %d: train loss %f 192 0.19043976664543152\n",
      "Epoch 192: val loss 0.744019\n",
      "\n",
      "Epoch %d: train loss %f 193 0.16657144501805304\n",
      "Epoch 193: val loss 0.760914\n",
      "\n",
      "Epoch %d: train loss %f 194 0.16208533868193625\n",
      "Epoch 194: val loss 0.738094\n",
      "\n",
      "Epoch %d: train loss %f 195 0.23548682183027267\n",
      "Epoch 195: val loss 0.751085\n",
      "\n",
      "Epoch %d: train loss %f 196 0.18441932573914527\n",
      "Epoch 196: val loss 0.714292\n",
      "\n",
      "Epoch %d: train loss %f 197 0.19227794781327248\n",
      "Epoch 197: val loss 0.718017\n",
      "\n",
      "Epoch %d: train loss %f 198 0.23707224130630494\n",
      "Epoch 198: val loss 0.719048\n",
      "\n",
      "Epoch %d: train loss %f 199 0.17266912311315535\n",
      "Epoch 199: val loss 0.774486\n",
      "\n",
      "Epoch %d: train loss %f 0 0.7113783836364747\n",
      "Epoch 0: val loss 0.711100\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6951505601406097\n",
      "Epoch 1: val loss 0.701496\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6750885725021363\n",
      "Epoch 2: val loss 0.682027\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6408182084560394\n",
      "Epoch 3: val loss 0.654529\n",
      "\n",
      "Epoch %d: train loss %f 4 0.5992423117160797\n",
      "Epoch 4: val loss 0.606324\n",
      "\n",
      "Epoch %d: train loss %f 5 0.5369654297828674\n",
      "Epoch 5: val loss 0.533200\n",
      "\n",
      "Epoch %d: train loss %f 6 0.4691309183835983\n",
      "Epoch 6: val loss 0.462592\n",
      "\n",
      "Epoch %d: train loss %f 7 0.4233779698610306\n",
      "Epoch 7: val loss 0.428051\n",
      "\n",
      "Epoch %d: train loss %f 8 0.4389026343822479\n",
      "Epoch 8: val loss 0.421619\n",
      "\n",
      "Epoch %d: train loss %f 9 0.4008972719311714\n",
      "Epoch 9: val loss 0.423984\n",
      "\n",
      "Epoch %d: train loss %f 10 0.3864919364452362\n",
      "Epoch 10: val loss 0.433662\n",
      "\n",
      "Epoch %d: train loss %f 11 0.432895302772522\n",
      "Epoch 11: val loss 0.435632\n",
      "\n",
      "Epoch %d: train loss %f 12 0.3970440194010735\n",
      "Epoch 12: val loss 0.436049\n",
      "\n",
      "Epoch %d: train loss %f 13 0.40587030351161957\n",
      "Epoch 13: val loss 0.441651\n",
      "\n",
      "Epoch %d: train loss %f 14 0.44906593263149264\n",
      "Epoch 14: val loss 0.444381\n",
      "\n",
      "Epoch %d: train loss %f 15 0.44271629452705386\n",
      "Epoch 15: val loss 0.436668\n",
      "\n",
      "Epoch %d: train loss %f 16 0.38926978707313536\n",
      "Epoch 16: val loss 0.434359\n",
      "\n",
      "Epoch %d: train loss %f 17 0.39164699912071227\n",
      "Epoch 17: val loss 0.441657\n",
      "\n",
      "Epoch %d: train loss %f 18 0.38834501057863235\n",
      "Epoch 18: val loss 0.443403\n",
      "\n",
      "Epoch %d: train loss %f 19 0.38719540238380434\n",
      "Epoch 19: val loss 0.443598\n",
      "\n",
      "Epoch %d: train loss %f 20 0.4209428310394287\n",
      "Epoch 20: val loss 0.441939\n",
      "\n",
      "Epoch %d: train loss %f 21 0.4472778648138046\n",
      "Epoch 21: val loss 0.440253\n",
      "\n",
      "Epoch %d: train loss %f 22 0.37272907644510267\n",
      "Epoch 22: val loss 0.452750\n",
      "\n",
      "Epoch %d: train loss %f 23 0.38011916875839236\n",
      "Epoch 23: val loss 0.444746\n",
      "\n",
      "Epoch %d: train loss %f 24 0.36703585386276244\n",
      "Epoch 24: val loss 0.439668\n",
      "\n",
      "Epoch %d: train loss %f 25 0.35727318525314333\n",
      "Epoch 25: val loss 0.439812\n",
      "\n",
      "Epoch %d: train loss %f 26 0.3659053236246109\n",
      "Epoch 26: val loss 0.451389\n",
      "\n",
      "Epoch %d: train loss %f 27 0.4172895818948746\n",
      "Epoch 27: val loss 0.461483\n",
      "\n",
      "Epoch %d: train loss %f 28 0.37236698865890505\n",
      "Epoch 28: val loss 0.466559\n",
      "\n",
      "Epoch %d: train loss %f 29 0.36338396221399305\n",
      "Epoch 29: val loss 0.452459\n",
      "\n",
      "Epoch %d: train loss %f 30 0.3680483281612396\n",
      "Epoch 30: val loss 0.448827\n",
      "\n",
      "Epoch %d: train loss %f 31 0.42874104529619217\n",
      "Epoch 31: val loss 0.448828\n",
      "\n",
      "Epoch %d: train loss %f 32 0.42259337902069094\n",
      "Epoch 32: val loss 0.454551\n",
      "\n",
      "Epoch %d: train loss %f 33 0.36203444600105283\n",
      "Epoch 33: val loss 0.451044\n",
      "\n",
      "Epoch %d: train loss %f 34 0.35883145928382876\n",
      "Epoch 34: val loss 0.451641\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4481256514787674\n",
      "Epoch 35: val loss 0.449193\n",
      "\n",
      "Epoch %d: train loss %f 36 0.34223086535930636\n",
      "Epoch 36: val loss 0.441947\n",
      "\n",
      "Epoch %d: train loss %f 37 0.37587698847055434\n",
      "Epoch 37: val loss 0.441822\n",
      "\n",
      "Epoch %d: train loss %f 38 0.34801028072834017\n",
      "Epoch 38: val loss 0.449485\n",
      "\n",
      "Epoch %d: train loss %f 39 0.3952145233750343\n",
      "Epoch 39: val loss 0.462416\n",
      "\n",
      "Epoch %d: train loss %f 40 0.38441653102636336\n",
      "Epoch 40: val loss 0.463258\n",
      "\n",
      "Epoch %d: train loss %f 41 0.3862858548760414\n",
      "Epoch 41: val loss 0.442606\n",
      "\n",
      "Epoch %d: train loss %f 42 0.35216771364212035\n",
      "Epoch 42: val loss 0.435837\n",
      "\n",
      "Epoch %d: train loss %f 43 0.3977037474513054\n",
      "Epoch 43: val loss 0.436272\n",
      "\n",
      "Epoch %d: train loss %f 44 0.3570818156003952\n",
      "Epoch 44: val loss 0.424414\n",
      "\n",
      "Epoch %d: train loss %f 45 0.3551061138510704\n",
      "Epoch 45: val loss 0.418645\n",
      "\n",
      "Epoch %d: train loss %f 46 0.3434585139155388\n",
      "Epoch 46: val loss 0.434330\n",
      "\n",
      "Epoch %d: train loss %f 47 0.350461345911026\n",
      "Epoch 47: val loss 0.438853\n",
      "\n",
      "Epoch %d: train loss %f 48 0.38770621418952944\n",
      "Epoch 48: val loss 0.445862\n",
      "\n",
      "Epoch %d: train loss %f 49 0.347076877951622\n",
      "Epoch 49: val loss 0.439245\n",
      "\n",
      "Epoch %d: train loss %f 50 0.34842790812253954\n",
      "Epoch 50: val loss 0.431292\n",
      "\n",
      "Epoch %d: train loss %f 51 0.36939655244350433\n",
      "Epoch 51: val loss 0.430607\n",
      "\n",
      "Epoch %d: train loss %f 52 0.32603654712438584\n",
      "Epoch 52: val loss 0.425591\n",
      "\n",
      "Epoch %d: train loss %f 53 0.33968123644590376\n",
      "Epoch 53: val loss 0.429810\n",
      "\n",
      "Epoch %d: train loss %f 54 0.5008247002959252\n",
      "Epoch 54: val loss 0.428385\n",
      "\n",
      "Epoch %d: train loss %f 55 0.33305518627166747\n",
      "Epoch 55: val loss 0.414091\n",
      "\n",
      "Epoch %d: train loss %f 56 0.33458097726106645\n",
      "Epoch 56: val loss 0.419979\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4048871353268623\n",
      "Epoch 57: val loss 0.432898\n",
      "\n",
      "Epoch %d: train loss %f 58 0.32596650123596194\n",
      "Epoch 58: val loss 0.444419\n",
      "\n",
      "Epoch %d: train loss %f 59 0.33130844235420226\n",
      "Epoch 59: val loss 0.447922\n",
      "\n",
      "Epoch %d: train loss %f 60 0.3277335107326508\n",
      "Epoch 60: val loss 0.463530\n",
      "\n",
      "Epoch %d: train loss %f 61 0.3583536848425865\n",
      "Epoch 61: val loss 0.459039\n",
      "\n",
      "Epoch %d: train loss %f 62 0.3532806098461151\n",
      "Epoch 62: val loss 0.449250\n",
      "\n",
      "Epoch %d: train loss %f 63 0.321274396777153\n",
      "Epoch 63: val loss 0.442837\n",
      "\n",
      "Epoch %d: train loss %f 64 0.35669782012701035\n",
      "Epoch 64: val loss 0.453273\n",
      "\n",
      "Epoch %d: train loss %f 65 0.3473175972700119\n",
      "Epoch 65: val loss 0.463827\n",
      "\n",
      "Epoch %d: train loss %f 66 0.34394845068454744\n",
      "Epoch 66: val loss 0.451725\n",
      "\n",
      "Epoch %d: train loss %f 67 0.30889474898576735\n",
      "Epoch 67: val loss 0.448195\n",
      "\n",
      "Epoch %d: train loss %f 68 0.33466940820217134\n",
      "Epoch 68: val loss 0.453136\n",
      "\n",
      "Epoch %d: train loss %f 69 0.36134308725595476\n",
      "Epoch 69: val loss 0.473683\n",
      "\n",
      "Epoch %d: train loss %f 70 0.37078275382518766\n",
      "Epoch 70: val loss 0.470898\n",
      "\n",
      "Epoch %d: train loss %f 71 0.38789546191692353\n",
      "Epoch 71: val loss 0.466127\n",
      "\n",
      "Epoch %d: train loss %f 72 0.32402697056531904\n",
      "Epoch 72: val loss 0.458584\n",
      "\n",
      "Epoch %d: train loss %f 73 0.347747266292572\n",
      "Epoch 73: val loss 0.455135\n",
      "\n",
      "Epoch %d: train loss %f 74 0.32186952531337737\n",
      "Epoch 74: val loss 0.442780\n",
      "\n",
      "Epoch %d: train loss %f 75 0.323020002245903\n",
      "Epoch 75: val loss 0.457702\n",
      "\n",
      "Epoch %d: train loss %f 76 0.3189533233642578\n",
      "Epoch 76: val loss 0.472771\n",
      "\n",
      "Epoch %d: train loss %f 77 0.36198976784944537\n",
      "Epoch 77: val loss 0.490317\n",
      "\n",
      "Epoch %d: train loss %f 78 0.29598053842782973\n",
      "Epoch 78: val loss 0.480072\n",
      "\n",
      "Epoch %d: train loss %f 79 0.3159807801246643\n",
      "Epoch 79: val loss 0.487131\n",
      "\n",
      "Epoch %d: train loss %f 80 0.2931437656283379\n",
      "Epoch 80: val loss 0.485022\n",
      "\n",
      "Epoch %d: train loss %f 81 0.38853471279144286\n",
      "Epoch 81: val loss 0.482769\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3392479941248894\n",
      "Epoch 82: val loss 0.483451\n",
      "\n",
      "Epoch %d: train loss %f 83 0.31831719130277636\n",
      "Epoch 83: val loss 0.459010\n",
      "\n",
      "Epoch %d: train loss %f 84 0.36235389709472654\n",
      "Epoch 84: val loss 0.445202\n",
      "\n",
      "Epoch %d: train loss %f 85 0.3059076100587845\n",
      "Epoch 85: val loss 0.435086\n",
      "\n",
      "Epoch %d: train loss %f 86 0.4097991153597832\n",
      "Epoch 86: val loss 0.433617\n",
      "\n",
      "Epoch %d: train loss %f 87 0.32975030690431595\n",
      "Epoch 87: val loss 0.452139\n",
      "\n",
      "Epoch %d: train loss %f 88 0.31105553060770036\n",
      "Epoch 88: val loss 0.451743\n",
      "\n",
      "Epoch %d: train loss %f 89 0.3398677885532379\n",
      "Epoch 89: val loss 0.464977\n",
      "\n",
      "Epoch %d: train loss %f 90 0.2979391157627106\n",
      "Epoch 90: val loss 0.489246\n",
      "\n",
      "Epoch %d: train loss %f 91 0.30474280267953874\n",
      "Epoch 91: val loss 0.484717\n",
      "\n",
      "Epoch %d: train loss %f 92 0.30838820338249207\n",
      "Epoch 92: val loss 0.478204\n",
      "\n",
      "Epoch %d: train loss %f 93 0.29905862361192703\n",
      "Epoch 93: val loss 0.479725\n",
      "\n",
      "Epoch %d: train loss %f 94 0.286639666557312\n",
      "Epoch 94: val loss 0.490800\n",
      "\n",
      "Epoch %d: train loss %f 95 0.374636647105217\n",
      "Epoch 95: val loss 0.492573\n",
      "\n",
      "Epoch %d: train loss %f 96 0.2820870891213417\n",
      "Epoch 96: val loss 0.498400\n",
      "\n",
      "Epoch %d: train loss %f 97 0.27531361281871797\n",
      "Epoch 97: val loss 0.486647\n",
      "\n",
      "Epoch %d: train loss %f 98 0.3994615271687508\n",
      "Epoch 98: val loss 0.480031\n",
      "\n",
      "Epoch %d: train loss %f 99 0.2824287906289101\n",
      "Epoch 99: val loss 0.506860\n",
      "\n",
      "Epoch %d: train loss %f 100 0.35229675769805907\n",
      "Epoch 100: val loss 0.487420\n",
      "\n",
      "Epoch %d: train loss %f 101 0.27415843307971954\n",
      "Epoch 101: val loss 0.485045\n",
      "\n",
      "Epoch %d: train loss %f 102 0.30164407938718796\n",
      "Epoch 102: val loss 0.474514\n",
      "\n",
      "Epoch %d: train loss %f 103 0.31281218975782393\n",
      "Epoch 103: val loss 0.481582\n",
      "\n",
      "Epoch %d: train loss %f 104 0.29144873172044755\n",
      "Epoch 104: val loss 0.490245\n",
      "\n",
      "Epoch %d: train loss %f 105 0.29578021466732024\n",
      "Epoch 105: val loss 0.498046\n",
      "\n",
      "Epoch %d: train loss %f 106 0.26335425227880477\n",
      "Epoch 106: val loss 0.490840\n",
      "\n",
      "Epoch %d: train loss %f 107 0.2873356804251671\n",
      "Epoch 107: val loss 0.494807\n",
      "\n",
      "Epoch %d: train loss %f 108 0.2713026702404022\n",
      "Epoch 108: val loss 0.492219\n",
      "\n",
      "Epoch %d: train loss %f 109 0.2606682598590851\n",
      "Epoch 109: val loss 0.496561\n",
      "\n",
      "Epoch %d: train loss %f 110 0.2883480504155159\n",
      "Epoch 110: val loss 0.502800\n",
      "\n",
      "Epoch %d: train loss %f 111 0.26596577614545824\n",
      "Epoch 111: val loss 0.496559\n",
      "\n",
      "Epoch %d: train loss %f 112 0.31132975667715074\n",
      "Epoch 112: val loss 0.507058\n",
      "\n",
      "Epoch %d: train loss %f 113 0.2901004239916801\n",
      "Epoch 113: val loss 0.505510\n",
      "\n",
      "Epoch %d: train loss %f 114 0.2737241521477699\n",
      "Epoch 114: val loss 0.519662\n",
      "\n",
      "Epoch %d: train loss %f 115 0.2900997929275036\n",
      "Epoch 115: val loss 0.510675\n",
      "\n",
      "Epoch %d: train loss %f 116 0.2621737137436867\n",
      "Epoch 116: val loss 0.504654\n",
      "\n",
      "Epoch %d: train loss %f 117 0.29619851261377333\n",
      "Epoch 117: val loss 0.509575\n",
      "\n",
      "Epoch %d: train loss %f 118 0.27223493084311484\n",
      "Epoch 118: val loss 0.501965\n",
      "\n",
      "Epoch %d: train loss %f 119 0.27498390078544616\n",
      "Epoch 119: val loss 0.491423\n",
      "\n",
      "Epoch %d: train loss %f 120 0.26024051010608673\n",
      "Epoch 120: val loss 0.500466\n",
      "\n",
      "Epoch %d: train loss %f 121 0.25956769436597826\n",
      "Epoch 121: val loss 0.502839\n",
      "\n",
      "Epoch %d: train loss %f 122 0.240668486058712\n",
      "Epoch 122: val loss 0.519218\n",
      "\n",
      "Epoch %d: train loss %f 123 0.25163356959819794\n",
      "Epoch 123: val loss 0.512835\n",
      "\n",
      "Epoch %d: train loss %f 124 0.3315915778279305\n",
      "Epoch 124: val loss 0.507589\n",
      "\n",
      "Epoch %d: train loss %f 125 0.316847862303257\n",
      "Epoch 125: val loss 0.515606\n",
      "\n",
      "Epoch %d: train loss %f 126 0.2540253892540932\n",
      "Epoch 126: val loss 0.524806\n",
      "\n",
      "Epoch %d: train loss %f 127 0.41102674677968026\n",
      "Epoch 127: val loss 0.510970\n",
      "\n",
      "Epoch %d: train loss %f 128 0.25218559354543685\n",
      "Epoch 128: val loss 0.492746\n",
      "\n",
      "Epoch %d: train loss %f 129 0.32076538652181624\n",
      "Epoch 129: val loss 0.482022\n",
      "\n",
      "Epoch %d: train loss %f 130 0.24514950960874557\n",
      "Epoch 130: val loss 0.566231\n",
      "\n",
      "Epoch %d: train loss %f 131 0.25551619976758955\n",
      "Epoch 131: val loss 0.578787\n",
      "\n",
      "Epoch %d: train loss %f 132 0.2711344167590141\n",
      "Epoch 132: val loss 0.560964\n",
      "\n",
      "Epoch %d: train loss %f 133 0.25080968290567396\n",
      "Epoch 133: val loss 0.551547\n",
      "\n",
      "Epoch %d: train loss %f 134 0.24086357206106185\n",
      "Epoch 134: val loss 0.527867\n",
      "\n",
      "Epoch %d: train loss %f 135 0.2502652287483215\n",
      "Epoch 135: val loss 0.501997\n",
      "\n",
      "Epoch %d: train loss %f 136 0.25254043787717817\n",
      "Epoch 136: val loss 0.532022\n",
      "\n",
      "Epoch %d: train loss %f 137 0.22165496945381163\n",
      "Epoch 137: val loss 0.534002\n",
      "\n",
      "Epoch %d: train loss %f 138 0.27565584182739256\n",
      "Epoch 138: val loss 0.512221\n",
      "\n",
      "Epoch %d: train loss %f 139 0.2695633038878441\n",
      "Epoch 139: val loss 0.503158\n",
      "\n",
      "Epoch %d: train loss %f 140 0.2519319862127304\n",
      "Epoch 140: val loss 0.507552\n",
      "\n",
      "Epoch %d: train loss %f 141 0.26057131588459015\n",
      "Epoch 141: val loss 0.503479\n",
      "\n",
      "Epoch %d: train loss %f 142 0.2547623425722122\n",
      "Epoch 142: val loss 0.529176\n",
      "\n",
      "Epoch %d: train loss %f 143 0.26281149685382843\n",
      "Epoch 143: val loss 0.526472\n",
      "\n",
      "Epoch %d: train loss %f 144 0.2595086991786957\n",
      "Epoch 144: val loss 0.513893\n",
      "\n",
      "Epoch %d: train loss %f 145 0.2695458948612213\n",
      "Epoch 145: val loss 0.497156\n",
      "\n",
      "Epoch %d: train loss %f 146 0.29195627719163897\n",
      "Epoch 146: val loss 0.518034\n",
      "\n",
      "Epoch %d: train loss %f 147 0.2477661982178688\n",
      "Epoch 147: val loss 0.586573\n",
      "\n",
      "Epoch %d: train loss %f 148 0.2685063689947128\n",
      "Epoch 148: val loss 0.564786\n",
      "\n",
      "Epoch %d: train loss %f 149 0.2924016073346138\n",
      "Epoch 149: val loss 0.536677\n",
      "\n",
      "Epoch %d: train loss %f 150 0.26148051768541336\n",
      "Epoch 150: val loss 0.509327\n",
      "\n",
      "Epoch %d: train loss %f 151 0.2926826238632202\n",
      "Epoch 151: val loss 0.479721\n",
      "\n",
      "Epoch %d: train loss %f 152 0.2414302796125412\n",
      "Epoch 152: val loss 0.514052\n",
      "\n",
      "Epoch %d: train loss %f 153 0.40909286588430405\n",
      "Epoch 153: val loss 0.498325\n",
      "\n",
      "Epoch %d: train loss %f 154 0.295876806974411\n",
      "Epoch 154: val loss 0.468686\n",
      "\n",
      "Epoch %d: train loss %f 155 0.2550275340676308\n",
      "Epoch 155: val loss 0.510133\n",
      "\n",
      "Epoch %d: train loss %f 156 0.24230277687311172\n",
      "Epoch 156: val loss 0.499265\n",
      "\n",
      "Epoch %d: train loss %f 157 0.22506529688835145\n",
      "Epoch 157: val loss 0.509966\n",
      "\n",
      "Epoch %d: train loss %f 158 0.2689554154872894\n",
      "Epoch 158: val loss 0.510949\n",
      "\n",
      "Epoch %d: train loss %f 159 0.22175549566745759\n",
      "Epoch 159: val loss 0.502614\n",
      "\n",
      "Epoch %d: train loss %f 160 0.2422462522983551\n",
      "Epoch 160: val loss 0.491924\n",
      "\n",
      "Epoch %d: train loss %f 161 0.24220550954341888\n",
      "Epoch 161: val loss 0.488193\n",
      "\n",
      "Epoch %d: train loss %f 162 0.3036100000143051\n",
      "Epoch 162: val loss 0.508819\n",
      "\n",
      "Epoch %d: train loss %f 163 0.24176297336816788\n",
      "Epoch 163: val loss 0.531007\n",
      "\n",
      "Epoch %d: train loss %f 164 0.2452288880944252\n",
      "Epoch 164: val loss 0.544426\n",
      "\n",
      "Epoch %d: train loss %f 165 0.2653962686657906\n",
      "Epoch 165: val loss 0.526208\n",
      "\n",
      "Epoch %d: train loss %f 166 0.25946656838059423\n",
      "Epoch 166: val loss 0.489145\n",
      "\n",
      "Epoch %d: train loss %f 167 0.23541440665721894\n",
      "Epoch 167: val loss 0.503898\n",
      "\n",
      "Epoch %d: train loss %f 168 0.23513175845146178\n",
      "Epoch 168: val loss 0.495647\n",
      "\n",
      "Epoch %d: train loss %f 169 0.215040223300457\n",
      "Epoch 169: val loss 0.518089\n",
      "\n",
      "Epoch %d: train loss %f 170 0.23620490059256555\n",
      "Epoch 170: val loss 0.558000\n",
      "\n",
      "Epoch %d: train loss %f 171 0.232343690097332\n",
      "Epoch 171: val loss 0.533921\n",
      "\n",
      "Epoch %d: train loss %f 172 0.24581021964550018\n",
      "Epoch 172: val loss 0.539503\n",
      "\n",
      "Epoch %d: train loss %f 173 0.23288195133209227\n",
      "Epoch 173: val loss 0.550406\n",
      "\n",
      "Epoch %d: train loss %f 174 0.41369786858558655\n",
      "Epoch 174: val loss 0.539011\n",
      "\n",
      "Epoch %d: train loss %f 175 0.24223264306783676\n",
      "Epoch 175: val loss 0.564589\n",
      "\n",
      "Epoch %d: train loss %f 176 0.22857496589422227\n",
      "Epoch 176: val loss 0.542531\n",
      "\n",
      "Epoch %d: train loss %f 177 0.28837084770202637\n",
      "Epoch 177: val loss 0.542432\n",
      "\n",
      "Epoch %d: train loss %f 178 0.2727207213640213\n",
      "Epoch 178: val loss 0.544119\n",
      "\n",
      "Epoch %d: train loss %f 179 0.24588219970464706\n",
      "Epoch 179: val loss 0.595430\n",
      "\n",
      "Epoch %d: train loss %f 180 0.21835457980632783\n",
      "Epoch 180: val loss 0.606667\n",
      "\n",
      "Epoch %d: train loss %f 181 0.22884362414479256\n",
      "Epoch 181: val loss 0.562031\n",
      "\n",
      "Epoch %d: train loss %f 182 0.25560160875320437\n",
      "Epoch 182: val loss 0.560411\n",
      "\n",
      "Epoch %d: train loss %f 183 0.21451688706874847\n",
      "Epoch 183: val loss 0.569556\n",
      "\n",
      "Epoch %d: train loss %f 184 0.2647412776947021\n",
      "Epoch 184: val loss 0.551129\n",
      "\n",
      "Epoch %d: train loss %f 185 0.2491341143846512\n",
      "Epoch 185: val loss 0.513647\n",
      "\n",
      "Epoch %d: train loss %f 186 0.21724869757890702\n",
      "Epoch 186: val loss 0.523266\n",
      "\n",
      "Epoch %d: train loss %f 187 0.21089940816164016\n",
      "Epoch 187: val loss 0.519259\n",
      "\n",
      "Epoch %d: train loss %f 188 0.2722043551504612\n",
      "Epoch 188: val loss 0.524653\n",
      "\n",
      "Epoch %d: train loss %f 189 0.21613771468400955\n",
      "Epoch 189: val loss 0.523069\n",
      "\n",
      "Epoch %d: train loss %f 190 0.227408704161644\n",
      "Epoch 190: val loss 0.533794\n",
      "\n",
      "Epoch %d: train loss %f 191 0.19537290781736374\n",
      "Epoch 191: val loss 0.542325\n",
      "\n",
      "Epoch %d: train loss %f 192 0.31455743610858916\n",
      "Epoch 192: val loss 0.534354\n",
      "\n",
      "Epoch %d: train loss %f 193 0.25395176038146017\n",
      "Epoch 193: val loss 0.544492\n",
      "\n",
      "Epoch %d: train loss %f 194 0.2143261805176735\n",
      "Epoch 194: val loss 0.563417\n",
      "\n",
      "Epoch %d: train loss %f 195 0.2025159254670143\n",
      "Epoch 195: val loss 0.574572\n",
      "\n",
      "Epoch %d: train loss %f 196 0.2133065439760685\n",
      "Epoch 196: val loss 0.546053\n",
      "\n",
      "Epoch %d: train loss %f 197 0.2361026406288147\n",
      "Epoch 197: val loss 0.522275\n",
      "\n",
      "Epoch %d: train loss %f 198 0.22594642490148545\n",
      "Epoch 198: val loss 0.508550\n",
      "\n",
      "Epoch %d: train loss %f 199 0.25118699446320536\n",
      "Epoch 199: val loss 0.511141\n",
      "\n",
      "Epoch %d: train loss %f 0 0.7010592460632324\n",
      "Epoch 0: val loss 0.700849\n",
      "\n",
      "Epoch %d: train loss %f 1 0.690695458650589\n",
      "Epoch 1: val loss 0.692927\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6733073055744171\n",
      "Epoch 2: val loss 0.681409\n",
      "\n",
      "Epoch %d: train loss %f 3 0.646429044008255\n",
      "Epoch 3: val loss 0.655139\n",
      "\n",
      "Epoch %d: train loss %f 4 0.5981183171272277\n",
      "Epoch 4: val loss 0.607444\n",
      "\n",
      "Epoch %d: train loss %f 5 0.53457670211792\n",
      "Epoch 5: val loss 0.542377\n",
      "\n",
      "Epoch %d: train loss %f 6 0.4884991616010666\n",
      "Epoch 6: val loss 0.482168\n",
      "\n",
      "Epoch %d: train loss %f 7 0.4195378661155701\n",
      "Epoch 7: val loss 0.438500\n",
      "\n",
      "Epoch %d: train loss %f 8 0.4002402126789093\n",
      "Epoch 8: val loss 0.468683\n",
      "\n",
      "Epoch %d: train loss %f 9 0.3831643000245094\n",
      "Epoch 9: val loss 0.480822\n",
      "\n",
      "Epoch %d: train loss %f 10 0.4698077917098999\n",
      "Epoch 10: val loss 0.457151\n",
      "\n",
      "Epoch %d: train loss %f 11 0.4457964077591896\n",
      "Epoch 11: val loss 0.455738\n",
      "\n",
      "Epoch %d: train loss %f 12 0.3922337740659714\n",
      "Epoch 12: val loss 0.457610\n",
      "\n",
      "Epoch %d: train loss %f 13 0.36721582859754565\n",
      "Epoch 13: val loss 0.480880\n",
      "\n",
      "Epoch %d: train loss %f 14 0.3686280831694603\n",
      "Epoch 14: val loss 0.463569\n",
      "\n",
      "Epoch %d: train loss %f 15 0.4126908302307129\n",
      "Epoch 15: val loss 0.462633\n",
      "\n",
      "Epoch %d: train loss %f 16 0.42603746950626376\n",
      "Epoch 16: val loss 0.515104\n",
      "\n",
      "Epoch %d: train loss %f 17 0.3951457589864731\n",
      "Epoch 17: val loss 0.435832\n",
      "\n",
      "Epoch %d: train loss %f 18 0.38993721902370454\n",
      "Epoch 18: val loss 0.440611\n",
      "\n",
      "Epoch %d: train loss %f 19 0.3527013748884201\n",
      "Epoch 19: val loss 0.489874\n",
      "\n",
      "Epoch %d: train loss %f 20 0.3628396034240723\n",
      "Epoch 20: val loss 0.484721\n",
      "\n",
      "Epoch %d: train loss %f 21 0.36925541907548903\n",
      "Epoch 21: val loss 0.483490\n",
      "\n",
      "Epoch %d: train loss %f 22 0.39070054739713667\n",
      "Epoch 22: val loss 0.479140\n",
      "\n",
      "Epoch %d: train loss %f 23 0.3861193209886551\n",
      "Epoch 23: val loss 0.488012\n",
      "\n",
      "Epoch %d: train loss %f 24 0.3412905901670456\n",
      "Epoch 24: val loss 0.500911\n",
      "\n",
      "Epoch %d: train loss %f 25 0.38583649396896363\n",
      "Epoch 25: val loss 0.482788\n",
      "\n",
      "Epoch %d: train loss %f 26 0.3410216301679611\n",
      "Epoch 26: val loss 0.497301\n",
      "\n",
      "Epoch %d: train loss %f 27 0.3360858760774136\n",
      "Epoch 27: val loss 0.473678\n",
      "\n",
      "Epoch %d: train loss %f 28 0.3408373430371284\n",
      "Epoch 28: val loss 0.468955\n",
      "\n",
      "Epoch %d: train loss %f 29 0.36885173469781873\n",
      "Epoch 29: val loss 0.485616\n",
      "\n",
      "Epoch %d: train loss %f 30 0.35763782262802124\n",
      "Epoch 30: val loss 0.482209\n",
      "\n",
      "Epoch %d: train loss %f 31 0.3554606631398201\n",
      "Epoch 31: val loss 0.517398\n",
      "\n",
      "Epoch %d: train loss %f 32 0.3558784455060959\n",
      "Epoch 32: val loss 0.508581\n",
      "\n",
      "Epoch %d: train loss %f 33 0.33578380942344666\n",
      "Epoch 33: val loss 0.519827\n",
      "\n",
      "Epoch %d: train loss %f 34 0.3717759534716606\n",
      "Epoch 34: val loss 0.521994\n",
      "\n",
      "Epoch %d: train loss %f 35 0.3354864284396172\n",
      "Epoch 35: val loss 0.507794\n",
      "\n",
      "Epoch %d: train loss %f 36 0.3313142418861389\n",
      "Epoch 36: val loss 0.504422\n",
      "\n",
      "Epoch %d: train loss %f 37 0.3725635349750519\n",
      "Epoch 37: val loss 0.520232\n",
      "\n",
      "Epoch %d: train loss %f 38 0.32512289732694627\n",
      "Epoch 38: val loss 0.537994\n",
      "\n",
      "Epoch %d: train loss %f 39 0.3398487016558647\n",
      "Epoch 39: val loss 0.525272\n",
      "\n",
      "Epoch %d: train loss %f 40 0.3271414652466774\n",
      "Epoch 40: val loss 0.524493\n",
      "\n",
      "Epoch %d: train loss %f 41 0.30678649693727494\n",
      "Epoch 41: val loss 0.536618\n",
      "\n",
      "Epoch %d: train loss %f 42 0.3709574341773987\n",
      "Epoch 42: val loss 0.530232\n",
      "\n",
      "Epoch %d: train loss %f 43 0.3390104413032532\n",
      "Epoch 43: val loss 0.518686\n",
      "\n",
      "Epoch %d: train loss %f 44 0.3361261963844299\n",
      "Epoch 44: val loss 0.525448\n",
      "\n",
      "Epoch %d: train loss %f 45 0.316975525021553\n",
      "Epoch 45: val loss 0.511751\n",
      "\n",
      "Epoch %d: train loss %f 46 0.3287246450781822\n",
      "Epoch 46: val loss 0.520644\n",
      "\n",
      "Epoch %d: train loss %f 47 0.2956412747502327\n",
      "Epoch 47: val loss 0.533995\n",
      "\n",
      "Epoch %d: train loss %f 48 0.3034913420677185\n",
      "Epoch 48: val loss 0.534578\n",
      "\n",
      "Epoch %d: train loss %f 49 0.29510038942098615\n",
      "Epoch 49: val loss 0.561024\n",
      "\n",
      "Epoch %d: train loss %f 50 0.4081516370177269\n",
      "Epoch 50: val loss 0.619744\n",
      "\n",
      "Epoch %d: train loss %f 51 0.31834834069013596\n",
      "Epoch 51: val loss 0.551751\n",
      "\n",
      "Epoch %d: train loss %f 52 0.3293921187520027\n",
      "Epoch 52: val loss 0.503155\n",
      "\n",
      "Epoch %d: train loss %f 53 0.31051626056432724\n",
      "Epoch 53: val loss 0.545703\n",
      "\n",
      "Epoch %d: train loss %f 54 0.3088710650801659\n",
      "Epoch 54: val loss 0.536117\n",
      "\n",
      "Epoch %d: train loss %f 55 0.287366946041584\n",
      "Epoch 55: val loss 0.542727\n",
      "\n",
      "Epoch %d: train loss %f 56 0.32974747866392135\n",
      "Epoch 56: val loss 0.555307\n",
      "\n",
      "Epoch %d: train loss %f 57 0.29016842097043993\n",
      "Epoch 57: val loss 0.547633\n",
      "\n",
      "Epoch %d: train loss %f 58 0.3519780546426773\n",
      "Epoch 58: val loss 0.547722\n",
      "\n",
      "Epoch %d: train loss %f 59 0.3745360836386681\n",
      "Epoch 59: val loss 0.582960\n",
      "\n",
      "Epoch %d: train loss %f 60 0.314045837521553\n",
      "Epoch 60: val loss 0.625106\n",
      "\n",
      "Epoch %d: train loss %f 61 0.30553423464298246\n",
      "Epoch 61: val loss 0.566190\n",
      "\n",
      "Epoch %d: train loss %f 62 0.30210260599851607\n",
      "Epoch 62: val loss 0.573994\n",
      "\n",
      "Epoch %d: train loss %f 63 0.3037882953882217\n",
      "Epoch 63: val loss 0.603547\n",
      "\n",
      "Epoch %d: train loss %f 64 0.32375961244106294\n",
      "Epoch 64: val loss 0.589722\n",
      "\n",
      "Epoch %d: train loss %f 65 0.2769344873726368\n",
      "Epoch 65: val loss 0.584560\n",
      "\n",
      "Epoch %d: train loss %f 66 0.28748766928911207\n",
      "Epoch 66: val loss 0.599279\n",
      "\n",
      "Epoch %d: train loss %f 67 0.2814407482743263\n",
      "Epoch 67: val loss 0.590053\n",
      "\n",
      "Epoch %d: train loss %f 68 0.3547523871064186\n",
      "Epoch 68: val loss 0.604373\n",
      "\n",
      "Epoch %d: train loss %f 69 0.302336835116148\n",
      "Epoch 69: val loss 0.605535\n",
      "\n",
      "Epoch %d: train loss %f 70 0.29805206060409545\n",
      "Epoch 70: val loss 0.663397\n",
      "\n",
      "Epoch %d: train loss %f 71 0.31946893632411955\n",
      "Epoch 71: val loss 0.608126\n",
      "\n",
      "Epoch %d: train loss %f 72 0.2981365993618965\n",
      "Epoch 72: val loss 0.617935\n",
      "\n",
      "Epoch %d: train loss %f 73 0.2715394489467144\n",
      "Epoch 73: val loss 0.645743\n",
      "\n",
      "Epoch %d: train loss %f 74 0.3237662002444267\n",
      "Epoch 74: val loss 0.607880\n",
      "\n",
      "Epoch %d: train loss %f 75 0.280813193321228\n",
      "Epoch 75: val loss 0.609701\n",
      "\n",
      "Epoch %d: train loss %f 76 0.32735800594091413\n",
      "Epoch 76: val loss 0.584383\n",
      "\n",
      "Epoch %d: train loss %f 77 0.279543686658144\n",
      "Epoch 77: val loss 0.605539\n",
      "\n",
      "Epoch %d: train loss %f 78 0.32069066166877747\n",
      "Epoch 78: val loss 0.634257\n",
      "\n",
      "Epoch %d: train loss %f 79 0.2934457689523697\n",
      "Epoch 79: val loss 0.568546\n",
      "\n",
      "Epoch %d: train loss %f 80 0.34514486193656924\n",
      "Epoch 80: val loss 0.607367\n",
      "\n",
      "Epoch %d: train loss %f 81 0.2625083401799202\n",
      "Epoch 81: val loss 0.566588\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3038233548402786\n",
      "Epoch 82: val loss 0.567970\n",
      "\n",
      "Epoch %d: train loss %f 83 0.3307938352227211\n",
      "Epoch 83: val loss 0.574843\n",
      "\n",
      "Epoch %d: train loss %f 84 0.28269803076982497\n",
      "Epoch 84: val loss 0.589887\n",
      "\n",
      "Epoch %d: train loss %f 85 0.30608066320419314\n",
      "Epoch 85: val loss 0.654994\n",
      "\n",
      "Epoch %d: train loss %f 86 0.30513221174478533\n",
      "Epoch 86: val loss 0.608599\n",
      "\n",
      "Epoch %d: train loss %f 87 0.28643185943365096\n",
      "Epoch 87: val loss 0.633056\n",
      "\n",
      "Epoch %d: train loss %f 88 0.28858437240123747\n",
      "Epoch 88: val loss 0.681040\n",
      "\n",
      "Epoch %d: train loss %f 89 0.33838808834552764\n",
      "Epoch 89: val loss 0.661788\n",
      "\n",
      "Epoch %d: train loss %f 90 0.26682418659329415\n",
      "Epoch 90: val loss 0.664177\n",
      "\n",
      "Epoch %d: train loss %f 91 0.2832298934459686\n",
      "Epoch 91: val loss 0.654786\n",
      "\n",
      "Epoch %d: train loss %f 92 0.2610688365995884\n",
      "Epoch 92: val loss 0.646565\n",
      "\n",
      "Epoch %d: train loss %f 93 0.24806229248642922\n",
      "Epoch 93: val loss 0.662220\n",
      "\n",
      "Epoch %d: train loss %f 94 0.2717076979577541\n",
      "Epoch 94: val loss 0.645594\n",
      "\n",
      "Epoch %d: train loss %f 95 0.274007485806942\n",
      "Epoch 95: val loss 0.645062\n",
      "\n",
      "Epoch %d: train loss %f 96 0.2593835100531578\n",
      "Epoch 96: val loss 0.654868\n",
      "\n",
      "Epoch %d: train loss %f 97 0.272667196393013\n",
      "Epoch 97: val loss 0.660937\n",
      "\n",
      "Epoch %d: train loss %f 98 0.24285498335957528\n",
      "Epoch 98: val loss 0.683716\n",
      "\n",
      "Epoch %d: train loss %f 99 0.2799095168709755\n",
      "Epoch 99: val loss 0.687364\n",
      "\n",
      "Epoch %d: train loss %f 100 0.2386063888669014\n",
      "Epoch 100: val loss 0.680515\n",
      "\n",
      "Epoch %d: train loss %f 101 0.2641749694943428\n",
      "Epoch 101: val loss 0.680397\n",
      "\n",
      "Epoch %d: train loss %f 102 0.23172783218324183\n",
      "Epoch 102: val loss 0.697983\n",
      "\n",
      "Epoch %d: train loss %f 103 0.24741568714380263\n",
      "Epoch 103: val loss 0.712603\n",
      "\n",
      "Epoch %d: train loss %f 104 0.248196392506361\n",
      "Epoch 104: val loss 0.712331\n",
      "\n",
      "Epoch %d: train loss %f 105 0.3034125827252865\n",
      "Epoch 105: val loss 0.715237\n",
      "\n",
      "Epoch %d: train loss %f 106 0.22561885379254817\n",
      "Epoch 106: val loss 0.747689\n",
      "\n",
      "Epoch %d: train loss %f 107 0.2745242863893509\n",
      "Epoch 107: val loss 0.713140\n",
      "\n",
      "Epoch %d: train loss %f 108 0.22932216599583627\n",
      "Epoch 108: val loss 0.734212\n",
      "\n",
      "Epoch %d: train loss %f 109 0.2457273669540882\n",
      "Epoch 109: val loss 0.688844\n",
      "\n",
      "Epoch %d: train loss %f 110 0.21890835985541343\n",
      "Epoch 110: val loss 0.710559\n",
      "\n",
      "Epoch %d: train loss %f 111 0.21833430379629135\n",
      "Epoch 111: val loss 0.699264\n",
      "\n",
      "Epoch %d: train loss %f 112 0.2512281835079193\n",
      "Epoch 112: val loss 0.709524\n",
      "\n",
      "Epoch %d: train loss %f 113 0.2296009141020477\n",
      "Epoch 113: val loss 0.735137\n",
      "\n",
      "Epoch %d: train loss %f 114 0.2573798321187496\n",
      "Epoch 114: val loss 0.712568\n",
      "\n",
      "Epoch %d: train loss %f 115 0.2984153687953949\n",
      "Epoch 115: val loss 0.682298\n",
      "\n",
      "Epoch %d: train loss %f 116 0.25057811439037325\n",
      "Epoch 116: val loss 0.735010\n",
      "\n",
      "Epoch %d: train loss %f 117 0.2606888547539711\n",
      "Epoch 117: val loss 0.788883\n",
      "\n",
      "Epoch %d: train loss %f 118 0.2750658705830574\n",
      "Epoch 118: val loss 0.692243\n",
      "\n",
      "Epoch %d: train loss %f 119 0.245855450630188\n",
      "Epoch 119: val loss 0.690491\n",
      "\n",
      "Epoch %d: train loss %f 120 0.3228162005543709\n",
      "Epoch 120: val loss 0.721709\n",
      "\n",
      "Epoch %d: train loss %f 121 0.23863702788949012\n",
      "Epoch 121: val loss 0.761111\n",
      "\n",
      "Epoch %d: train loss %f 122 0.2512755610048771\n",
      "Epoch 122: val loss 0.679284\n",
      "\n",
      "Epoch %d: train loss %f 123 0.29548329412937163\n",
      "Epoch 123: val loss 0.711745\n",
      "\n",
      "Epoch %d: train loss %f 124 0.25134492442011835\n",
      "Epoch 124: val loss 0.701177\n",
      "\n",
      "Epoch %d: train loss %f 125 0.23169322162866593\n",
      "Epoch 125: val loss 0.681003\n",
      "\n",
      "Epoch %d: train loss %f 126 0.2727608248591423\n",
      "Epoch 126: val loss 0.695099\n",
      "\n",
      "Epoch %d: train loss %f 127 0.23192851655185223\n",
      "Epoch 127: val loss 0.698927\n",
      "\n",
      "Epoch %d: train loss %f 128 0.22408756166696547\n",
      "Epoch 128: val loss 0.688874\n",
      "\n",
      "Epoch %d: train loss %f 129 0.2413706397637725\n",
      "Epoch 129: val loss 0.716868\n",
      "\n",
      "Epoch %d: train loss %f 130 0.2919567719101906\n",
      "Epoch 130: val loss 0.752746\n",
      "\n",
      "Epoch %d: train loss %f 131 0.2625784546136856\n",
      "Epoch 131: val loss 0.734748\n",
      "\n",
      "Epoch %d: train loss %f 132 0.23448195010423661\n",
      "Epoch 132: val loss 0.695518\n",
      "\n",
      "Epoch %d: train loss %f 133 0.2879788801074028\n",
      "Epoch 133: val loss 0.741262\n",
      "\n",
      "Epoch %d: train loss %f 134 0.2801085948944092\n",
      "Epoch 134: val loss 0.728418\n",
      "\n",
      "Epoch %d: train loss %f 135 0.22891905680298805\n",
      "Epoch 135: val loss 0.716490\n",
      "\n",
      "Epoch %d: train loss %f 136 0.2815315663814545\n",
      "Epoch 136: val loss 0.732322\n",
      "\n",
      "Epoch %d: train loss %f 137 0.21671825125813485\n",
      "Epoch 137: val loss 0.681284\n",
      "\n",
      "Epoch %d: train loss %f 138 0.24040365032851696\n",
      "Epoch 138: val loss 0.717468\n",
      "\n",
      "Epoch %d: train loss %f 139 0.19604480564594268\n",
      "Epoch 139: val loss 0.723077\n",
      "\n",
      "Epoch %d: train loss %f 140 0.3100570850074291\n",
      "Epoch 140: val loss 0.719612\n",
      "\n",
      "Epoch %d: train loss %f 141 0.23816282376646997\n",
      "Epoch 141: val loss 0.734578\n",
      "\n",
      "Epoch %d: train loss %f 142 0.24224309325218202\n",
      "Epoch 142: val loss 0.701234\n",
      "\n",
      "Epoch %d: train loss %f 143 0.21615559458732606\n",
      "Epoch 143: val loss 0.735220\n",
      "\n",
      "Epoch %d: train loss %f 144 0.2525479972362518\n",
      "Epoch 144: val loss 0.707886\n",
      "\n",
      "Epoch %d: train loss %f 145 0.2609952539205551\n",
      "Epoch 145: val loss 0.731261\n",
      "\n",
      "Epoch %d: train loss %f 146 0.2989795684814453\n",
      "Epoch 146: val loss 0.753623\n",
      "\n",
      "Epoch %d: train loss %f 147 0.23970986008644105\n",
      "Epoch 147: val loss 0.767917\n",
      "\n",
      "Epoch %d: train loss %f 148 0.2325942635536194\n",
      "Epoch 148: val loss 0.781875\n",
      "\n",
      "Epoch %d: train loss %f 149 0.28487016558647155\n",
      "Epoch 149: val loss 0.753548\n",
      "\n",
      "Epoch %d: train loss %f 150 0.3248942166566849\n",
      "Epoch 150: val loss 0.759404\n",
      "\n",
      "Epoch %d: train loss %f 151 0.25588481575250627\n",
      "Epoch 151: val loss 0.777912\n",
      "\n",
      "Epoch %d: train loss %f 152 0.25449633598327637\n",
      "Epoch 152: val loss 0.700280\n",
      "\n",
      "Epoch %d: train loss %f 153 0.24069752506911754\n",
      "Epoch 153: val loss 0.751747\n",
      "\n",
      "Epoch %d: train loss %f 154 0.2312220573425293\n",
      "Epoch 154: val loss 0.714393\n",
      "\n",
      "Epoch %d: train loss %f 155 0.28201072216033934\n",
      "Epoch 155: val loss 0.694826\n",
      "\n",
      "Epoch %d: train loss %f 156 0.28280535340309143\n",
      "Epoch 156: val loss 0.760340\n",
      "\n",
      "Epoch %d: train loss %f 157 0.27263189777731894\n",
      "Epoch 157: val loss 0.801921\n",
      "\n",
      "Epoch %d: train loss %f 158 0.2693870052695274\n",
      "Epoch 158: val loss 0.760900\n",
      "\n",
      "Epoch %d: train loss %f 159 0.3547281801700592\n",
      "Epoch 159: val loss 0.781287\n",
      "\n",
      "Epoch %d: train loss %f 160 0.23408870352432132\n",
      "Epoch 160: val loss 0.783702\n",
      "\n",
      "Epoch %d: train loss %f 161 0.25202234983444216\n",
      "Epoch 161: val loss 0.764210\n",
      "\n",
      "Epoch %d: train loss %f 162 0.32624848634004594\n",
      "Epoch 162: val loss 0.732441\n",
      "\n",
      "Epoch %d: train loss %f 163 0.2414454147219658\n",
      "Epoch 163: val loss 0.723403\n",
      "\n",
      "Epoch %d: train loss %f 164 0.21677139587700367\n",
      "Epoch 164: val loss 0.827742\n",
      "\n",
      "Epoch %d: train loss %f 165 0.23589122891426087\n",
      "Epoch 165: val loss 0.759557\n",
      "\n",
      "Epoch %d: train loss %f 166 0.2233136735856533\n",
      "Epoch 166: val loss 0.764485\n",
      "\n",
      "Epoch %d: train loss %f 167 0.2376201331615448\n",
      "Epoch 167: val loss 0.783305\n",
      "\n",
      "Epoch %d: train loss %f 168 0.22872241139411925\n",
      "Epoch 168: val loss 0.769996\n",
      "\n",
      "Epoch %d: train loss %f 169 0.2114996798336506\n",
      "Epoch 169: val loss 0.785704\n",
      "\n",
      "Epoch %d: train loss %f 170 0.25925209671258925\n",
      "Epoch 170: val loss 0.757976\n",
      "\n",
      "Epoch %d: train loss %f 171 0.2247568652033806\n",
      "Epoch 171: val loss 0.767298\n",
      "\n",
      "Epoch %d: train loss %f 172 0.2150080505758524\n",
      "Epoch 172: val loss 0.784069\n",
      "\n",
      "Epoch %d: train loss %f 173 0.18625394850969315\n",
      "Epoch 173: val loss 0.778276\n",
      "\n",
      "Epoch %d: train loss %f 174 0.20679447948932647\n",
      "Epoch 174: val loss 0.819767\n",
      "\n",
      "Epoch %d: train loss %f 175 0.21061722859740256\n",
      "Epoch 175: val loss 0.789050\n",
      "\n",
      "Epoch %d: train loss %f 176 0.28866944164037706\n",
      "Epoch 176: val loss 0.808015\n",
      "\n",
      "Epoch %d: train loss %f 177 0.20018923580646514\n",
      "Epoch 177: val loss 0.754053\n",
      "\n",
      "Epoch %d: train loss %f 178 0.21108479890972376\n",
      "Epoch 178: val loss 0.758929\n",
      "\n",
      "Epoch %d: train loss %f 179 0.21137562878429889\n",
      "Epoch 179: val loss 0.770742\n",
      "\n",
      "Epoch %d: train loss %f 180 0.19187354519963265\n",
      "Epoch 180: val loss 0.784234\n",
      "\n",
      "Epoch %d: train loss %f 181 0.18913175240159036\n",
      "Epoch 181: val loss 0.832524\n",
      "\n",
      "Epoch %d: train loss %f 182 0.18521538451313974\n",
      "Epoch 182: val loss 0.796086\n",
      "\n",
      "Epoch %d: train loss %f 183 0.23988615944981576\n",
      "Epoch 183: val loss 0.798324\n",
      "\n",
      "Epoch %d: train loss %f 184 0.2451253592967987\n",
      "Epoch 184: val loss 0.818016\n",
      "\n",
      "Epoch %d: train loss %f 185 0.20014894083142282\n",
      "Epoch 185: val loss 0.800876\n",
      "\n",
      "Epoch %d: train loss %f 186 0.20329713374376296\n",
      "Epoch 186: val loss 0.811133\n",
      "\n",
      "Epoch %d: train loss %f 187 0.24531058669090272\n",
      "Epoch 187: val loss 0.822920\n",
      "\n",
      "Epoch %d: train loss %f 188 0.20542269945144653\n",
      "Epoch 188: val loss 0.887125\n",
      "\n",
      "Epoch %d: train loss %f 189 0.2321397177875042\n",
      "Epoch 189: val loss 0.808903\n",
      "\n",
      "Epoch %d: train loss %f 190 0.18614369817078114\n",
      "Epoch 190: val loss 0.813260\n",
      "\n",
      "Epoch %d: train loss %f 191 0.21614155173301697\n",
      "Epoch 191: val loss 0.835676\n",
      "\n",
      "Epoch %d: train loss %f 192 0.16049863174557685\n",
      "Epoch 192: val loss 0.856416\n",
      "\n",
      "Epoch %d: train loss %f 193 0.20855021774768828\n",
      "Epoch 193: val loss 0.889105\n",
      "\n",
      "Epoch %d: train loss %f 194 0.2513139933347702\n",
      "Epoch 194: val loss 0.920910\n",
      "\n",
      "Epoch %d: train loss %f 195 0.1894239142537117\n",
      "Epoch 195: val loss 0.893234\n",
      "\n",
      "Epoch %d: train loss %f 196 0.2611774794757366\n",
      "Epoch 196: val loss 0.879858\n",
      "\n",
      "Epoch %d: train loss %f 197 0.22592479437589646\n",
      "Epoch 197: val loss 0.918942\n",
      "\n",
      "Epoch %d: train loss %f 198 0.24584449604153633\n",
      "Epoch 198: val loss 0.916233\n",
      "\n",
      "Epoch %d: train loss %f 199 0.23356463015079498\n",
      "Epoch 199: val loss 0.827726\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6594719529151917\n",
      "Epoch 0: val loss 0.660453\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6515082001686097\n",
      "Epoch 1: val loss 0.654552\n",
      "\n",
      "Epoch %d: train loss %f 2 0.636334240436554\n",
      "Epoch 2: val loss 0.646000\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6162421524524688\n",
      "Epoch 3: val loss 0.628525\n",
      "\n",
      "Epoch %d: train loss %f 4 0.5942528307437897\n",
      "Epoch 4: val loss 0.598016\n",
      "\n",
      "Epoch %d: train loss %f 5 0.5538145542144776\n",
      "Epoch 5: val loss 0.562821\n",
      "\n",
      "Epoch %d: train loss %f 6 0.49716185331344603\n",
      "Epoch 6: val loss 0.502976\n",
      "\n",
      "Epoch %d: train loss %f 7 0.4975497305393219\n",
      "Epoch 7: val loss 0.468300\n",
      "\n",
      "Epoch %d: train loss %f 8 0.40333869606256484\n",
      "Epoch 8: val loss 0.497405\n",
      "\n",
      "Epoch %d: train loss %f 9 0.4193469941616058\n",
      "Epoch 9: val loss 0.460563\n",
      "\n",
      "Epoch %d: train loss %f 10 0.3803878426551819\n",
      "Epoch 10: val loss 0.472135\n",
      "\n",
      "Epoch %d: train loss %f 11 0.4050919398665428\n",
      "Epoch 11: val loss 0.485531\n",
      "\n",
      "Epoch %d: train loss %f 12 0.40821767747402193\n",
      "Epoch 12: val loss 0.454769\n",
      "\n",
      "Epoch %d: train loss %f 13 0.3722130089998245\n",
      "Epoch 13: val loss 0.437720\n",
      "\n",
      "Epoch %d: train loss %f 14 0.40308955013751985\n",
      "Epoch 14: val loss 0.483003\n",
      "\n",
      "Epoch %d: train loss %f 15 0.3684074617922306\n",
      "Epoch 15: val loss 0.456750\n",
      "\n",
      "Epoch %d: train loss %f 16 0.37464042007923126\n",
      "Epoch 16: val loss 0.468293\n",
      "\n",
      "Epoch %d: train loss %f 17 0.38064380586147306\n",
      "Epoch 17: val loss 0.489664\n",
      "\n",
      "Epoch %d: train loss %f 18 0.42193143218755724\n",
      "Epoch 18: val loss 0.492429\n",
      "\n",
      "Epoch %d: train loss %f 19 0.3739463299512863\n",
      "Epoch 19: val loss 0.469845\n",
      "\n",
      "Epoch %d: train loss %f 20 0.4025407746434212\n",
      "Epoch 20: val loss 0.465515\n",
      "\n",
      "Epoch %d: train loss %f 21 0.38218327462673185\n",
      "Epoch 21: val loss 0.478261\n",
      "\n",
      "Epoch %d: train loss %f 22 0.3886146157979965\n",
      "Epoch 22: val loss 0.493256\n",
      "\n",
      "Epoch %d: train loss %f 23 0.35388264805078506\n",
      "Epoch 23: val loss 0.500218\n",
      "\n",
      "Epoch %d: train loss %f 24 0.3392052099108696\n",
      "Epoch 24: val loss 0.488269\n",
      "\n",
      "Epoch %d: train loss %f 25 0.37040795981884\n",
      "Epoch 25: val loss 0.485782\n",
      "\n",
      "Epoch %d: train loss %f 26 0.3647691160440445\n",
      "Epoch 26: val loss 0.520028\n",
      "\n",
      "Epoch %d: train loss %f 27 0.3512973949313164\n",
      "Epoch 27: val loss 0.501113\n",
      "\n",
      "Epoch %d: train loss %f 28 0.352356082201004\n",
      "Epoch 28: val loss 0.478246\n",
      "\n",
      "Epoch %d: train loss %f 29 0.3432300895452499\n",
      "Epoch 29: val loss 0.518263\n",
      "\n",
      "Epoch %d: train loss %f 30 0.33995459228754044\n",
      "Epoch 30: val loss 0.486867\n",
      "\n",
      "Epoch %d: train loss %f 31 0.3741381958127022\n",
      "Epoch 31: val loss 0.520949\n",
      "\n",
      "Epoch %d: train loss %f 32 0.383880652487278\n",
      "Epoch 32: val loss 0.519387\n",
      "\n",
      "Epoch %d: train loss %f 33 0.3497155547142029\n",
      "Epoch 33: val loss 0.502658\n",
      "\n",
      "Epoch %d: train loss %f 34 0.3720764309167862\n",
      "Epoch 34: val loss 0.505199\n",
      "\n",
      "Epoch %d: train loss %f 35 0.35828342884778974\n",
      "Epoch 35: val loss 0.496829\n",
      "\n",
      "Epoch %d: train loss %f 36 0.3359227731823921\n",
      "Epoch 36: val loss 0.514818\n",
      "\n",
      "Epoch %d: train loss %f 37 0.3752515971660614\n",
      "Epoch 37: val loss 0.514173\n",
      "\n",
      "Epoch %d: train loss %f 38 0.31859966889023783\n",
      "Epoch 38: val loss 0.540134\n",
      "\n",
      "Epoch %d: train loss %f 39 0.3396940931677818\n",
      "Epoch 39: val loss 0.524762\n",
      "\n",
      "Epoch %d: train loss %f 40 0.36384670734405516\n",
      "Epoch 40: val loss 0.504647\n",
      "\n",
      "Epoch %d: train loss %f 41 0.36962359845638276\n",
      "Epoch 41: val loss 0.498815\n",
      "\n",
      "Epoch %d: train loss %f 42 0.3453573450446129\n",
      "Epoch 42: val loss 0.505664\n",
      "\n",
      "Epoch %d: train loss %f 43 0.35302578955888747\n",
      "Epoch 43: val loss 0.559914\n",
      "\n",
      "Epoch %d: train loss %f 44 0.33348885327577593\n",
      "Epoch 44: val loss 0.564666\n",
      "\n",
      "Epoch %d: train loss %f 45 0.3235885128378868\n",
      "Epoch 45: val loss 0.538334\n",
      "\n",
      "Epoch %d: train loss %f 46 0.33115375488996507\n",
      "Epoch 46: val loss 0.546403\n",
      "\n",
      "Epoch %d: train loss %f 47 0.3219026252627373\n",
      "Epoch 47: val loss 0.543788\n",
      "\n",
      "Epoch %d: train loss %f 48 0.322774575650692\n",
      "Epoch 48: val loss 0.569438\n",
      "\n",
      "Epoch %d: train loss %f 49 0.3177496775984764\n",
      "Epoch 49: val loss 0.586462\n",
      "\n",
      "Epoch %d: train loss %f 50 0.3190002262592316\n",
      "Epoch 50: val loss 0.563272\n",
      "\n",
      "Epoch %d: train loss %f 51 0.30056211203336713\n",
      "Epoch 51: val loss 0.560838\n",
      "\n",
      "Epoch %d: train loss %f 52 0.3154782585799694\n",
      "Epoch 52: val loss 0.569724\n",
      "\n",
      "Epoch %d: train loss %f 53 0.3292255401611328\n",
      "Epoch 53: val loss 0.578094\n",
      "\n",
      "Epoch %d: train loss %f 54 0.3463897630572319\n",
      "Epoch 54: val loss 0.569173\n",
      "\n",
      "Epoch %d: train loss %f 55 0.3419467002153397\n",
      "Epoch 55: val loss 0.544458\n",
      "\n",
      "Epoch %d: train loss %f 56 0.3185981780290604\n",
      "Epoch 56: val loss 0.529245\n",
      "\n",
      "Epoch %d: train loss %f 57 0.30570486932992935\n",
      "Epoch 57: val loss 0.542602\n",
      "\n",
      "Epoch %d: train loss %f 58 0.3046583980321884\n",
      "Epoch 58: val loss 0.563154\n",
      "\n",
      "Epoch %d: train loss %f 59 0.35582254976034167\n",
      "Epoch 59: val loss 0.560259\n",
      "\n",
      "Epoch %d: train loss %f 60 0.3266366392374039\n",
      "Epoch 60: val loss 0.564986\n",
      "\n",
      "Epoch %d: train loss %f 61 0.3017801038920879\n",
      "Epoch 61: val loss 0.557432\n",
      "\n",
      "Epoch %d: train loss %f 62 0.29050313159823415\n",
      "Epoch 62: val loss 0.548676\n",
      "\n",
      "Epoch %d: train loss %f 63 0.30094102323055266\n",
      "Epoch 63: val loss 0.565528\n",
      "\n",
      "Epoch %d: train loss %f 64 0.29694201797246933\n",
      "Epoch 64: val loss 0.586613\n",
      "\n",
      "Epoch %d: train loss %f 65 0.362116739153862\n",
      "Epoch 65: val loss 0.589085\n",
      "\n",
      "Epoch %d: train loss %f 66 0.32234551161527636\n",
      "Epoch 66: val loss 0.538516\n",
      "\n",
      "Epoch %d: train loss %f 67 0.3656794518232346\n",
      "Epoch 67: val loss 0.571974\n",
      "\n",
      "Epoch %d: train loss %f 68 0.3247114971280098\n",
      "Epoch 68: val loss 0.622984\n",
      "\n",
      "Epoch %d: train loss %f 69 0.3323205292224884\n",
      "Epoch 69: val loss 0.592191\n",
      "\n",
      "Epoch %d: train loss %f 70 0.30773500055074693\n",
      "Epoch 70: val loss 0.553652\n",
      "\n",
      "Epoch %d: train loss %f 71 0.3162307575345039\n",
      "Epoch 71: val loss 0.557854\n",
      "\n",
      "Epoch %d: train loss %f 72 0.3055916041135788\n",
      "Epoch 72: val loss 0.563741\n",
      "\n",
      "Epoch %d: train loss %f 73 0.31538220047950744\n",
      "Epoch 73: val loss 0.595466\n",
      "\n",
      "Epoch %d: train loss %f 74 0.347186353802681\n",
      "Epoch 74: val loss 0.568726\n",
      "\n",
      "Epoch %d: train loss %f 75 0.28543292805552484\n",
      "Epoch 75: val loss 0.546679\n",
      "\n",
      "Epoch %d: train loss %f 76 0.2922678992152214\n",
      "Epoch 76: val loss 0.558902\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3020139031112194\n",
      "Epoch 77: val loss 0.572142\n",
      "\n",
      "Epoch %d: train loss %f 78 0.33733786046504977\n",
      "Epoch 78: val loss 0.584084\n",
      "\n",
      "Epoch %d: train loss %f 79 0.3179625660181046\n",
      "Epoch 79: val loss 0.574586\n",
      "\n",
      "Epoch %d: train loss %f 80 0.2935005262494087\n",
      "Epoch 80: val loss 0.575325\n",
      "\n",
      "Epoch %d: train loss %f 81 0.3686283826828003\n",
      "Epoch 81: val loss 0.589339\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3342346325516701\n",
      "Epoch 82: val loss 0.590023\n",
      "\n",
      "Epoch %d: train loss %f 83 0.2922947958111763\n",
      "Epoch 83: val loss 0.555851\n",
      "\n",
      "Epoch %d: train loss %f 84 0.2916014060378075\n",
      "Epoch 84: val loss 0.571465\n",
      "\n",
      "Epoch %d: train loss %f 85 0.27999183982610704\n",
      "Epoch 85: val loss 0.593332\n",
      "\n",
      "Epoch %d: train loss %f 86 0.3266453966498375\n",
      "Epoch 86: val loss 0.579603\n",
      "\n",
      "Epoch %d: train loss %f 87 0.2682292640209198\n",
      "Epoch 87: val loss 0.568671\n",
      "\n",
      "Epoch %d: train loss %f 88 0.27744476646184923\n",
      "Epoch 88: val loss 0.594674\n",
      "\n",
      "Epoch %d: train loss %f 89 0.27124410420656203\n",
      "Epoch 89: val loss 0.588156\n",
      "\n",
      "Epoch %d: train loss %f 90 0.2806000724434853\n",
      "Epoch 90: val loss 0.601248\n",
      "\n",
      "Epoch %d: train loss %f 91 0.26290009170770645\n",
      "Epoch 91: val loss 0.616230\n",
      "\n",
      "Epoch %d: train loss %f 92 0.2850671514868736\n",
      "Epoch 92: val loss 0.636056\n",
      "\n",
      "Epoch %d: train loss %f 93 0.3455601707100868\n",
      "Epoch 93: val loss 0.651587\n",
      "\n",
      "Epoch %d: train loss %f 94 0.29769094437360766\n",
      "Epoch 94: val loss 0.611693\n",
      "\n",
      "Epoch %d: train loss %f 95 0.289034291356802\n",
      "Epoch 95: val loss 0.621811\n",
      "\n",
      "Epoch %d: train loss %f 96 0.29080585986375806\n",
      "Epoch 96: val loss 0.630933\n",
      "\n",
      "Epoch %d: train loss %f 97 0.3165555536746979\n",
      "Epoch 97: val loss 0.633696\n",
      "\n",
      "Epoch %d: train loss %f 98 0.28512399941682814\n",
      "Epoch 98: val loss 0.589393\n",
      "\n",
      "Epoch %d: train loss %f 99 0.2771773487329483\n",
      "Epoch 99: val loss 0.594925\n",
      "\n",
      "Epoch %d: train loss %f 100 0.2793660819530487\n",
      "Epoch 100: val loss 0.629982\n",
      "\n",
      "Epoch %d: train loss %f 101 0.266818605363369\n",
      "Epoch 101: val loss 0.621119\n",
      "\n",
      "Epoch %d: train loss %f 102 0.30839929431676866\n",
      "Epoch 102: val loss 0.588156\n",
      "\n",
      "Epoch %d: train loss %f 103 0.26298166811466217\n",
      "Epoch 103: val loss 0.633104\n",
      "\n",
      "Epoch %d: train loss %f 104 0.299135659635067\n",
      "Epoch 104: val loss 0.639076\n",
      "\n",
      "Epoch %d: train loss %f 105 0.28437779247760775\n",
      "Epoch 105: val loss 0.610738\n",
      "\n",
      "Epoch %d: train loss %f 106 0.259619428217411\n",
      "Epoch 106: val loss 0.634275\n",
      "\n",
      "Epoch %d: train loss %f 107 0.2719070136547089\n",
      "Epoch 107: val loss 0.651787\n",
      "\n",
      "Epoch %d: train loss %f 108 0.25141026079654694\n",
      "Epoch 108: val loss 0.691964\n",
      "\n",
      "Epoch %d: train loss %f 109 0.3484190046787262\n",
      "Epoch 109: val loss 0.645130\n",
      "\n",
      "Epoch %d: train loss %f 110 0.27876579612493513\n",
      "Epoch 110: val loss 0.607352\n",
      "\n",
      "Epoch %d: train loss %f 111 0.30971344113349913\n",
      "Epoch 111: val loss 0.648802\n",
      "\n",
      "Epoch %d: train loss %f 112 0.31420705169439317\n",
      "Epoch 112: val loss 0.653480\n",
      "\n",
      "Epoch %d: train loss %f 113 0.28425144255161283\n",
      "Epoch 113: val loss 0.634889\n",
      "\n",
      "Epoch %d: train loss %f 114 0.2655677482485771\n",
      "Epoch 114: val loss 0.636625\n",
      "\n",
      "Epoch %d: train loss %f 115 0.2701148971915245\n",
      "Epoch 115: val loss 0.676201\n",
      "\n",
      "Epoch %d: train loss %f 116 0.2762495145201683\n",
      "Epoch 116: val loss 0.658781\n",
      "\n",
      "Epoch %d: train loss %f 117 0.2764703996479511\n",
      "Epoch 117: val loss 0.690674\n",
      "\n",
      "Epoch %d: train loss %f 118 0.2922415167093277\n",
      "Epoch 118: val loss 0.650328\n",
      "\n",
      "Epoch %d: train loss %f 119 0.21696571111679078\n",
      "Epoch 119: val loss 0.726939\n",
      "\n",
      "Epoch %d: train loss %f 120 0.2851399272680283\n",
      "Epoch 120: val loss 0.661312\n",
      "\n",
      "Epoch %d: train loss %f 121 0.26057168170809747\n",
      "Epoch 121: val loss 0.696607\n",
      "\n",
      "Epoch %d: train loss %f 122 0.2825100630521774\n",
      "Epoch 122: val loss 0.706988\n",
      "\n",
      "Epoch %d: train loss %f 123 0.25834010182879863\n",
      "Epoch 123: val loss 0.672718\n",
      "\n",
      "Epoch %d: train loss %f 124 0.25102216750383377\n",
      "Epoch 124: val loss 0.671961\n",
      "\n",
      "Epoch %d: train loss %f 125 0.2752333492040634\n",
      "Epoch 125: val loss 0.694570\n",
      "\n",
      "Epoch %d: train loss %f 126 0.2547393023967743\n",
      "Epoch 126: val loss 0.708175\n",
      "\n",
      "Epoch %d: train loss %f 127 0.2735647842288017\n",
      "Epoch 127: val loss 0.694144\n",
      "\n",
      "Epoch %d: train loss %f 128 0.27565855607390405\n",
      "Epoch 128: val loss 0.707282\n",
      "\n",
      "Epoch %d: train loss %f 129 0.2511041074991226\n",
      "Epoch 129: val loss 0.708743\n",
      "\n",
      "Epoch %d: train loss %f 130 0.22939857020974158\n",
      "Epoch 130: val loss 0.708974\n",
      "\n",
      "Epoch %d: train loss %f 131 0.25107055753469465\n",
      "Epoch 131: val loss 0.664148\n",
      "\n",
      "Epoch %d: train loss %f 132 0.23350501880049707\n",
      "Epoch 132: val loss 0.642919\n",
      "\n",
      "Epoch %d: train loss %f 133 0.23046558797359468\n",
      "Epoch 133: val loss 0.677846\n",
      "\n",
      "Epoch %d: train loss %f 134 0.23386824131011963\n",
      "Epoch 134: val loss 0.672263\n",
      "\n",
      "Epoch %d: train loss %f 135 0.2121524080634117\n",
      "Epoch 135: val loss 0.686458\n",
      "\n",
      "Epoch %d: train loss %f 136 0.23109830021858216\n",
      "Epoch 136: val loss 0.707336\n",
      "\n",
      "Epoch %d: train loss %f 137 0.24866437017917634\n",
      "Epoch 137: val loss 0.698039\n",
      "\n",
      "Epoch %d: train loss %f 138 0.2334490790963173\n",
      "Epoch 138: val loss 0.697348\n",
      "\n",
      "Epoch %d: train loss %f 139 0.2746837750077248\n",
      "Epoch 139: val loss 0.746724\n",
      "\n",
      "Epoch %d: train loss %f 140 0.2459145486354828\n",
      "Epoch 140: val loss 0.716883\n",
      "\n",
      "Epoch %d: train loss %f 141 0.21320374701172112\n",
      "Epoch 141: val loss 0.750775\n",
      "\n",
      "Epoch %d: train loss %f 142 0.24972724169492722\n",
      "Epoch 142: val loss 0.719312\n",
      "\n",
      "Epoch %d: train loss %f 143 0.26278517246246336\n",
      "Epoch 143: val loss 0.697963\n",
      "\n",
      "Epoch %d: train loss %f 144 0.24123430997133255\n",
      "Epoch 144: val loss 0.751322\n",
      "\n",
      "Epoch %d: train loss %f 145 0.20506815873086454\n",
      "Epoch 145: val loss 0.791540\n",
      "\n",
      "Epoch %d: train loss %f 146 0.22976638302206992\n",
      "Epoch 146: val loss 0.729305\n",
      "\n",
      "Epoch %d: train loss %f 147 0.224777552485466\n",
      "Epoch 147: val loss 0.793461\n",
      "\n",
      "Epoch %d: train loss %f 148 0.20118626207113266\n",
      "Epoch 148: val loss 0.773644\n",
      "\n",
      "Epoch %d: train loss %f 149 0.2031751647591591\n",
      "Epoch 149: val loss 0.738745\n",
      "\n",
      "Epoch %d: train loss %f 150 0.21810752414166928\n",
      "Epoch 150: val loss 0.723972\n",
      "\n",
      "Epoch %d: train loss %f 151 0.20110363587737085\n",
      "Epoch 151: val loss 0.790972\n",
      "\n",
      "Epoch %d: train loss %f 152 0.2536733955144882\n",
      "Epoch 152: val loss 0.774057\n",
      "\n",
      "Epoch %d: train loss %f 153 0.27148613035678865\n",
      "Epoch 153: val loss 0.744023\n",
      "\n",
      "Epoch %d: train loss %f 154 0.234004607796669\n",
      "Epoch 154: val loss 0.850279\n",
      "\n",
      "Epoch %d: train loss %f 155 0.25560700595378877\n",
      "Epoch 155: val loss 0.868833\n",
      "\n",
      "Epoch %d: train loss %f 156 0.23007606193423272\n",
      "Epoch 156: val loss 0.749941\n",
      "\n",
      "Epoch %d: train loss %f 157 0.22196641191840172\n",
      "Epoch 157: val loss 0.739612\n",
      "\n",
      "Epoch %d: train loss %f 158 0.21473145186901094\n",
      "Epoch 158: val loss 0.823631\n",
      "\n",
      "Epoch %d: train loss %f 159 0.25135323852300645\n",
      "Epoch 159: val loss 0.754450\n",
      "\n",
      "Epoch %d: train loss %f 160 0.23444305881857871\n",
      "Epoch 160: val loss 0.681364\n",
      "\n",
      "Epoch %d: train loss %f 161 0.21109357178211213\n",
      "Epoch 161: val loss 0.698431\n",
      "\n",
      "Epoch %d: train loss %f 162 0.21554514840245248\n",
      "Epoch 162: val loss 0.709862\n",
      "\n",
      "Epoch %d: train loss %f 163 0.24917110949754714\n",
      "Epoch 163: val loss 0.708825\n",
      "\n",
      "Epoch %d: train loss %f 164 0.21269998252391814\n",
      "Epoch 164: val loss 0.762552\n",
      "\n",
      "Epoch %d: train loss %f 165 0.21942696571350098\n",
      "Epoch 165: val loss 0.728202\n",
      "\n",
      "Epoch %d: train loss %f 166 0.20625126957893372\n",
      "Epoch 166: val loss 0.790827\n",
      "\n",
      "Epoch %d: train loss %f 167 0.2862661138176918\n",
      "Epoch 167: val loss 0.773272\n",
      "\n",
      "Epoch %d: train loss %f 168 0.21413831561803817\n",
      "Epoch 168: val loss 0.804081\n",
      "\n",
      "Epoch %d: train loss %f 169 0.2554887603968382\n",
      "Epoch 169: val loss 0.767625\n",
      "\n",
      "Epoch %d: train loss %f 170 0.2393413633108139\n",
      "Epoch 170: val loss 0.792354\n",
      "\n",
      "Epoch %d: train loss %f 171 0.21732991486787795\n",
      "Epoch 171: val loss 0.730351\n",
      "\n",
      "Epoch %d: train loss %f 172 0.22553912922739983\n",
      "Epoch 172: val loss 0.777473\n",
      "\n",
      "Epoch %d: train loss %f 173 0.2438171923160553\n",
      "Epoch 173: val loss 0.818277\n",
      "\n",
      "Epoch %d: train loss %f 174 0.20724339857697488\n",
      "Epoch 174: val loss 0.793725\n",
      "\n",
      "Epoch %d: train loss %f 175 0.21381231695413588\n",
      "Epoch 175: val loss 0.760175\n",
      "\n",
      "Epoch %d: train loss %f 176 0.2200035572052002\n",
      "Epoch 176: val loss 0.825746\n",
      "\n",
      "Epoch %d: train loss %f 177 0.2434106580913067\n",
      "Epoch 177: val loss 0.797130\n",
      "\n",
      "Epoch %d: train loss %f 178 0.22210630401968956\n",
      "Epoch 178: val loss 0.744292\n",
      "\n",
      "Epoch %d: train loss %f 179 0.21156569570302963\n",
      "Epoch 179: val loss 0.786731\n",
      "\n",
      "Epoch %d: train loss %f 180 0.1797719407826662\n",
      "Epoch 180: val loss 0.812338\n",
      "\n",
      "Epoch %d: train loss %f 181 0.20980580300092697\n",
      "Epoch 181: val loss 0.785478\n",
      "\n",
      "Epoch %d: train loss %f 182 0.2098701775074005\n",
      "Epoch 182: val loss 0.795885\n",
      "\n",
      "Epoch %d: train loss %f 183 0.20806517116725445\n",
      "Epoch 183: val loss 0.786692\n",
      "\n",
      "Epoch %d: train loss %f 184 0.18867177926003934\n",
      "Epoch 184: val loss 0.797594\n",
      "\n",
      "Epoch %d: train loss %f 185 0.19719432424753905\n",
      "Epoch 185: val loss 0.835413\n",
      "\n",
      "Epoch %d: train loss %f 186 0.2153671272099018\n",
      "Epoch 186: val loss 0.818382\n",
      "\n",
      "Epoch %d: train loss %f 187 0.22081786692142485\n",
      "Epoch 187: val loss 0.816331\n",
      "\n",
      "Epoch %d: train loss %f 188 0.2692610539495945\n",
      "Epoch 188: val loss 0.866569\n",
      "\n",
      "Epoch %d: train loss %f 189 0.2500531107187271\n",
      "Epoch 189: val loss 0.838561\n",
      "\n",
      "Epoch %d: train loss %f 190 0.188868348300457\n",
      "Epoch 190: val loss 0.853682\n",
      "\n",
      "Epoch %d: train loss %f 191 0.21685882657766342\n",
      "Epoch 191: val loss 0.870360\n",
      "\n",
      "Epoch %d: train loss %f 192 0.22632625252008437\n",
      "Epoch 192: val loss 0.862723\n",
      "\n",
      "Epoch %d: train loss %f 193 0.213682209700346\n",
      "Epoch 193: val loss 0.751939\n",
      "\n",
      "Epoch %d: train loss %f 194 0.17095219139009715\n",
      "Epoch 194: val loss 0.804525\n",
      "\n",
      "Epoch %d: train loss %f 195 0.30079114735126494\n",
      "Epoch 195: val loss 0.906234\n",
      "\n",
      "Epoch %d: train loss %f 196 0.20497441440820693\n",
      "Epoch 196: val loss 0.836597\n",
      "\n",
      "Epoch %d: train loss %f 197 0.19476403146982194\n",
      "Epoch 197: val loss 0.877507\n",
      "\n",
      "Epoch %d: train loss %f 198 0.1849012017250061\n",
      "Epoch 198: val loss 0.895919\n",
      "\n",
      "Epoch %d: train loss %f 199 0.32183540239930153\n",
      "Epoch 199: val loss 0.850627\n",
      "\n",
      "Epoch %d: train loss %f 0 0.665503231378702\n",
      "Epoch 0: val loss 0.665566\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6444853039888235\n",
      "Epoch 1: val loss 0.652607\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6099675893783569\n",
      "Epoch 2: val loss 0.618454\n",
      "\n",
      "Epoch %d: train loss %f 3 0.5599173307418823\n",
      "Epoch 3: val loss 0.545803\n",
      "\n",
      "Epoch %d: train loss %f 4 0.48684510588645935\n",
      "Epoch 4: val loss 0.480097\n",
      "\n",
      "Epoch %d: train loss %f 5 0.4301955975019015\n",
      "Epoch 5: val loss 0.445522\n",
      "\n",
      "Epoch %d: train loss %f 6 0.43002789524885326\n",
      "Epoch 6: val loss 0.455106\n",
      "\n",
      "Epoch %d: train loss %f 7 0.4153680136570564\n",
      "Epoch 7: val loss 0.464462\n",
      "\n",
      "Epoch %d: train loss %f 8 0.3909939286800531\n",
      "Epoch 8: val loss 0.451329\n",
      "\n",
      "Epoch %d: train loss %f 9 0.4051954322136365\n",
      "Epoch 9: val loss 0.455872\n",
      "\n",
      "Epoch %d: train loss %f 10 0.3906593013268251\n",
      "Epoch 10: val loss 0.465842\n",
      "\n",
      "Epoch %d: train loss %f 11 0.4436479394252484\n",
      "Epoch 11: val loss 0.476304\n",
      "\n",
      "Epoch %d: train loss %f 12 0.4443688415564023\n",
      "Epoch 12: val loss 0.476583\n",
      "\n",
      "Epoch %d: train loss %f 13 0.41045277279156905\n",
      "Epoch 13: val loss 0.463841\n",
      "\n",
      "Epoch %d: train loss %f 14 0.39305348694324493\n",
      "Epoch 14: val loss 0.447266\n",
      "\n",
      "Epoch %d: train loss %f 15 0.42313944834929246\n",
      "Epoch 15: val loss 0.459944\n",
      "\n",
      "Epoch %d: train loss %f 16 0.38660498880423033\n",
      "Epoch 16: val loss 0.473913\n",
      "\n",
      "Epoch %d: train loss %f 17 0.38336180035884565\n",
      "Epoch 17: val loss 0.475026\n",
      "\n",
      "Epoch %d: train loss %f 18 0.4364234289297691\n",
      "Epoch 18: val loss 0.488796\n",
      "\n",
      "Epoch %d: train loss %f 19 0.37734583478707534\n",
      "Epoch 19: val loss 0.479626\n",
      "\n",
      "Epoch %d: train loss %f 20 0.38330337405204773\n",
      "Epoch 20: val loss 0.462964\n",
      "\n",
      "Epoch %d: train loss %f 21 0.3779198538798552\n",
      "Epoch 21: val loss 0.471782\n",
      "\n",
      "Epoch %d: train loss %f 22 0.361691673214619\n",
      "Epoch 22: val loss 0.499138\n",
      "\n",
      "Epoch %d: train loss %f 23 0.35309474628705245\n",
      "Epoch 23: val loss 0.493595\n",
      "\n",
      "Epoch %d: train loss %f 24 0.4360845249432784\n",
      "Epoch 24: val loss 0.489088\n",
      "\n",
      "Epoch %d: train loss %f 25 0.3755250836794193\n",
      "Epoch 25: val loss 0.463776\n",
      "\n",
      "Epoch %d: train loss %f 26 0.38438432950239915\n",
      "Epoch 26: val loss 0.478083\n",
      "\n",
      "Epoch %d: train loss %f 27 0.4425232227032001\n",
      "Epoch 27: val loss 0.470279\n",
      "\n",
      "Epoch %d: train loss %f 28 0.35653084172652316\n",
      "Epoch 28: val loss 0.464852\n",
      "\n",
      "Epoch %d: train loss %f 29 0.3898148192809178\n",
      "Epoch 29: val loss 0.466638\n",
      "\n",
      "Epoch %d: train loss %f 30 0.3742943761440424\n",
      "Epoch 30: val loss 0.455048\n",
      "\n",
      "Epoch %d: train loss %f 31 0.36209599100626433\n",
      "Epoch 31: val loss 0.459411\n",
      "\n",
      "Epoch %d: train loss %f 32 0.38087288576823014\n",
      "Epoch 32: val loss 0.473873\n",
      "\n",
      "Epoch %d: train loss %f 33 0.37805179678476775\n",
      "Epoch 33: val loss 0.474849\n",
      "\n",
      "Epoch %d: train loss %f 34 0.3478973926259921\n",
      "Epoch 34: val loss 0.469886\n",
      "\n",
      "Epoch %d: train loss %f 35 0.3776000073322883\n",
      "Epoch 35: val loss 0.480805\n",
      "\n",
      "Epoch %d: train loss %f 36 0.3738906142803339\n",
      "Epoch 36: val loss 0.489487\n",
      "\n",
      "Epoch %d: train loss %f 37 0.3569699628995015\n",
      "Epoch 37: val loss 0.494174\n",
      "\n",
      "Epoch %d: train loss %f 38 0.36912684601086837\n",
      "Epoch 38: val loss 0.460549\n",
      "\n",
      "Epoch %d: train loss %f 39 0.34502889674443465\n",
      "Epoch 39: val loss 0.495728\n",
      "\n",
      "Epoch %d: train loss %f 40 0.37061023941406834\n",
      "Epoch 40: val loss 0.475275\n",
      "\n",
      "Epoch %d: train loss %f 41 0.3605431616306305\n",
      "Epoch 41: val loss 0.454773\n",
      "\n",
      "Epoch %d: train loss %f 42 0.34470778474440944\n",
      "Epoch 42: val loss 0.485249\n",
      "\n",
      "Epoch %d: train loss %f 43 0.3613846989778372\n",
      "Epoch 43: val loss 0.482990\n",
      "\n",
      "Epoch %d: train loss %f 44 0.33143141063360065\n",
      "Epoch 44: val loss 0.467302\n",
      "\n",
      "Epoch %d: train loss %f 45 0.33289100802861726\n",
      "Epoch 45: val loss 0.480187\n",
      "\n",
      "Epoch %d: train loss %f 0 0.7008600560101595\n",
      "Epoch 0: val loss 0.701169\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6956373019651934\n",
      "Epoch 1: val loss 0.698485\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6884589357809587\n",
      "Epoch 2: val loss 0.694789\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6752555478702892\n",
      "Epoch 3: val loss 0.688576\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6568844318389893\n",
      "Epoch 4: val loss 0.678468\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6366131793368947\n",
      "Epoch 5: val loss 0.665043\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6073410727761008\n",
      "Epoch 6: val loss 0.653649\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5860154032707214\n",
      "Epoch 7: val loss 0.650241\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5644548318602822\n",
      "Epoch 8: val loss 0.660050\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5475816482847388\n",
      "Epoch 9: val loss 0.642235\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5290674404664473\n",
      "Epoch 10: val loss 0.644506\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5474201359532096\n",
      "Epoch 11: val loss 0.643897\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5458396619016473\n",
      "Epoch 12: val loss 0.651262\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5274141999808225\n",
      "Epoch 13: val loss 0.647090\n",
      "\n",
      "Epoch %d: train loss %f 14 0.534695105119185\n",
      "Epoch 14: val loss 0.653791\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5099666606296193\n",
      "Epoch 15: val loss 0.639308\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5315880341963335\n",
      "Epoch 16: val loss 0.693289\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5381264253096147\n",
      "Epoch 17: val loss 0.648141\n",
      "\n",
      "Epoch %d: train loss %f 18 0.52028304067525\n",
      "Epoch 18: val loss 0.625563\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5258467603813518\n",
      "Epoch 19: val loss 0.623143\n",
      "\n",
      "Epoch %d: train loss %f 20 0.503884805874391\n",
      "Epoch 20: val loss 0.677070\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5069963742386211\n",
      "Epoch 21: val loss 0.623121\n",
      "\n",
      "Epoch %d: train loss %f 22 0.504186833446676\n",
      "Epoch 22: val loss 0.636208\n",
      "\n",
      "Epoch %d: train loss %f 23 0.4831635897809809\n",
      "Epoch 23: val loss 0.620227\n",
      "\n",
      "Epoch %d: train loss %f 24 0.501929990269921\n",
      "Epoch 24: val loss 0.640908\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4911393414844166\n",
      "Epoch 25: val loss 0.658735\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4850054085254669\n",
      "Epoch 26: val loss 0.615430\n",
      "\n",
      "Epoch %d: train loss %f 27 0.49043247916481714\n",
      "Epoch 27: val loss 0.650234\n",
      "\n",
      "Epoch %d: train loss %f 28 0.484327879819003\n",
      "Epoch 28: val loss 0.631413\n",
      "\n",
      "Epoch %d: train loss %f 29 0.49684425917538727\n",
      "Epoch 29: val loss 0.624993\n",
      "\n",
      "Epoch %d: train loss %f 30 0.49220229278911243\n",
      "Epoch 30: val loss 0.638386\n",
      "\n",
      "Epoch %d: train loss %f 31 0.49392844059250574\n",
      "Epoch 31: val loss 0.597420\n",
      "\n",
      "Epoch %d: train loss %f 32 0.47168501669710333\n",
      "Epoch 32: val loss 0.599810\n",
      "\n",
      "Epoch %d: train loss %f 33 0.47793886065483093\n",
      "Epoch 33: val loss 0.639843\n",
      "\n",
      "Epoch %d: train loss %f 34 0.49539040435444226\n",
      "Epoch 34: val loss 0.603772\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4619358615441756\n",
      "Epoch 35: val loss 0.606840\n",
      "\n",
      "Epoch %d: train loss %f 36 0.45693501830101013\n",
      "Epoch 36: val loss 0.601181\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4682485109025782\n",
      "Epoch 37: val loss 0.618776\n",
      "\n",
      "Epoch %d: train loss %f 38 0.4720474021001296\n",
      "Epoch 38: val loss 0.585153\n",
      "\n",
      "Epoch %d: train loss %f 39 0.45314590497450397\n",
      "Epoch 39: val loss 0.591897\n",
      "\n",
      "Epoch %d: train loss %f 40 0.45079013434323395\n",
      "Epoch 40: val loss 0.621021\n",
      "\n",
      "Epoch %d: train loss %f 41 0.44869504462588916\n",
      "Epoch 41: val loss 0.587667\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4566529501568187\n",
      "Epoch 42: val loss 0.602875\n",
      "\n",
      "Epoch %d: train loss %f 43 0.45134298367933795\n",
      "Epoch 43: val loss 0.591325\n",
      "\n",
      "Epoch %d: train loss %f 44 0.4472326392477209\n",
      "Epoch 44: val loss 0.597502\n",
      "\n",
      "Epoch %d: train loss %f 45 0.44040105559609155\n",
      "Epoch 45: val loss 0.600676\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4347746101292697\n",
      "Epoch 46: val loss 0.592728\n",
      "\n",
      "Epoch %d: train loss %f 47 0.4364273250102997\n",
      "Epoch 47: val loss 0.605543\n",
      "\n",
      "Epoch %d: train loss %f 48 0.4347694787112149\n",
      "Epoch 48: val loss 0.600509\n",
      "\n",
      "Epoch %d: train loss %f 49 0.43543892015110364\n",
      "Epoch 49: val loss 0.623911\n",
      "\n",
      "Epoch %d: train loss %f 50 0.4366353723135861\n",
      "Epoch 50: val loss 0.589405\n",
      "\n",
      "Epoch %d: train loss %f 51 0.41284914992072363\n",
      "Epoch 51: val loss 0.594793\n",
      "\n",
      "Epoch %d: train loss %f 52 0.4147740331563083\n",
      "Epoch 52: val loss 0.587663\n",
      "\n",
      "Epoch %d: train loss %f 53 0.4357487518679012\n",
      "Epoch 53: val loss 0.584567\n",
      "\n",
      "Epoch %d: train loss %f 54 0.3895113224332983\n",
      "Epoch 54: val loss 0.614072\n",
      "\n",
      "Epoch %d: train loss %f 55 0.4218094619837674\n",
      "Epoch 55: val loss 0.603773\n",
      "\n",
      "Epoch %d: train loss %f 56 0.39071580225771124\n",
      "Epoch 56: val loss 0.613267\n",
      "\n",
      "Epoch %d: train loss %f 57 0.3726017746058377\n",
      "Epoch 57: val loss 0.579593\n",
      "\n",
      "Epoch %d: train loss %f 58 0.4074301096526059\n",
      "Epoch 58: val loss 0.585129\n",
      "\n",
      "Epoch %d: train loss %f 59 0.399278849363327\n",
      "Epoch 59: val loss 0.585494\n",
      "\n",
      "Epoch %d: train loss %f 60 0.41119774363257666\n",
      "Epoch 60: val loss 0.656193\n",
      "\n",
      "Epoch %d: train loss %f 61 0.42604479464617645\n",
      "Epoch 61: val loss 0.578013\n",
      "\n",
      "Epoch %d: train loss %f 62 0.42898382381959393\n",
      "Epoch 62: val loss 0.619443\n",
      "\n",
      "Epoch %d: train loss %f 63 0.3826881701296026\n",
      "Epoch 63: val loss 0.564283\n",
      "\n",
      "Epoch %d: train loss %f 64 0.3971821963787079\n",
      "Epoch 64: val loss 0.620172\n",
      "\n",
      "Epoch %d: train loss %f 65 0.3952337476340207\n",
      "Epoch 65: val loss 0.607265\n",
      "\n",
      "Epoch %d: train loss %f 66 0.38864870775829663\n",
      "Epoch 66: val loss 0.624761\n",
      "\n",
      "Epoch %d: train loss %f 67 0.39483109658414667\n",
      "Epoch 67: val loss 0.591411\n",
      "\n",
      "Epoch %d: train loss %f 68 0.3864165883172642\n",
      "Epoch 68: val loss 0.614266\n",
      "\n",
      "Epoch %d: train loss %f 69 0.40973741628906946\n",
      "Epoch 69: val loss 0.582765\n",
      "\n",
      "Epoch %d: train loss %f 70 0.36753337627107446\n",
      "Epoch 70: val loss 0.629068\n",
      "\n",
      "Epoch %d: train loss %f 71 0.399260702458295\n",
      "Epoch 71: val loss 0.604285\n",
      "\n",
      "Epoch %d: train loss %f 72 0.3774692646481774\n",
      "Epoch 72: val loss 0.605236\n",
      "\n",
      "Epoch %d: train loss %f 73 0.36862594160166656\n",
      "Epoch 73: val loss 0.609542\n",
      "\n",
      "Epoch %d: train loss %f 74 0.3749944974075664\n",
      "Epoch 74: val loss 0.589376\n",
      "\n",
      "Epoch %d: train loss %f 75 0.4021870033307509\n",
      "Epoch 75: val loss 0.597871\n",
      "\n",
      "Epoch %d: train loss %f 76 0.3730866421352733\n",
      "Epoch 76: val loss 0.598870\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3420001837340268\n",
      "Epoch 77: val loss 0.614971\n",
      "\n",
      "Epoch %d: train loss %f 78 0.3803893029689789\n",
      "Epoch 78: val loss 0.598721\n",
      "\n",
      "Epoch %d: train loss %f 79 0.3867856968532909\n",
      "Epoch 79: val loss 0.611149\n",
      "\n",
      "Epoch %d: train loss %f 80 0.4168812713839791\n",
      "Epoch 80: val loss 0.635386\n",
      "\n",
      "Epoch %d: train loss %f 81 0.3438454866409302\n",
      "Epoch 81: val loss 0.597818\n",
      "\n",
      "Epoch %d: train loss %f 82 0.395945669575171\n",
      "Epoch 82: val loss 0.615737\n",
      "\n",
      "Epoch %d: train loss %f 83 0.39793702418153937\n",
      "Epoch 83: val loss 0.615718\n",
      "\n",
      "Epoch %d: train loss %f 84 0.3824210600419478\n",
      "Epoch 84: val loss 0.614144\n",
      "\n",
      "Epoch %d: train loss %f 85 0.3664570315317674\n",
      "Epoch 85: val loss 0.626996\n",
      "\n",
      "Epoch %d: train loss %f 86 0.37498118525201624\n",
      "Epoch 86: val loss 0.576313\n",
      "\n",
      "Epoch %d: train loss %f 87 0.4035127731886777\n",
      "Epoch 87: val loss 0.607396\n",
      "\n",
      "Epoch %d: train loss %f 88 0.34677783332087775\n",
      "Epoch 88: val loss 0.582691\n",
      "\n",
      "Epoch %d: train loss %f 89 0.3804608109322461\n",
      "Epoch 89: val loss 0.594026\n",
      "\n",
      "Epoch %d: train loss %f 90 0.34055171636017884\n",
      "Epoch 90: val loss 0.606934\n",
      "\n",
      "Epoch %d: train loss %f 91 0.3337428854270415\n",
      "Epoch 91: val loss 0.623065\n",
      "\n",
      "Epoch %d: train loss %f 92 0.34589089859615674\n",
      "Epoch 92: val loss 0.636721\n",
      "\n",
      "Epoch %d: train loss %f 93 0.35733088850975037\n",
      "Epoch 93: val loss 0.617988\n",
      "\n",
      "Epoch %d: train loss %f 94 0.3372018499807878\n",
      "Epoch 94: val loss 0.609287\n",
      "\n",
      "Epoch %d: train loss %f 95 0.36389433253895154\n",
      "Epoch 95: val loss 0.634520\n",
      "\n",
      "Epoch %d: train loss %f 96 0.33833125504580414\n",
      "Epoch 96: val loss 0.623837\n",
      "\n",
      "Epoch %d: train loss %f 97 0.34376744519580493\n",
      "Epoch 97: val loss 0.629117\n",
      "\n",
      "Epoch %d: train loss %f 98 0.33261400054801593\n",
      "Epoch 98: val loss 0.594534\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3370038325136358\n",
      "Epoch 99: val loss 0.610941\n",
      "\n",
      "Epoch %d: train loss %f 100 0.34635648131370544\n",
      "Epoch 100: val loss 0.626021\n",
      "\n",
      "Epoch %d: train loss %f 101 0.32563706690614874\n",
      "Epoch 101: val loss 0.591001\n",
      "\n",
      "Epoch %d: train loss %f 102 0.3815395886247808\n",
      "Epoch 102: val loss 0.655071\n",
      "\n",
      "Epoch %d: train loss %f 103 0.35446569188074634\n",
      "Epoch 103: val loss 0.603034\n",
      "\n",
      "Epoch %d: train loss %f 104 0.35451758720658044\n",
      "Epoch 104: val loss 0.626459\n",
      "\n",
      "Epoch %d: train loss %f 105 0.31650045243176544\n",
      "Epoch 105: val loss 0.616470\n",
      "\n",
      "Epoch %d: train loss %f 106 0.32955849170684814\n",
      "Epoch 106: val loss 0.616210\n",
      "\n",
      "Epoch %d: train loss %f 107 0.3483909449794076\n",
      "Epoch 107: val loss 0.625323\n",
      "\n",
      "Epoch %d: train loss %f 108 0.3354183313521472\n",
      "Epoch 108: val loss 0.641789\n",
      "\n",
      "Epoch %d: train loss %f 109 0.3404285961931402\n",
      "Epoch 109: val loss 0.634942\n",
      "\n",
      "Epoch %d: train loss %f 110 0.3611658906394785\n",
      "Epoch 110: val loss 0.622319\n",
      "\n",
      "Epoch %d: train loss %f 111 0.36235911873253907\n",
      "Epoch 111: val loss 0.634268\n",
      "\n",
      "Epoch %d: train loss %f 112 0.3240510049191388\n",
      "Epoch 112: val loss 0.591332\n",
      "\n",
      "Epoch %d: train loss %f 113 0.37547948414629156\n",
      "Epoch 113: val loss 0.608747\n",
      "\n",
      "Epoch %d: train loss %f 114 0.3580827469175512\n",
      "Epoch 114: val loss 0.614743\n",
      "\n",
      "Epoch %d: train loss %f 115 0.30419292504137213\n",
      "Epoch 115: val loss 0.670383\n",
      "\n",
      "Epoch %d: train loss %f 116 0.3466649367050691\n",
      "Epoch 116: val loss 0.615011\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3479466871781783\n",
      "Epoch 117: val loss 0.606155\n",
      "\n",
      "Epoch %d: train loss %f 118 0.3486515988003124\n",
      "Epoch 118: val loss 0.651899\n",
      "\n",
      "Epoch %d: train loss %f 119 0.3330770446495576\n",
      "Epoch 119: val loss 0.632843\n",
      "\n",
      "Epoch %d: train loss %f 120 0.34751707450910047\n",
      "Epoch 120: val loss 0.666490\n",
      "\n",
      "Epoch %d: train loss %f 121 0.32705283435908233\n",
      "Epoch 121: val loss 0.639708\n",
      "\n",
      "Epoch %d: train loss %f 122 0.36434982446106995\n",
      "Epoch 122: val loss 0.625270\n",
      "\n",
      "Epoch %d: train loss %f 123 0.3594435372135856\n",
      "Epoch 123: val loss 0.644952\n",
      "\n",
      "Epoch %d: train loss %f 124 0.2971326559782028\n",
      "Epoch 124: val loss 0.628312\n",
      "\n",
      "Epoch %d: train loss %f 125 0.31552164392037824\n",
      "Epoch 125: val loss 0.649129\n",
      "\n",
      "Epoch %d: train loss %f 126 0.30489088323983277\n",
      "Epoch 126: val loss 0.630355\n",
      "\n",
      "Epoch %d: train loss %f 127 0.3073811016299508\n",
      "Epoch 127: val loss 0.672019\n",
      "\n",
      "Epoch %d: train loss %f 128 0.29051094434478064\n",
      "Epoch 128: val loss 0.600794\n",
      "\n",
      "Epoch %d: train loss %f 129 0.3437863872809844\n",
      "Epoch 129: val loss 0.688015\n",
      "\n",
      "Epoch %d: train loss %f 130 0.32154479216445575\n",
      "Epoch 130: val loss 0.624756\n",
      "\n",
      "Epoch %d: train loss %f 131 0.3804271031509746\n",
      "Epoch 131: val loss 0.716611\n",
      "\n",
      "Epoch %d: train loss %f 132 0.3568749495527961\n",
      "Epoch 132: val loss 0.651039\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3493714671243321\n",
      "Epoch 133: val loss 0.647394\n",
      "\n",
      "Epoch %d: train loss %f 134 0.3440334620800885\n",
      "Epoch 134: val loss 0.665526\n",
      "\n",
      "Epoch %d: train loss %f 135 0.3401302017948844\n",
      "Epoch 135: val loss 0.650937\n",
      "\n",
      "Epoch %d: train loss %f 136 0.31383070620623504\n",
      "Epoch 136: val loss 0.654825\n",
      "\n",
      "Epoch %d: train loss %f 137 0.3259563229300759\n",
      "Epoch 137: val loss 0.608284\n",
      "\n",
      "Epoch %d: train loss %f 138 0.314376331188462\n",
      "Epoch 138: val loss 0.642360\n",
      "\n",
      "Epoch %d: train loss %f 139 0.34703011268919165\n",
      "Epoch 139: val loss 0.617230\n",
      "\n",
      "Epoch %d: train loss %f 140 0.32106767188418994\n",
      "Epoch 140: val loss 0.652810\n",
      "\n",
      "Epoch %d: train loss %f 141 0.3235594521869313\n",
      "Epoch 141: val loss 0.614995\n",
      "\n",
      "Epoch %d: train loss %f 142 0.34019678831100464\n",
      "Epoch 142: val loss 0.619045\n",
      "\n",
      "Epoch %d: train loss %f 143 0.31403906643390656\n",
      "Epoch 143: val loss 0.649810\n",
      "\n",
      "Epoch %d: train loss %f 144 0.3207656145095825\n",
      "Epoch 144: val loss 0.626382\n",
      "\n",
      "Epoch %d: train loss %f 145 0.33020352775400336\n",
      "Epoch 145: val loss 0.678909\n",
      "\n",
      "Epoch %d: train loss %f 146 0.2950346375053579\n",
      "Epoch 146: val loss 0.669318\n",
      "\n",
      "Epoch %d: train loss %f 147 0.2913537079637701\n",
      "Epoch 147: val loss 0.634034\n",
      "\n",
      "Epoch %d: train loss %f 148 0.29922717674212024\n",
      "Epoch 148: val loss 0.686302\n",
      "\n",
      "Epoch %d: train loss %f 149 0.3021473789756948\n",
      "Epoch 149: val loss 0.660016\n",
      "\n",
      "Epoch %d: train loss %f 150 0.3076734082265334\n",
      "Epoch 150: val loss 0.621856\n",
      "\n",
      "Epoch %d: train loss %f 151 0.2832560742443258\n",
      "Epoch 151: val loss 0.630641\n",
      "\n",
      "Epoch %d: train loss %f 152 0.2921747727827592\n",
      "Epoch 152: val loss 0.620734\n",
      "\n",
      "Epoch %d: train loss %f 153 0.295947176488963\n",
      "Epoch 153: val loss 0.678360\n",
      "\n",
      "Epoch %d: train loss %f 154 0.33112860403277655\n",
      "Epoch 154: val loss 0.629252\n",
      "\n",
      "Epoch %d: train loss %f 155 0.3232330652800473\n",
      "Epoch 155: val loss 0.706069\n",
      "\n",
      "Epoch %d: train loss %f 156 0.2974794967608018\n",
      "Epoch 156: val loss 0.633370\n",
      "\n",
      "Epoch %d: train loss %f 157 0.28821259466084564\n",
      "Epoch 157: val loss 0.724055\n",
      "\n",
      "Epoch %d: train loss %f 158 0.31353801624341443\n",
      "Epoch 158: val loss 0.659705\n",
      "\n",
      "Epoch %d: train loss %f 159 0.27321549708193\n",
      "Epoch 159: val loss 0.648617\n",
      "\n",
      "Epoch %d: train loss %f 160 0.2674886326898228\n",
      "Epoch 160: val loss 0.624703\n",
      "\n",
      "Epoch %d: train loss %f 161 0.30393893068486993\n",
      "Epoch 161: val loss 0.680755\n",
      "\n",
      "Epoch %d: train loss %f 162 0.27800131792371924\n",
      "Epoch 162: val loss 0.641053\n",
      "\n",
      "Epoch %d: train loss %f 163 0.273074899207462\n",
      "Epoch 163: val loss 0.718157\n",
      "\n",
      "Epoch %d: train loss %f 164 0.293004728176377\n",
      "Epoch 164: val loss 0.619253\n",
      "\n",
      "Epoch %d: train loss %f 165 0.29234292019497266\n",
      "Epoch 165: val loss 0.681661\n",
      "\n",
      "Epoch %d: train loss %f 166 0.2768727893179113\n",
      "Epoch 166: val loss 0.661701\n",
      "\n",
      "Epoch %d: train loss %f 167 0.2751476480202241\n",
      "Epoch 167: val loss 0.657995\n",
      "\n",
      "Epoch %d: train loss %f 168 0.34408940781246533\n",
      "Epoch 168: val loss 0.660943\n",
      "\n",
      "Epoch %d: train loss %f 169 0.26348146525296295\n",
      "Epoch 169: val loss 0.625764\n",
      "\n",
      "Epoch %d: train loss %f 170 0.3300626034086401\n",
      "Epoch 170: val loss 0.638860\n",
      "\n",
      "Epoch %d: train loss %f 171 0.280661555853757\n",
      "Epoch 171: val loss 0.628508\n",
      "\n",
      "Epoch %d: train loss %f 172 0.3166302171620456\n",
      "Epoch 172: val loss 0.721890\n",
      "\n",
      "Epoch %d: train loss %f 173 0.30066822469234467\n",
      "Epoch 173: val loss 0.636316\n",
      "\n",
      "Epoch %d: train loss %f 174 0.31570686399936676\n",
      "Epoch 174: val loss 0.649299\n",
      "\n",
      "Epoch %d: train loss %f 175 0.28147230635989795\n",
      "Epoch 175: val loss 0.624192\n",
      "\n",
      "Epoch %d: train loss %f 176 0.29205084253441205\n",
      "Epoch 176: val loss 0.629222\n",
      "\n",
      "Epoch %d: train loss %f 177 0.30572920495813544\n",
      "Epoch 177: val loss 0.679308\n",
      "\n",
      "Epoch %d: train loss %f 178 0.3395071950825778\n",
      "Epoch 178: val loss 0.601459\n",
      "\n",
      "Epoch %d: train loss %f 179 0.272277833385901\n",
      "Epoch 179: val loss 0.652881\n",
      "\n",
      "Epoch %d: train loss %f 180 0.27211877568201587\n",
      "Epoch 180: val loss 0.625680\n",
      "\n",
      "Epoch %d: train loss %f 181 0.2660727555101568\n",
      "Epoch 181: val loss 0.638400\n",
      "\n",
      "Epoch %d: train loss %f 182 0.2948973233049566\n",
      "Epoch 182: val loss 0.681645\n",
      "\n",
      "Epoch %d: train loss %f 183 0.28244596313346515\n",
      "Epoch 183: val loss 0.672121\n",
      "\n",
      "Epoch %d: train loss %f 184 0.33570514754815534\n",
      "Epoch 184: val loss 0.651805\n",
      "\n",
      "Epoch %d: train loss %f 185 0.27147317068143323\n",
      "Epoch 185: val loss 0.663417\n",
      "\n",
      "Epoch %d: train loss %f 186 0.28873505240136926\n",
      "Epoch 186: val loss 0.678550\n",
      "\n",
      "Epoch %d: train loss %f 187 0.29119478301568463\n",
      "Epoch 187: val loss 0.656208\n",
      "\n",
      "Epoch %d: train loss %f 188 0.275789313018322\n",
      "Epoch 188: val loss 0.635827\n",
      "\n",
      "Epoch %d: train loss %f 189 0.2639119029045105\n",
      "Epoch 189: val loss 0.627726\n",
      "\n",
      "Epoch %d: train loss %f 190 0.2955914532596415\n",
      "Epoch 190: val loss 0.627932\n",
      "\n",
      "Epoch %d: train loss %f 191 0.275885208086534\n",
      "Epoch 191: val loss 0.644733\n",
      "\n",
      "Epoch %d: train loss %f 192 0.2926694845611399\n",
      "Epoch 192: val loss 0.617352\n",
      "\n",
      "Epoch %d: train loss %f 193 0.2912248881025748\n",
      "Epoch 193: val loss 0.670446\n",
      "\n",
      "Epoch %d: train loss %f 194 0.2779356146400625\n",
      "Epoch 194: val loss 0.662330\n",
      "\n",
      "Epoch %d: train loss %f 195 0.2816936021501368\n",
      "Epoch 195: val loss 0.644290\n",
      "\n",
      "Epoch %d: train loss %f 196 0.260771862485192\n",
      "Epoch 196: val loss 0.648342\n",
      "\n",
      "Epoch %d: train loss %f 197 0.2820003710009835\n",
      "Epoch 197: val loss 0.633411\n",
      "\n",
      "Epoch %d: train loss %f 198 0.2698387720368125\n",
      "Epoch 198: val loss 0.720664\n",
      "\n",
      "Epoch %d: train loss %f 199 0.2896396951241927\n",
      "Epoch 199: val loss 0.681750\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6872761032798074\n",
      "Epoch 0: val loss 0.686381\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6814863952723417\n",
      "Epoch 1: val loss 0.682987\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6693519191308455\n",
      "Epoch 2: val loss 0.677188\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6570142832669345\n",
      "Epoch 3: val loss 0.669394\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6389790556647561\n",
      "Epoch 4: val loss 0.662142\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6251026554541155\n",
      "Epoch 5: val loss 0.656028\n",
      "\n",
      "Epoch %d: train loss %f 6 0.5923130295493386\n",
      "Epoch 6: val loss 0.663167\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5930004011500966\n",
      "Epoch 7: val loss 0.641940\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5870278694412925\n",
      "Epoch 8: val loss 0.632918\n",
      "\n",
      "Epoch %d: train loss %f 9 0.572439432144165\n",
      "Epoch 9: val loss 0.639682\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5635444034229625\n",
      "Epoch 10: val loss 0.625199\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5635485676201907\n",
      "Epoch 11: val loss 0.612471\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5585754134438254\n",
      "Epoch 12: val loss 0.601998\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5411866659467871\n",
      "Epoch 13: val loss 0.606298\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5434230322187598\n",
      "Epoch 14: val loss 0.609840\n",
      "\n",
      "Epoch %d: train loss %f 15 0.542763582684777\n",
      "Epoch 15: val loss 0.603036\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5334557565775785\n",
      "Epoch 16: val loss 0.611718\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5359639715064656\n",
      "Epoch 17: val loss 0.598420\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5336823924021288\n",
      "Epoch 18: val loss 0.603797\n",
      "\n",
      "Epoch %d: train loss %f 19 0.525192832404917\n",
      "Epoch 19: val loss 0.588731\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5183150903745131\n",
      "Epoch 20: val loss 0.600434\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5109529034657911\n",
      "Epoch 21: val loss 0.597853\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5114802609790455\n",
      "Epoch 22: val loss 0.581653\n",
      "\n",
      "Epoch %d: train loss %f 23 0.5066214881160043\n",
      "Epoch 23: val loss 0.583509\n",
      "\n",
      "Epoch %d: train loss %f 24 0.4971490773287686\n",
      "Epoch 24: val loss 0.581948\n",
      "\n",
      "Epoch %d: train loss %f 25 0.48961215940388764\n",
      "Epoch 25: val loss 0.572740\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4997713430361314\n",
      "Epoch 26: val loss 0.599250\n",
      "\n",
      "Epoch %d: train loss %f 27 0.49871091680093244\n",
      "Epoch 27: val loss 0.589446\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4791052287275141\n",
      "Epoch 28: val loss 0.571098\n",
      "\n",
      "Epoch %d: train loss %f 29 0.48673323338682\n",
      "Epoch 29: val loss 0.589267\n",
      "\n",
      "Epoch %d: train loss %f 30 0.4790720831264149\n",
      "Epoch 30: val loss 0.579101\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4797590402039615\n",
      "Epoch 31: val loss 0.579081\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4607728204943917\n",
      "Epoch 32: val loss 0.580348\n",
      "\n",
      "Epoch %d: train loss %f 33 0.45714651996439154\n",
      "Epoch 33: val loss 0.585956\n",
      "\n",
      "Epoch %d: train loss %f 34 0.48437200351194903\n",
      "Epoch 34: val loss 0.557361\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4633540186015042\n",
      "Epoch 35: val loss 0.573454\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4566690054806796\n",
      "Epoch 36: val loss 0.559116\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4444487149065191\n",
      "Epoch 37: val loss 0.560694\n",
      "\n",
      "Epoch %d: train loss %f 38 0.45408980412916705\n",
      "Epoch 38: val loss 0.568799\n",
      "\n",
      "Epoch %d: train loss %f 39 0.45053373683582654\n",
      "Epoch 39: val loss 0.560600\n",
      "\n",
      "Epoch %d: train loss %f 40 0.4456308294426311\n",
      "Epoch 40: val loss 0.567315\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4629503298889507\n",
      "Epoch 41: val loss 0.564311\n",
      "\n",
      "Epoch %d: train loss %f 42 0.44455726038325916\n",
      "Epoch 42: val loss 0.574099\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4462439634583213\n",
      "Epoch 43: val loss 0.548042\n",
      "\n",
      "Epoch %d: train loss %f 44 0.45397759025747125\n",
      "Epoch 44: val loss 0.574956\n",
      "\n",
      "Epoch %d: train loss %f 45 0.43203823132948443\n",
      "Epoch 45: val loss 0.546711\n",
      "\n",
      "Epoch %d: train loss %f 46 0.45389173247597436\n",
      "Epoch 46: val loss 0.545397\n",
      "\n",
      "Epoch %d: train loss %f 47 0.4276010285724293\n",
      "Epoch 47: val loss 0.558357\n",
      "\n",
      "Epoch %d: train loss %f 48 0.4470834569497542\n",
      "Epoch 48: val loss 0.539276\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4404356452551755\n",
      "Epoch 49: val loss 0.550592\n",
      "\n",
      "Epoch %d: train loss %f 50 0.434212018143047\n",
      "Epoch 50: val loss 0.548297\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4452982382340865\n",
      "Epoch 51: val loss 0.569348\n",
      "\n",
      "Epoch %d: train loss %f 52 0.4272800548510118\n",
      "Epoch 52: val loss 0.534453\n",
      "\n",
      "Epoch %d: train loss %f 53 0.4296216477047313\n",
      "Epoch 53: val loss 0.550322\n",
      "\n",
      "Epoch %d: train loss %f 54 0.43390128016471863\n",
      "Epoch 54: val loss 0.538191\n",
      "\n",
      "Epoch %d: train loss %f 55 0.42676566405729816\n",
      "Epoch 55: val loss 0.529041\n",
      "\n",
      "Epoch %d: train loss %f 56 0.42497115514495154\n",
      "Epoch 56: val loss 0.546788\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4038147682493383\n",
      "Epoch 57: val loss 0.562193\n",
      "\n",
      "Epoch %d: train loss %f 58 0.45178236202760175\n",
      "Epoch 58: val loss 0.576923\n",
      "\n",
      "Epoch %d: train loss %f 59 0.4220189235427163\n",
      "Epoch 59: val loss 0.579058\n",
      "\n",
      "Epoch %d: train loss %f 60 0.40127181194045325\n",
      "Epoch 60: val loss 0.535285\n",
      "\n",
      "Epoch %d: train loss %f 61 0.41690427335825836\n",
      "Epoch 61: val loss 0.562255\n",
      "\n",
      "Epoch %d: train loss %f 62 0.4069550931453705\n",
      "Epoch 62: val loss 0.570924\n",
      "\n",
      "Epoch %d: train loss %f 63 0.4254925305193121\n",
      "Epoch 63: val loss 0.556369\n",
      "\n",
      "Epoch %d: train loss %f 64 0.3963865502314134\n",
      "Epoch 64: val loss 0.580949\n",
      "\n",
      "Epoch %d: train loss %f 65 0.4445177208293568\n",
      "Epoch 65: val loss 0.576948\n",
      "\n",
      "Epoch %d: train loss %f 66 0.4058848863298243\n",
      "Epoch 66: val loss 0.565928\n",
      "\n",
      "Epoch %d: train loss %f 67 0.3872227208180861\n",
      "Epoch 67: val loss 0.546935\n",
      "\n",
      "Epoch %d: train loss %f 68 0.4168343516913327\n",
      "Epoch 68: val loss 0.559212\n",
      "\n",
      "Epoch %d: train loss %f 69 0.37644672800194134\n",
      "Epoch 69: val loss 0.564310\n",
      "\n",
      "Epoch %d: train loss %f 70 0.4063550694422288\n",
      "Epoch 70: val loss 0.570358\n",
      "\n",
      "Epoch %d: train loss %f 71 0.38136665387587115\n",
      "Epoch 71: val loss 0.543555\n",
      "\n",
      "Epoch %d: train loss %f 72 0.36803789030421863\n",
      "Epoch 72: val loss 0.538242\n",
      "\n",
      "Epoch %d: train loss %f 73 0.4125159003517844\n",
      "Epoch 73: val loss 0.572139\n",
      "\n",
      "Epoch %d: train loss %f 74 0.3838931213725697\n",
      "Epoch 74: val loss 0.546613\n",
      "\n",
      "Epoch %d: train loss %f 75 0.37819772416895087\n",
      "Epoch 75: val loss 0.548803\n",
      "\n",
      "Epoch %d: train loss %f 76 0.3913592520085248\n",
      "Epoch 76: val loss 0.560491\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3733712001280351\n",
      "Epoch 77: val loss 0.568494\n",
      "\n",
      "Epoch %d: train loss %f 78 0.37295142764394934\n",
      "Epoch 78: val loss 0.556644\n",
      "\n",
      "Epoch %d: train loss %f 79 0.4138749512759122\n",
      "Epoch 79: val loss 0.557390\n",
      "\n",
      "Epoch %d: train loss %f 80 0.35077706250277435\n",
      "Epoch 80: val loss 0.552195\n",
      "\n",
      "Epoch %d: train loss %f 81 0.38034726814790204\n",
      "Epoch 81: val loss 0.549998\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3662224222313274\n",
      "Epoch 82: val loss 0.598922\n",
      "\n",
      "Epoch %d: train loss %f 83 0.39525492760268127\n",
      "Epoch 83: val loss 0.558593\n",
      "\n",
      "Epoch %d: train loss %f 84 0.38120178200981836\n",
      "Epoch 84: val loss 0.549887\n",
      "\n",
      "Epoch %d: train loss %f 85 0.3935935375365344\n",
      "Epoch 85: val loss 0.530408\n",
      "\n",
      "Epoch %d: train loss %f 86 0.36994463205337524\n",
      "Epoch 86: val loss 0.549066\n",
      "\n",
      "Epoch %d: train loss %f 87 0.3873352598060261\n",
      "Epoch 87: val loss 0.544529\n",
      "\n",
      "Epoch %d: train loss %f 88 0.35522845116528595\n",
      "Epoch 88: val loss 0.563517\n",
      "\n",
      "Epoch %d: train loss %f 89 0.3843800588087602\n",
      "Epoch 89: val loss 0.566245\n",
      "\n",
      "Epoch %d: train loss %f 90 0.37317038395188074\n",
      "Epoch 90: val loss 0.575013\n",
      "\n",
      "Epoch %d: train loss %f 91 0.3905713205987757\n",
      "Epoch 91: val loss 0.589106\n",
      "\n",
      "Epoch %d: train loss %f 92 0.36111188205805694\n",
      "Epoch 92: val loss 0.558782\n",
      "\n",
      "Epoch %d: train loss %f 93 0.36653699387203564\n",
      "Epoch 93: val loss 0.582881\n",
      "\n",
      "Epoch %d: train loss %f 94 0.3458087065003135\n",
      "Epoch 94: val loss 0.570856\n",
      "\n",
      "Epoch %d: train loss %f 95 0.38362449407577515\n",
      "Epoch 95: val loss 0.554255\n",
      "\n",
      "Epoch %d: train loss %f 96 0.35513275319879706\n",
      "Epoch 96: val loss 0.572548\n",
      "\n",
      "Epoch %d: train loss %f 97 0.3693567988547412\n",
      "Epoch 97: val loss 0.581803\n",
      "\n",
      "Epoch %d: train loss %f 98 0.35885717652060767\n",
      "Epoch 98: val loss 0.571712\n",
      "\n",
      "Epoch %d: train loss %f 99 0.34704746305942535\n",
      "Epoch 99: val loss 0.522168\n",
      "\n",
      "Epoch %d: train loss %f 100 0.33740257945927704\n",
      "Epoch 100: val loss 0.573596\n",
      "\n",
      "Epoch %d: train loss %f 101 0.3674023043025624\n",
      "Epoch 101: val loss 0.588921\n",
      "\n",
      "Epoch %d: train loss %f 102 0.3518611585552042\n",
      "Epoch 102: val loss 0.556953\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3232543007893996\n",
      "Epoch 103: val loss 0.566847\n",
      "\n",
      "Epoch %d: train loss %f 104 0.36741125041788275\n",
      "Epoch 104: val loss 0.554522\n",
      "\n",
      "Epoch %d: train loss %f 105 0.3268617350946773\n",
      "Epoch 105: val loss 0.554600\n",
      "\n",
      "Epoch %d: train loss %f 106 0.3432701514525847\n",
      "Epoch 106: val loss 0.555623\n",
      "\n",
      "Epoch %d: train loss %f 107 0.34966626221483404\n",
      "Epoch 107: val loss 0.579334\n",
      "\n",
      "Epoch %d: train loss %f 108 0.39609641514041205\n",
      "Epoch 108: val loss 0.551025\n",
      "\n",
      "Epoch %d: train loss %f 109 0.36401383172382007\n",
      "Epoch 109: val loss 0.589938\n",
      "\n",
      "Epoch %d: train loss %f 110 0.3557097613811493\n",
      "Epoch 110: val loss 0.582797\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3392351134256883\n",
      "Epoch 111: val loss 0.575243\n",
      "\n",
      "Epoch %d: train loss %f 112 0.3913652923974124\n",
      "Epoch 112: val loss 0.561306\n",
      "\n",
      "Epoch %d: train loss %f 113 0.37544390152801166\n",
      "Epoch 113: val loss 0.582090\n",
      "\n",
      "Epoch %d: train loss %f 114 0.3352187682281841\n",
      "Epoch 114: val loss 0.566490\n",
      "\n",
      "Epoch %d: train loss %f 115 0.337238837372173\n",
      "Epoch 115: val loss 0.557351\n",
      "\n",
      "Epoch %d: train loss %f 116 0.33369393240321765\n",
      "Epoch 116: val loss 0.577510\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3350263793360103\n",
      "Epoch 117: val loss 0.579654\n",
      "\n",
      "Epoch %d: train loss %f 118 0.31600919230417773\n",
      "Epoch 118: val loss 0.559967\n",
      "\n",
      "Epoch %d: train loss %f 119 0.34302635761824524\n",
      "Epoch 119: val loss 0.583747\n",
      "\n",
      "Epoch %d: train loss %f 120 0.347305030985312\n",
      "Epoch 120: val loss 0.552436\n",
      "\n",
      "Epoch %d: train loss %f 121 0.32972344891591504\n",
      "Epoch 121: val loss 0.598049\n",
      "\n",
      "Epoch %d: train loss %f 122 0.3547913513400338\n",
      "Epoch 122: val loss 0.530426\n",
      "\n",
      "Epoch %d: train loss %f 123 0.3667207577011802\n",
      "Epoch 123: val loss 0.576104\n",
      "\n",
      "Epoch %d: train loss %f 124 0.3338607996702194\n",
      "Epoch 124: val loss 0.595300\n",
      "\n",
      "Epoch %d: train loss %f 125 0.3558028679002415\n",
      "Epoch 125: val loss 0.587033\n",
      "\n",
      "Epoch %d: train loss %f 126 0.34203011203895917\n",
      "Epoch 126: val loss 0.589534\n",
      "\n",
      "Epoch %d: train loss %f 127 0.37758846987377515\n",
      "Epoch 127: val loss 0.589346\n",
      "\n",
      "Epoch %d: train loss %f 128 0.3000018881125884\n",
      "Epoch 128: val loss 0.546998\n",
      "\n",
      "Epoch %d: train loss %f 129 0.3381909849968823\n",
      "Epoch 129: val loss 0.560772\n",
      "\n",
      "Epoch %d: train loss %f 130 0.34097554602406244\n",
      "Epoch 130: val loss 0.557261\n",
      "\n",
      "Epoch %d: train loss %f 131 0.32177943126721814\n",
      "Epoch 131: val loss 0.543740\n",
      "\n",
      "Epoch %d: train loss %f 132 0.3318827043880116\n",
      "Epoch 132: val loss 0.602841\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3418684926899997\n",
      "Epoch 133: val loss 0.555728\n",
      "\n",
      "Epoch %d: train loss %f 134 0.3176145147193562\n",
      "Epoch 134: val loss 0.586937\n",
      "\n",
      "Epoch %d: train loss %f 135 0.36455500396815216\n",
      "Epoch 135: val loss 0.558825\n",
      "\n",
      "Epoch %d: train loss %f 136 0.3265037319876931\n",
      "Epoch 136: val loss 0.559957\n",
      "\n",
      "Epoch %d: train loss %f 137 0.2918509272011844\n",
      "Epoch 137: val loss 0.554358\n",
      "\n",
      "Epoch %d: train loss %f 138 0.3335296321998943\n",
      "Epoch 138: val loss 0.570401\n",
      "\n",
      "Epoch %d: train loss %f 139 0.31335020336237823\n",
      "Epoch 139: val loss 0.554740\n",
      "\n",
      "Epoch %d: train loss %f 140 0.33365587212822656\n",
      "Epoch 140: val loss 0.556177\n",
      "\n",
      "Epoch %d: train loss %f 141 0.330097726800225\n",
      "Epoch 141: val loss 0.544793\n",
      "\n",
      "Epoch %d: train loss %f 142 0.3249808996915817\n",
      "Epoch 142: val loss 0.599184\n",
      "\n",
      "Epoch %d: train loss %f 143 0.3220206295902079\n",
      "Epoch 143: val loss 0.554938\n",
      "\n",
      "Epoch %d: train loss %f 144 0.28617152029817755\n",
      "Epoch 144: val loss 0.537279\n",
      "\n",
      "Epoch %d: train loss %f 145 0.31413874707438727\n",
      "Epoch 145: val loss 0.578242\n",
      "\n",
      "Epoch %d: train loss %f 146 0.2966740510680459\n",
      "Epoch 146: val loss 0.557099\n",
      "\n",
      "Epoch %d: train loss %f 147 0.3217700855298476\n",
      "Epoch 147: val loss 0.533502\n",
      "\n",
      "Epoch %d: train loss %f 148 0.36391778696667065\n",
      "Epoch 148: val loss 0.572220\n",
      "\n",
      "Epoch %d: train loss %f 149 0.3130878887393258\n",
      "Epoch 149: val loss 0.550727\n",
      "\n",
      "Epoch %d: train loss %f 150 0.3460262837735089\n",
      "Epoch 150: val loss 0.607130\n",
      "\n",
      "Epoch %d: train loss %f 151 0.29105044630440796\n",
      "Epoch 151: val loss 0.572173\n",
      "\n",
      "Epoch %d: train loss %f 152 0.2976622540842403\n",
      "Epoch 152: val loss 0.568696\n",
      "\n",
      "Epoch %d: train loss %f 153 0.3328828445889733\n",
      "Epoch 153: val loss 0.583180\n",
      "\n",
      "Epoch %d: train loss %f 154 0.3260603750293905\n",
      "Epoch 154: val loss 0.597435\n",
      "\n",
      "Epoch %d: train loss %f 155 0.3120089349421588\n",
      "Epoch 155: val loss 0.542170\n",
      "\n",
      "Epoch %d: train loss %f 156 0.2993283393708142\n",
      "Epoch 156: val loss 0.558304\n",
      "\n",
      "Epoch %d: train loss %f 157 0.31180531870235095\n",
      "Epoch 157: val loss 0.577146\n",
      "\n",
      "Epoch %d: train loss %f 158 0.29279030046679755\n",
      "Epoch 158: val loss 0.548476\n",
      "\n",
      "Epoch %d: train loss %f 159 0.33349808102304285\n",
      "Epoch 159: val loss 0.574760\n",
      "\n",
      "Epoch %d: train loss %f 160 0.3077916475859555\n",
      "Epoch 160: val loss 0.563104\n",
      "\n",
      "Epoch %d: train loss %f 161 0.2876062325455926\n",
      "Epoch 161: val loss 0.571327\n",
      "\n",
      "Epoch %d: train loss %f 162 0.27444864132187585\n",
      "Epoch 162: val loss 0.595795\n",
      "\n",
      "Epoch %d: train loss %f 163 0.311995575373823\n",
      "Epoch 163: val loss 0.614095\n",
      "\n",
      "Epoch %d: train loss %f 164 0.29087045653299853\n",
      "Epoch 164: val loss 0.561190\n",
      "\n",
      "Epoch %d: train loss %f 165 0.3281155255707828\n",
      "Epoch 165: val loss 0.569984\n",
      "\n",
      "Epoch %d: train loss %f 166 0.30607750605453143\n",
      "Epoch 166: val loss 0.589874\n",
      "\n",
      "Epoch %d: train loss %f 167 0.3206134411421689\n",
      "Epoch 167: val loss 0.600430\n",
      "\n",
      "Epoch %d: train loss %f 168 0.3267362646081231\n",
      "Epoch 168: val loss 0.583099\n",
      "\n",
      "Epoch %d: train loss %f 169 0.3022520447319204\n",
      "Epoch 169: val loss 0.569610\n",
      "\n",
      "Epoch %d: train loss %f 170 0.2956080585718155\n",
      "Epoch 170: val loss 0.579545\n",
      "\n",
      "Epoch %d: train loss %f 171 0.2980227145281705\n",
      "Epoch 171: val loss 0.542551\n",
      "\n",
      "Epoch %d: train loss %f 172 0.3014495196667584\n",
      "Epoch 172: val loss 0.596537\n",
      "\n",
      "Epoch %d: train loss %f 173 0.2914127693934874\n",
      "Epoch 173: val loss 0.560878\n",
      "\n",
      "Epoch %d: train loss %f 174 0.2593533843755722\n",
      "Epoch 174: val loss 0.578216\n",
      "\n",
      "Epoch %d: train loss %f 175 0.28272032873197034\n",
      "Epoch 175: val loss 0.562231\n",
      "\n",
      "Epoch %d: train loss %f 176 0.3284981846809387\n",
      "Epoch 176: val loss 0.585928\n",
      "\n",
      "Epoch %d: train loss %f 177 0.291634971445257\n",
      "Epoch 177: val loss 0.625881\n",
      "\n",
      "Epoch %d: train loss %f 178 0.2919123294678601\n",
      "Epoch 178: val loss 0.612954\n",
      "\n",
      "Epoch %d: train loss %f 179 0.2859240458770232\n",
      "Epoch 179: val loss 0.601536\n",
      "\n",
      "Epoch %d: train loss %f 180 0.29567007991400635\n",
      "Epoch 180: val loss 0.594978\n",
      "\n",
      "Epoch %d: train loss %f 181 0.3334761749614369\n",
      "Epoch 181: val loss 0.578777\n",
      "\n",
      "Epoch %d: train loss %f 182 0.2998600927266208\n",
      "Epoch 182: val loss 0.584456\n",
      "\n",
      "Epoch %d: train loss %f 183 0.31322055242278357\n",
      "Epoch 183: val loss 0.662482\n",
      "\n",
      "Epoch %d: train loss %f 184 0.26611675999381323\n",
      "Epoch 184: val loss 0.563927\n",
      "\n",
      "Epoch %d: train loss %f 185 0.2858669635924426\n",
      "Epoch 185: val loss 0.572806\n",
      "\n",
      "Epoch %d: train loss %f 186 0.32942521572113037\n",
      "Epoch 186: val loss 0.595057\n",
      "\n",
      "Epoch %d: train loss %f 187 0.2631372321735729\n",
      "Epoch 187: val loss 0.619392\n",
      "\n",
      "Epoch %d: train loss %f 188 0.2971054396846078\n",
      "Epoch 188: val loss 0.573882\n",
      "\n",
      "Epoch %d: train loss %f 189 0.29951381276954303\n",
      "Epoch 189: val loss 0.567106\n",
      "\n",
      "Epoch %d: train loss %f 190 0.269729194315997\n",
      "Epoch 190: val loss 0.587405\n",
      "\n",
      "Epoch %d: train loss %f 191 0.3200465887784958\n",
      "Epoch 191: val loss 0.586960\n",
      "\n",
      "Epoch %d: train loss %f 192 0.27689159322868695\n",
      "Epoch 192: val loss 0.570803\n",
      "\n",
      "Epoch %d: train loss %f 193 0.3124604563821446\n",
      "Epoch 193: val loss 0.578945\n",
      "\n",
      "Epoch %d: train loss %f 194 0.284944931214506\n",
      "Epoch 194: val loss 0.593815\n",
      "\n",
      "Epoch %d: train loss %f 195 0.2881580604748292\n",
      "Epoch 195: val loss 0.649944\n",
      "\n",
      "Epoch %d: train loss %f 196 0.3277555148709904\n",
      "Epoch 196: val loss 0.638742\n",
      "\n",
      "Epoch %d: train loss %f 197 0.24906351349570535\n",
      "Epoch 197: val loss 0.593978\n",
      "\n",
      "Epoch %d: train loss %f 198 0.2610711191188205\n",
      "Epoch 198: val loss 0.627800\n",
      "\n",
      "Epoch %d: train loss %f 199 0.29341012781316583\n",
      "Epoch 199: val loss 0.560439\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6991731361909346\n",
      "Epoch 0: val loss 0.697458\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6922336220741272\n",
      "Epoch 1: val loss 0.695069\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6853564002297141\n",
      "Epoch 2: val loss 0.691455\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6743205135518854\n",
      "Epoch 3: val loss 0.684421\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6584860248999163\n",
      "Epoch 4: val loss 0.671023\n",
      "\n",
      "Epoch %d: train loss %f 5 0.64380220933394\n",
      "Epoch 5: val loss 0.658114\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6274012110450051\n",
      "Epoch 6: val loss 0.645805\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6037373488599603\n",
      "Epoch 7: val loss 0.624652\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5758496685461565\n",
      "Epoch 8: val loss 0.609001\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5636757043274966\n",
      "Epoch 9: val loss 0.591805\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5640539797869596\n",
      "Epoch 10: val loss 0.590970\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5555227778174661\n",
      "Epoch 11: val loss 0.591751\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5368174531243064\n",
      "Epoch 12: val loss 0.592787\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5344163233583624\n",
      "Epoch 13: val loss 0.601922\n",
      "\n",
      "Epoch %d: train loss %f 14 0.534307146614248\n",
      "Epoch 14: val loss 0.579242\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5284348943016746\n",
      "Epoch 15: val loss 0.585883\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5106672454964031\n",
      "Epoch 16: val loss 0.580163\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5055434134873477\n",
      "Epoch 17: val loss 0.600029\n",
      "\n",
      "Epoch %d: train loss %f 18 0.500192479653792\n",
      "Epoch 18: val loss 0.589791\n",
      "\n",
      "Epoch %d: train loss %f 19 0.4861257889054038\n",
      "Epoch 19: val loss 0.581125\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5084670646624132\n",
      "Epoch 20: val loss 0.592335\n",
      "\n",
      "Epoch %d: train loss %f 21 0.49705104394392535\n",
      "Epoch 21: val loss 0.590471\n",
      "\n",
      "Epoch %d: train loss %f 22 0.489347216757861\n",
      "Epoch 22: val loss 0.589026\n",
      "\n",
      "Epoch %d: train loss %f 23 0.4849399274045771\n",
      "Epoch 23: val loss 0.609124\n",
      "\n",
      "Epoch %d: train loss %f 24 0.48611303622072394\n",
      "Epoch 24: val loss 0.599556\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4805496903983029\n",
      "Epoch 25: val loss 0.593572\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4801543111150915\n",
      "Epoch 26: val loss 0.610670\n",
      "\n",
      "Epoch %d: train loss %f 27 0.488404165614735\n",
      "Epoch 27: val loss 0.591437\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4724429222670468\n",
      "Epoch 28: val loss 0.606227\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4645298556848006\n",
      "Epoch 29: val loss 0.599805\n",
      "\n",
      "Epoch %d: train loss %f 30 0.48250335454940796\n",
      "Epoch 30: val loss 0.628706\n",
      "\n",
      "Epoch %d: train loss %f 31 0.49152094396677887\n",
      "Epoch 31: val loss 0.597086\n",
      "\n",
      "Epoch %d: train loss %f 32 0.46607831662351434\n",
      "Epoch 32: val loss 0.599999\n",
      "\n",
      "Epoch %d: train loss %f 33 0.46278672326694836\n",
      "Epoch 33: val loss 0.610070\n",
      "\n",
      "Epoch %d: train loss %f 34 0.4636468128724532\n",
      "Epoch 34: val loss 0.622332\n",
      "\n",
      "Epoch %d: train loss %f 35 0.43632641434669495\n",
      "Epoch 35: val loss 0.618918\n",
      "\n",
      "Epoch %d: train loss %f 36 0.461063642393459\n",
      "Epoch 36: val loss 0.631301\n",
      "\n",
      "Epoch %d: train loss %f 37 0.45390105518427765\n",
      "Epoch 37: val loss 0.626098\n",
      "\n",
      "Epoch %d: train loss %f 38 0.46231724186377093\n",
      "Epoch 38: val loss 0.609280\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4418867067857222\n",
      "Epoch 39: val loss 0.611210\n",
      "\n",
      "Epoch %d: train loss %f 40 0.44292586229064246\n",
      "Epoch 40: val loss 0.634047\n",
      "\n",
      "Epoch %d: train loss %f 41 0.44557206468148663\n",
      "Epoch 41: val loss 0.645530\n",
      "\n",
      "Epoch %d: train loss %f 42 0.46524510058489715\n",
      "Epoch 42: val loss 0.631518\n",
      "\n",
      "Epoch %d: train loss %f 43 0.44366253776983783\n",
      "Epoch 43: val loss 0.637687\n",
      "\n",
      "Epoch %d: train loss %f 44 0.44733006033030426\n",
      "Epoch 44: val loss 0.639625\n",
      "\n",
      "Epoch %d: train loss %f 45 0.4637888453223489\n",
      "Epoch 45: val loss 0.616291\n",
      "\n",
      "Epoch %d: train loss %f 46 0.44350368732755835\n",
      "Epoch 46: val loss 0.657967\n",
      "\n",
      "Epoch %d: train loss %f 47 0.4429426816376773\n",
      "Epoch 47: val loss 0.646222\n",
      "\n",
      "Epoch %d: train loss %f 48 0.43276318365877325\n",
      "Epoch 48: val loss 0.630642\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4258039349859411\n",
      "Epoch 49: val loss 0.655855\n",
      "\n",
      "Epoch %d: train loss %f 50 0.4291535588828\n",
      "Epoch 50: val loss 0.649902\n",
      "\n",
      "Epoch %d: train loss %f 51 0.42907808314670215\n",
      "Epoch 51: val loss 0.663979\n",
      "\n",
      "Epoch %d: train loss %f 52 0.41687898202375934\n",
      "Epoch 52: val loss 0.657517\n",
      "\n",
      "Epoch %d: train loss %f 53 0.42708212679082697\n",
      "Epoch 53: val loss 0.645260\n",
      "\n",
      "Epoch %d: train loss %f 54 0.41531628370285034\n",
      "Epoch 54: val loss 0.636336\n",
      "\n",
      "Epoch %d: train loss %f 55 0.4053640744902871\n",
      "Epoch 55: val loss 0.639269\n",
      "\n",
      "Epoch %d: train loss %f 56 0.4288117397915233\n",
      "Epoch 56: val loss 0.665950\n",
      "\n",
      "Epoch %d: train loss %f 57 0.399636218493635\n",
      "Epoch 57: val loss 0.664783\n",
      "\n",
      "Epoch %d: train loss %f 58 0.4202388037334789\n",
      "Epoch 58: val loss 0.654451\n",
      "\n",
      "Epoch %d: train loss %f 59 0.4215647131204605\n",
      "Epoch 59: val loss 0.651562\n",
      "\n",
      "Epoch %d: train loss %f 60 0.4157204302874478\n",
      "Epoch 60: val loss 0.650691\n",
      "\n",
      "Epoch %d: train loss %f 61 0.44097084077921783\n",
      "Epoch 61: val loss 0.637268\n",
      "\n",
      "Epoch %d: train loss %f 62 0.4423750774426894\n",
      "Epoch 62: val loss 0.649099\n",
      "\n",
      "Epoch %d: train loss %f 63 0.408088819547133\n",
      "Epoch 63: val loss 0.633691\n",
      "\n",
      "Epoch %d: train loss %f 64 0.41435105963186786\n",
      "Epoch 64: val loss 0.664364\n",
      "\n",
      "Epoch %d: train loss %f 65 0.4198307124051181\n",
      "Epoch 65: val loss 0.652996\n",
      "\n",
      "Epoch %d: train loss %f 66 0.3850920227440921\n",
      "Epoch 66: val loss 0.667804\n",
      "\n",
      "Epoch %d: train loss %f 67 0.4362480179830031\n",
      "Epoch 67: val loss 0.673948\n",
      "\n",
      "Epoch %d: train loss %f 68 0.43085386807268317\n",
      "Epoch 68: val loss 0.641950\n",
      "\n",
      "Epoch %d: train loss %f 69 0.38766831159591675\n",
      "Epoch 69: val loss 0.646677\n",
      "\n",
      "Epoch %d: train loss %f 70 0.4204579618844119\n",
      "Epoch 70: val loss 0.651517\n",
      "\n",
      "Epoch %d: train loss %f 71 0.3920727737925269\n",
      "Epoch 71: val loss 0.650756\n",
      "\n",
      "Epoch %d: train loss %f 72 0.40592447736046533\n",
      "Epoch 72: val loss 0.643016\n",
      "\n",
      "Epoch %d: train loss %f 73 0.392507251013409\n",
      "Epoch 73: val loss 0.652481\n",
      "\n",
      "Epoch %d: train loss %f 74 0.387930764393373\n",
      "Epoch 74: val loss 0.651887\n",
      "\n",
      "Epoch %d: train loss %f 75 0.42654548720879987\n",
      "Epoch 75: val loss 0.664154\n",
      "\n",
      "Epoch %d: train loss %f 76 0.40852234851230274\n",
      "Epoch 76: val loss 0.657811\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3936779038472609\n",
      "Epoch 77: val loss 0.625585\n",
      "\n",
      "Epoch %d: train loss %f 78 0.3747103674845262\n",
      "Epoch 78: val loss 0.624264\n",
      "\n",
      "Epoch %d: train loss %f 79 0.39296274293552746\n",
      "Epoch 79: val loss 0.649722\n",
      "\n",
      "Epoch %d: train loss %f 80 0.3950514251535589\n",
      "Epoch 80: val loss 0.662248\n",
      "\n",
      "Epoch %d: train loss %f 81 0.3761329027739438\n",
      "Epoch 81: val loss 0.650202\n",
      "\n",
      "Epoch %d: train loss %f 82 0.38520568067377264\n",
      "Epoch 82: val loss 0.694666\n",
      "\n",
      "Epoch %d: train loss %f 83 0.44206823814998975\n",
      "Epoch 83: val loss 0.702923\n",
      "\n",
      "Epoch %d: train loss %f 84 0.3978184705430811\n",
      "Epoch 84: val loss 0.637406\n",
      "\n",
      "Epoch %d: train loss %f 85 0.40505792877890845\n",
      "Epoch 85: val loss 0.646613\n",
      "\n",
      "Epoch %d: train loss %f 86 0.4064885689453645\n",
      "Epoch 86: val loss 0.683498\n",
      "\n",
      "Epoch %d: train loss %f 87 0.4063751047307795\n",
      "Epoch 87: val loss 0.654114\n",
      "\n",
      "Epoch %d: train loss %f 88 0.37192267314954236\n",
      "Epoch 88: val loss 0.654520\n",
      "\n",
      "Epoch %d: train loss %f 89 0.39864266460592096\n",
      "Epoch 89: val loss 0.650832\n",
      "\n",
      "Epoch %d: train loss %f 90 0.39381086826324463\n",
      "Epoch 90: val loss 0.648597\n",
      "\n",
      "Epoch %d: train loss %f 91 0.3985088264400309\n",
      "Epoch 91: val loss 0.674960\n",
      "\n",
      "Epoch %d: train loss %f 92 0.35674314336343244\n",
      "Epoch 92: val loss 0.648727\n",
      "\n",
      "Epoch %d: train loss %f 93 0.4209999740123749\n",
      "Epoch 93: val loss 0.651214\n",
      "\n",
      "Epoch %d: train loss %f 94 0.3868907771327279\n",
      "Epoch 94: val loss 0.660447\n",
      "\n",
      "Epoch %d: train loss %f 95 0.37161949276924133\n",
      "Epoch 95: val loss 0.664277\n",
      "\n",
      "Epoch %d: train loss %f 96 0.359191280874339\n",
      "Epoch 96: val loss 0.628383\n",
      "\n",
      "Epoch %d: train loss %f 97 0.35956594347953796\n",
      "Epoch 97: val loss 0.655578\n",
      "\n",
      "Epoch %d: train loss %f 98 0.3517319451678883\n",
      "Epoch 98: val loss 0.655028\n",
      "\n",
      "Epoch %d: train loss %f 99 0.38360927728089417\n",
      "Epoch 99: val loss 0.638524\n",
      "\n",
      "Epoch %d: train loss %f 100 0.35825722867792303\n",
      "Epoch 100: val loss 0.640377\n",
      "\n",
      "Epoch %d: train loss %f 101 0.3383735472505743\n",
      "Epoch 101: val loss 0.678704\n",
      "\n",
      "Epoch %d: train loss %f 102 0.3626358752900904\n",
      "Epoch 102: val loss 0.633102\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3716503625566309\n",
      "Epoch 103: val loss 0.635858\n",
      "\n",
      "Epoch %d: train loss %f 104 0.35495805740356445\n",
      "Epoch 104: val loss 0.672442\n",
      "\n",
      "Epoch %d: train loss %f 105 0.3510755544359034\n",
      "Epoch 105: val loss 0.656859\n",
      "\n",
      "Epoch %d: train loss %f 106 0.33634823018854315\n",
      "Epoch 106: val loss 0.648836\n",
      "\n",
      "Epoch %d: train loss %f 107 0.3625565387985923\n",
      "Epoch 107: val loss 0.681503\n",
      "\n",
      "Epoch %d: train loss %f 108 0.35861609876155853\n",
      "Epoch 108: val loss 0.678691\n",
      "\n",
      "Epoch %d: train loss %f 109 0.37367395108396356\n",
      "Epoch 109: val loss 0.658749\n",
      "\n",
      "Epoch %d: train loss %f 110 0.3588449467312206\n",
      "Epoch 110: val loss 0.657100\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3765040094202215\n",
      "Epoch 111: val loss 0.643173\n",
      "\n",
      "Epoch %d: train loss %f 112 0.33416556499221106\n",
      "Epoch 112: val loss 0.663350\n",
      "\n",
      "Epoch %d: train loss %f 113 0.34059657833792945\n",
      "Epoch 113: val loss 0.704774\n",
      "\n",
      "Epoch %d: train loss %f 114 0.38127218051390216\n",
      "Epoch 114: val loss 0.675098\n",
      "\n",
      "Epoch %d: train loss %f 115 0.3355241539803418\n",
      "Epoch 115: val loss 0.673790\n",
      "\n",
      "Epoch %d: train loss %f 116 0.36767336048863153\n",
      "Epoch 116: val loss 0.646869\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3445436968044801\n",
      "Epoch 117: val loss 0.646075\n",
      "\n",
      "Epoch %d: train loss %f 118 0.3770813373002139\n",
      "Epoch 118: val loss 0.657971\n",
      "\n",
      "Epoch %d: train loss %f 119 0.3187633143229918\n",
      "Epoch 119: val loss 0.686933\n",
      "\n",
      "Epoch %d: train loss %f 120 0.3314617709680037\n",
      "Epoch 120: val loss 0.662837\n",
      "\n",
      "Epoch %d: train loss %f 121 0.3468317701057954\n",
      "Epoch 121: val loss 0.696639\n",
      "\n",
      "Epoch %d: train loss %f 122 0.3460452231493863\n",
      "Epoch 122: val loss 0.679573\n",
      "\n",
      "Epoch %d: train loss %f 123 0.31944313645362854\n",
      "Epoch 123: val loss 0.665042\n",
      "\n",
      "Epoch %d: train loss %f 124 0.3420300998471\n",
      "Epoch 124: val loss 0.654931\n",
      "\n",
      "Epoch %d: train loss %f 125 0.3604570220817219\n",
      "Epoch 125: val loss 0.655481\n",
      "\n",
      "Epoch %d: train loss %f 126 0.36050325496630237\n",
      "Epoch 126: val loss 0.709782\n",
      "\n",
      "Epoch %d: train loss %f 127 0.31764383614063263\n",
      "Epoch 127: val loss 0.699208\n",
      "\n",
      "Epoch %d: train loss %f 128 0.3198891539465297\n",
      "Epoch 128: val loss 0.651972\n",
      "\n",
      "Epoch %d: train loss %f 129 0.33187041770328174\n",
      "Epoch 129: val loss 0.698034\n",
      "\n",
      "Epoch %d: train loss %f 130 0.3611518916758624\n",
      "Epoch 130: val loss 0.703318\n",
      "\n",
      "Epoch %d: train loss %f 131 0.32840801910920575\n",
      "Epoch 131: val loss 0.685958\n",
      "\n",
      "Epoch %d: train loss %f 132 0.3588168052109805\n",
      "Epoch 132: val loss 0.695151\n",
      "\n",
      "Epoch %d: train loss %f 133 0.35415020178664813\n",
      "Epoch 133: val loss 0.670355\n",
      "\n",
      "Epoch %d: train loss %f 134 0.33983217857100745\n",
      "Epoch 134: val loss 0.671764\n",
      "\n",
      "Epoch %d: train loss %f 135 0.3568167821927504\n",
      "Epoch 135: val loss 0.646750\n",
      "\n",
      "Epoch %d: train loss %f 136 0.33451518280939624\n",
      "Epoch 136: val loss 0.678822\n",
      "\n",
      "Epoch %d: train loss %f 137 0.3443528400226073\n",
      "Epoch 137: val loss 0.682024\n",
      "\n",
      "Epoch %d: train loss %f 138 0.3344720019535585\n",
      "Epoch 138: val loss 0.679518\n",
      "\n",
      "Epoch %d: train loss %f 139 0.35226177356459876\n",
      "Epoch 139: val loss 0.668794\n",
      "\n",
      "Epoch %d: train loss %f 140 0.33732221207835456\n",
      "Epoch 140: val loss 0.676240\n",
      "\n",
      "Epoch %d: train loss %f 141 0.3219028806144541\n",
      "Epoch 141: val loss 0.653303\n",
      "\n",
      "Epoch %d: train loss %f 142 0.3270746862346476\n",
      "Epoch 142: val loss 0.651810\n",
      "\n",
      "Epoch %d: train loss %f 143 0.3212774775244973\n",
      "Epoch 143: val loss 0.682368\n",
      "\n",
      "Epoch %d: train loss %f 144 0.34353548559275543\n",
      "Epoch 144: val loss 0.658227\n",
      "\n",
      "Epoch %d: train loss %f 145 0.3115414489399303\n",
      "Epoch 145: val loss 0.637494\n",
      "\n",
      "Epoch %d: train loss %f 146 0.33003377643498505\n",
      "Epoch 146: val loss 0.661753\n",
      "\n",
      "Epoch %d: train loss %f 147 0.30381390994245355\n",
      "Epoch 147: val loss 0.626646\n",
      "\n",
      "Epoch %d: train loss %f 148 0.2745354311032729\n",
      "Epoch 148: val loss 0.621834\n",
      "\n",
      "Epoch %d: train loss %f 149 0.29652001370083203\n",
      "Epoch 149: val loss 0.665768\n",
      "\n",
      "Epoch %d: train loss %f 150 0.29484550519423053\n",
      "Epoch 150: val loss 0.684569\n",
      "\n",
      "Epoch %d: train loss %f 151 0.31354021348736505\n",
      "Epoch 151: val loss 0.675939\n",
      "\n",
      "Epoch %d: train loss %f 152 0.33564966781572864\n",
      "Epoch 152: val loss 0.649535\n",
      "\n",
      "Epoch %d: train loss %f 153 0.2951599522070451\n",
      "Epoch 153: val loss 0.673101\n",
      "\n",
      "Epoch %d: train loss %f 154 0.3399342637170445\n",
      "Epoch 154: val loss 0.663944\n",
      "\n",
      "Epoch %d: train loss %f 155 0.3123462037606673\n",
      "Epoch 155: val loss 0.702705\n",
      "\n",
      "Epoch %d: train loss %f 156 0.34097008271650836\n",
      "Epoch 156: val loss 0.705531\n",
      "\n",
      "Epoch %d: train loss %f 157 0.31726861000061035\n",
      "Epoch 157: val loss 0.670036\n",
      "\n",
      "Epoch %d: train loss %f 158 0.2996730696071278\n",
      "Epoch 158: val loss 0.694789\n",
      "\n",
      "Epoch %d: train loss %f 159 0.3158482773737474\n",
      "Epoch 159: val loss 0.695185\n",
      "\n",
      "Epoch %d: train loss %f 160 0.30326223237947986\n",
      "Epoch 160: val loss 0.661338\n",
      "\n",
      "Epoch %d: train loss %f 161 0.3405274626883594\n",
      "Epoch 161: val loss 0.657098\n",
      "\n",
      "Epoch %d: train loss %f 162 0.33129529654979706\n",
      "Epoch 162: val loss 0.688595\n",
      "\n",
      "Epoch %d: train loss %f 163 0.28699202835559845\n",
      "Epoch 163: val loss 0.667511\n",
      "\n",
      "Epoch %d: train loss %f 164 0.28058314323425293\n",
      "Epoch 164: val loss 0.652833\n",
      "\n",
      "Epoch %d: train loss %f 165 0.33668390864675696\n",
      "Epoch 165: val loss 0.697855\n",
      "\n",
      "Epoch %d: train loss %f 166 0.30658425661650573\n",
      "Epoch 166: val loss 0.681240\n",
      "\n",
      "Epoch %d: train loss %f 167 0.265217010947791\n",
      "Epoch 167: val loss 0.694399\n",
      "\n",
      "Epoch %d: train loss %f 168 0.29501710019328375\n",
      "Epoch 168: val loss 0.649846\n",
      "\n",
      "Epoch %d: train loss %f 169 0.27764987674626435\n",
      "Epoch 169: val loss 0.670663\n",
      "\n",
      "Epoch %d: train loss %f 170 0.32692500948905945\n",
      "Epoch 170: val loss 0.669255\n",
      "\n",
      "Epoch %d: train loss %f 171 0.310932301662185\n",
      "Epoch 171: val loss 0.702118\n",
      "\n",
      "Epoch %d: train loss %f 172 0.2799649943004955\n",
      "Epoch 172: val loss 0.663577\n",
      "\n",
      "Epoch %d: train loss %f 173 0.30537245354869147\n",
      "Epoch 173: val loss 0.683233\n",
      "\n",
      "Epoch %d: train loss %f 174 0.26629482345147565\n",
      "Epoch 174: val loss 0.688815\n",
      "\n",
      "Epoch %d: train loss %f 175 0.30535764585841785\n",
      "Epoch 175: val loss 0.708094\n",
      "\n",
      "Epoch %d: train loss %f 176 0.28343489224260504\n",
      "Epoch 176: val loss 0.716641\n",
      "\n",
      "Epoch %d: train loss %f 177 0.3047280799258839\n",
      "Epoch 177: val loss 0.677173\n",
      "\n",
      "Epoch %d: train loss %f 178 0.2886127152226188\n",
      "Epoch 178: val loss 0.654066\n",
      "\n",
      "Epoch %d: train loss %f 179 0.2707218040119518\n",
      "Epoch 179: val loss 0.682957\n",
      "\n",
      "Epoch %d: train loss %f 180 0.2559158341451125\n",
      "Epoch 180: val loss 0.709881\n",
      "\n",
      "Epoch %d: train loss %f 181 0.2577765475619923\n",
      "Epoch 181: val loss 0.703183\n",
      "\n",
      "Epoch %d: train loss %f 182 0.3116454685276205\n",
      "Epoch 182: val loss 0.705724\n",
      "\n",
      "Epoch %d: train loss %f 183 0.28582394935868005\n",
      "Epoch 183: val loss 0.688977\n",
      "\n",
      "Epoch %d: train loss %f 184 0.3176817934621464\n",
      "Epoch 184: val loss 0.685646\n",
      "\n",
      "Epoch %d: train loss %f 185 0.2581391232934865\n",
      "Epoch 185: val loss 0.642531\n",
      "\n",
      "Epoch %d: train loss %f 186 0.30172568695111707\n",
      "Epoch 186: val loss 0.684850\n",
      "\n",
      "Epoch %d: train loss %f 187 0.28440404344688763\n",
      "Epoch 187: val loss 0.712293\n",
      "\n",
      "Epoch %d: train loss %f 188 0.2727224691347642\n",
      "Epoch 188: val loss 0.690568\n",
      "\n",
      "Epoch %d: train loss %f 189 0.29827253181825986\n",
      "Epoch 189: val loss 0.702674\n",
      "\n",
      "Epoch %d: train loss %f 190 0.2823342965407805\n",
      "Epoch 190: val loss 0.739597\n",
      "\n",
      "Epoch %d: train loss %f 191 0.27794167128476227\n",
      "Epoch 191: val loss 0.725290\n",
      "\n",
      "Epoch %d: train loss %f 192 0.27048811451955274\n",
      "Epoch 192: val loss 0.692589\n",
      "\n",
      "Epoch %d: train loss %f 193 0.24263313548131424\n",
      "Epoch 193: val loss 0.683461\n",
      "\n",
      "Epoch %d: train loss %f 194 0.26735780049454083\n",
      "Epoch 194: val loss 0.705277\n",
      "\n",
      "Epoch %d: train loss %f 195 0.28082744032144547\n",
      "Epoch 195: val loss 0.701810\n",
      "\n",
      "Epoch %d: train loss %f 196 0.29850065911358054\n",
      "Epoch 196: val loss 0.709129\n",
      "\n",
      "Epoch %d: train loss %f 197 0.28033757548440585\n",
      "Epoch 197: val loss 0.705017\n",
      "\n",
      "Epoch %d: train loss %f 198 0.27676479518413544\n",
      "Epoch 198: val loss 0.707100\n",
      "\n",
      "Epoch %d: train loss %f 199 0.29394889961589465\n",
      "Epoch 199: val loss 0.738525\n",
      "\n",
      "Epoch %d: train loss %f 0 0.684606438333338\n",
      "Epoch 0: val loss 0.683783\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6780831759626215\n",
      "Epoch 1: val loss 0.680099\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6720009825446389\n",
      "Epoch 2: val loss 0.672749\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6582101366736672\n",
      "Epoch 3: val loss 0.657884\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6454250053925947\n",
      "Epoch 4: val loss 0.636362\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6169559251178395\n",
      "Epoch 5: val loss 0.618056\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6081757653843273\n",
      "Epoch 6: val loss 0.592544\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5993898402560841\n",
      "Epoch 7: val loss 0.602860\n",
      "\n",
      "Epoch %d: train loss %f 8 0.590938085859472\n",
      "Epoch 8: val loss 0.612244\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5851000113920732\n",
      "Epoch 9: val loss 0.584263\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5757496709173376\n",
      "Epoch 10: val loss 0.594823\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5726882517337799\n",
      "Epoch 11: val loss 0.591679\n",
      "\n",
      "Epoch %d: train loss %f 12 0.569795551625165\n",
      "Epoch 12: val loss 0.589110\n",
      "\n",
      "Epoch %d: train loss %f 13 0.549207023598931\n",
      "Epoch 13: val loss 0.615087\n",
      "\n",
      "Epoch %d: train loss %f 14 0.55794281038371\n",
      "Epoch 14: val loss 0.593869\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5511875613169237\n",
      "Epoch 15: val loss 0.571135\n",
      "\n",
      "Epoch %d: train loss %f 16 0.533897806297649\n",
      "Epoch 16: val loss 0.580528\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5335267131978815\n",
      "Epoch 17: val loss 0.589614\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5148311040618203\n",
      "Epoch 18: val loss 0.560854\n",
      "\n",
      "Epoch %d: train loss %f 19 0.515957463871349\n",
      "Epoch 19: val loss 0.576126\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5283780152147467\n",
      "Epoch 20: val loss 0.596726\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5017390847206116\n",
      "Epoch 21: val loss 0.567167\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5128276456486095\n",
      "Epoch 22: val loss 0.570246\n",
      "\n",
      "Epoch %d: train loss %f 23 0.48574884100393817\n",
      "Epoch 23: val loss 0.567065\n",
      "\n",
      "Epoch %d: train loss %f 24 0.48088796030391345\n",
      "Epoch 24: val loss 0.569670\n",
      "\n",
      "Epoch %d: train loss %f 25 0.49022261933846906\n",
      "Epoch 25: val loss 0.583813\n",
      "\n",
      "Epoch %d: train loss %f 26 0.5034349994225935\n",
      "Epoch 26: val loss 0.573778\n",
      "\n",
      "Epoch %d: train loss %f 27 0.484955833716826\n",
      "Epoch 27: val loss 0.589666\n",
      "\n",
      "Epoch %d: train loss %f 28 0.46153500947085296\n",
      "Epoch 28: val loss 0.578211\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4839476617899808\n",
      "Epoch 29: val loss 0.575152\n",
      "\n",
      "Epoch %d: train loss %f 30 0.4725437706167048\n",
      "Epoch 30: val loss 0.583443\n",
      "\n",
      "Epoch %d: train loss %f 31 0.47239151326092804\n",
      "Epoch 31: val loss 0.577798\n",
      "\n",
      "Epoch %d: train loss %f 32 0.45815014839172363\n",
      "Epoch 32: val loss 0.595997\n",
      "\n",
      "Epoch %d: train loss %f 33 0.43713670156218787\n",
      "Epoch 33: val loss 0.580565\n",
      "\n",
      "Epoch %d: train loss %f 34 0.4615736549550837\n",
      "Epoch 34: val loss 0.595112\n",
      "\n",
      "Epoch %d: train loss %f 35 0.42449968511408026\n",
      "Epoch 35: val loss 0.601791\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4622822701931\n",
      "Epoch 36: val loss 0.605081\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4368340345946225\n",
      "Epoch 37: val loss 0.600854\n",
      "\n",
      "Epoch %d: train loss %f 38 0.42547344077717175\n",
      "Epoch 38: val loss 0.588988\n",
      "\n",
      "Epoch %d: train loss %f 39 0.40618895400654187\n",
      "Epoch 39: val loss 0.591240\n",
      "\n",
      "Epoch %d: train loss %f 40 0.45375517823479394\n",
      "Epoch 40: val loss 0.604211\n",
      "\n",
      "Epoch %d: train loss %f 41 0.46673461252992804\n",
      "Epoch 41: val loss 0.607311\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4214802912690423\n",
      "Epoch 42: val loss 0.622792\n",
      "\n",
      "Epoch %d: train loss %f 43 0.41783054850318213\n",
      "Epoch 43: val loss 0.604625\n",
      "\n",
      "Epoch %d: train loss %f 44 0.41127961061217566\n",
      "Epoch 44: val loss 0.606618\n",
      "\n",
      "Epoch %d: train loss %f 45 0.42557454109191895\n",
      "Epoch 45: val loss 0.603247\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4432609298012473\n",
      "Epoch 46: val loss 0.604827\n",
      "\n",
      "Epoch %d: train loss %f 47 0.4020685689015822\n",
      "Epoch 47: val loss 0.584617\n",
      "\n",
      "Epoch %d: train loss %f 48 0.4274723069234328\n",
      "Epoch 48: val loss 0.627894\n",
      "\n",
      "Epoch %d: train loss %f 49 0.401906587860801\n",
      "Epoch 49: val loss 0.606164\n",
      "\n",
      "Epoch %d: train loss %f 50 0.4239794639023868\n",
      "Epoch 50: val loss 0.602396\n",
      "\n",
      "Epoch %d: train loss %f 51 0.39055125550790265\n",
      "Epoch 51: val loss 0.610630\n",
      "\n",
      "Epoch %d: train loss %f 52 0.4016690240664916\n",
      "Epoch 52: val loss 0.610027\n",
      "\n",
      "Epoch %d: train loss %f 53 0.43523004109209235\n",
      "Epoch 53: val loss 0.627457\n",
      "\n",
      "Epoch %d: train loss %f 54 0.3816593722863631\n",
      "Epoch 54: val loss 0.625432\n",
      "\n",
      "Epoch %d: train loss %f 55 0.4234428121285005\n",
      "Epoch 55: val loss 0.637625\n",
      "\n",
      "Epoch %d: train loss %f 56 0.38292546434835956\n",
      "Epoch 56: val loss 0.627159\n",
      "\n",
      "Epoch %d: train loss %f 57 0.38375050913203845\n",
      "Epoch 57: val loss 0.616624\n",
      "\n",
      "Epoch %d: train loss %f 58 0.3958432105454532\n",
      "Epoch 58: val loss 0.611967\n",
      "\n",
      "Epoch %d: train loss %f 59 0.37714392759583215\n",
      "Epoch 59: val loss 0.614608\n",
      "\n",
      "Epoch %d: train loss %f 60 0.39057349616831\n",
      "Epoch 60: val loss 0.627435\n",
      "\n",
      "Epoch %d: train loss %f 61 0.39336751807819714\n",
      "Epoch 61: val loss 0.629058\n",
      "\n",
      "Epoch %d: train loss %f 62 0.38837570493871515\n",
      "Epoch 62: val loss 0.613751\n",
      "\n",
      "Epoch %d: train loss %f 63 0.3830802250992168\n",
      "Epoch 63: val loss 0.619998\n",
      "\n",
      "Epoch %d: train loss %f 64 0.35944822295145556\n",
      "Epoch 64: val loss 0.633525\n",
      "\n",
      "Epoch %d: train loss %f 65 0.35128536278551276\n",
      "Epoch 65: val loss 0.623085\n",
      "\n",
      "Epoch %d: train loss %f 66 0.37509574537927454\n",
      "Epoch 66: val loss 0.639782\n",
      "\n",
      "Epoch %d: train loss %f 67 0.3509120832790028\n",
      "Epoch 67: val loss 0.647445\n",
      "\n",
      "Epoch %d: train loss %f 68 0.36626956544139166\n",
      "Epoch 68: val loss 0.628364\n",
      "\n",
      "Epoch %d: train loss %f 69 0.3688256496732885\n",
      "Epoch 69: val loss 0.614602\n",
      "\n",
      "Epoch %d: train loss %f 70 0.3361383215947585\n",
      "Epoch 70: val loss 0.625051\n",
      "\n",
      "Epoch %d: train loss %f 71 0.36512549356980756\n",
      "Epoch 71: val loss 0.641012\n",
      "\n",
      "Epoch %d: train loss %f 72 0.3693629665808244\n",
      "Epoch 72: val loss 0.653169\n",
      "\n",
      "Epoch %d: train loss %f 73 0.35987750508568506\n",
      "Epoch 73: val loss 0.666979\n",
      "\n",
      "Epoch %d: train loss %f 74 0.3525679097934203\n",
      "Epoch 74: val loss 0.652651\n",
      "\n",
      "Epoch %d: train loss %f 75 0.33457851680842315\n",
      "Epoch 75: val loss 0.672162\n",
      "\n",
      "Epoch %d: train loss %f 76 0.36624558676372876\n",
      "Epoch 76: val loss 0.658789\n",
      "\n",
      "Epoch %d: train loss %f 77 0.3247164257548072\n",
      "Epoch 77: val loss 0.672597\n",
      "\n",
      "Epoch %d: train loss %f 78 0.3596606078473004\n",
      "Epoch 78: val loss 0.659574\n",
      "\n",
      "Epoch %d: train loss %f 79 0.355659391392361\n",
      "Epoch 79: val loss 0.651398\n",
      "\n",
      "Epoch %d: train loss %f 80 0.37216577475721185\n",
      "Epoch 80: val loss 0.665722\n",
      "\n",
      "Epoch %d: train loss %f 81 0.34321861646392127\n",
      "Epoch 81: val loss 0.646602\n",
      "\n",
      "Epoch %d: train loss %f 82 0.32581623711369256\n",
      "Epoch 82: val loss 0.646021\n",
      "\n",
      "Epoch %d: train loss %f 83 0.33967298811132257\n",
      "Epoch 83: val loss 0.638304\n",
      "\n",
      "Epoch %d: train loss %f 84 0.34053350714119995\n",
      "Epoch 84: val loss 0.631256\n",
      "\n",
      "Epoch %d: train loss %f 85 0.36660521951588715\n",
      "Epoch 85: val loss 0.629959\n",
      "\n",
      "Epoch %d: train loss %f 86 0.33908597041260113\n",
      "Epoch 86: val loss 0.640409\n",
      "\n",
      "Epoch %d: train loss %f 87 0.3424453640525991\n",
      "Epoch 87: val loss 0.645352\n",
      "\n",
      "Epoch %d: train loss %f 88 0.32261244275353174\n",
      "Epoch 88: val loss 0.645410\n",
      "\n",
      "Epoch %d: train loss %f 89 0.33270215446298773\n",
      "Epoch 89: val loss 0.642780\n",
      "\n",
      "Epoch %d: train loss %f 90 0.34179572625593707\n",
      "Epoch 90: val loss 0.633291\n",
      "\n",
      "Epoch %d: train loss %f 91 0.31072859330610797\n",
      "Epoch 91: val loss 0.662343\n",
      "\n",
      "Epoch %d: train loss %f 92 0.32309767739339307\n",
      "Epoch 92: val loss 0.674266\n",
      "\n",
      "Epoch %d: train loss %f 93 0.31162544136697595\n",
      "Epoch 93: val loss 0.669624\n",
      "\n",
      "Epoch %d: train loss %f 94 0.31922511891885236\n",
      "Epoch 94: val loss 0.655969\n",
      "\n",
      "Epoch %d: train loss %f 95 0.3166248554533178\n",
      "Epoch 95: val loss 0.694558\n",
      "\n",
      "Epoch %d: train loss %f 96 0.29988400502638385\n",
      "Epoch 96: val loss 0.683676\n",
      "\n",
      "Epoch %d: train loss %f 97 0.30533597008748486\n",
      "Epoch 97: val loss 0.699918\n",
      "\n",
      "Epoch %d: train loss %f 98 0.33524643968452106\n",
      "Epoch 98: val loss 0.668170\n",
      "\n",
      "Epoch %d: train loss %f 99 0.29525079374963586\n",
      "Epoch 99: val loss 0.661126\n",
      "\n",
      "Epoch %d: train loss %f 100 0.2901281727985902\n",
      "Epoch 100: val loss 0.674164\n",
      "\n",
      "Epoch %d: train loss %f 101 0.3146877654574134\n",
      "Epoch 101: val loss 0.675210\n",
      "\n",
      "Epoch %d: train loss %f 102 0.3089351098645817\n",
      "Epoch 102: val loss 0.698490\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3170050084590912\n",
      "Epoch 103: val loss 0.675341\n",
      "\n",
      "Epoch %d: train loss %f 104 0.3259872265837409\n",
      "Epoch 104: val loss 0.711152\n",
      "\n",
      "Epoch %d: train loss %f 105 0.31678593971512536\n",
      "Epoch 105: val loss 0.705766\n",
      "\n",
      "Epoch %d: train loss %f 106 0.27231748131188477\n",
      "Epoch 106: val loss 0.706164\n",
      "\n",
      "Epoch %d: train loss %f 107 0.293915945020589\n",
      "Epoch 107: val loss 0.719727\n",
      "\n",
      "Epoch %d: train loss %f 108 0.28748290511694824\n",
      "Epoch 108: val loss 0.742086\n",
      "\n",
      "Epoch %d: train loss %f 109 0.3000582158565521\n",
      "Epoch 109: val loss 0.698479\n",
      "\n",
      "Epoch %d: train loss %f 110 0.31118855015798047\n",
      "Epoch 110: val loss 0.695862\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3108789853074334\n",
      "Epoch 111: val loss 0.725541\n",
      "\n",
      "Epoch %d: train loss %f 112 0.2902982776815241\n",
      "Epoch 112: val loss 0.700261\n",
      "\n",
      "Epoch %d: train loss %f 113 0.31149839000268414\n",
      "Epoch 113: val loss 0.754018\n",
      "\n",
      "Epoch %d: train loss %f 114 0.2910204394297166\n",
      "Epoch 114: val loss 0.731177\n",
      "\n",
      "Epoch %d: train loss %f 115 0.3116056038574739\n",
      "Epoch 115: val loss 0.720783\n",
      "\n",
      "Epoch %d: train loss %f 116 0.3017580116336996\n",
      "Epoch 116: val loss 0.736731\n",
      "\n",
      "Epoch %d: train loss %f 117 0.24340445480563425\n",
      "Epoch 117: val loss 0.731466\n",
      "\n",
      "Epoch %d: train loss %f 118 0.23799073696136475\n",
      "Epoch 118: val loss 0.717591\n",
      "\n",
      "Epoch %d: train loss %f 119 0.2600015808235515\n",
      "Epoch 119: val loss 0.745139\n",
      "\n",
      "Epoch %d: train loss %f 120 0.2569660957564007\n",
      "Epoch 120: val loss 0.749965\n",
      "\n",
      "Epoch %d: train loss %f 121 0.2931562892415307\n",
      "Epoch 121: val loss 0.760018\n",
      "\n",
      "Epoch %d: train loss %f 122 0.2891537289727818\n",
      "Epoch 122: val loss 0.753625\n",
      "\n",
      "Epoch %d: train loss %f 123 0.28405085883357306\n",
      "Epoch 123: val loss 0.744656\n",
      "\n",
      "Epoch %d: train loss %f 124 0.26342744447968225\n",
      "Epoch 124: val loss 0.747164\n",
      "\n",
      "Epoch %d: train loss %f 125 0.268603962930766\n",
      "Epoch 125: val loss 0.747394\n",
      "\n",
      "Epoch %d: train loss %f 126 0.31171337447383185\n",
      "Epoch 126: val loss 0.746409\n",
      "\n",
      "Epoch %d: train loss %f 127 0.2666032842614434\n",
      "Epoch 127: val loss 0.715242\n",
      "\n",
      "Epoch %d: train loss %f 128 0.29749214378270233\n",
      "Epoch 128: val loss 0.723966\n",
      "\n",
      "Epoch %d: train loss %f 129 0.2410988834771243\n",
      "Epoch 129: val loss 0.779703\n",
      "\n",
      "Epoch %d: train loss %f 130 0.2740584137764844\n",
      "Epoch 130: val loss 0.765744\n",
      "\n",
      "Epoch %d: train loss %f 131 0.2750651931220835\n",
      "Epoch 131: val loss 0.773673\n",
      "\n",
      "Epoch %d: train loss %f 132 0.23686417801813645\n",
      "Epoch 132: val loss 0.755871\n",
      "\n",
      "Epoch %d: train loss %f 133 0.27495459602637723\n",
      "Epoch 133: val loss 0.751907\n",
      "\n",
      "Epoch %d: train loss %f 134 0.28654618561267853\n",
      "Epoch 134: val loss 0.762884\n",
      "\n",
      "Epoch %d: train loss %f 135 0.24944752861152997\n",
      "Epoch 135: val loss 0.743346\n",
      "\n",
      "Epoch %d: train loss %f 136 0.2765420241789384\n",
      "Epoch 136: val loss 0.778642\n",
      "\n",
      "Epoch %d: train loss %f 137 0.24930717999284918\n",
      "Epoch 137: val loss 0.762971\n",
      "\n",
      "Epoch %d: train loss %f 138 0.31306094202128326\n",
      "Epoch 138: val loss 0.766225\n",
      "\n",
      "Epoch %d: train loss %f 139 0.26532701606100256\n",
      "Epoch 139: val loss 0.772111\n",
      "\n",
      "Epoch %d: train loss %f 140 0.28407685323195025\n",
      "Epoch 140: val loss 0.774258\n",
      "\n",
      "Epoch %d: train loss %f 141 0.28765682740644977\n",
      "Epoch 141: val loss 0.781267\n",
      "\n",
      "Epoch %d: train loss %f 142 0.24064191363074564\n",
      "Epoch 142: val loss 0.799657\n",
      "\n",
      "Epoch %d: train loss %f 143 0.2981684465299953\n",
      "Epoch 143: val loss 0.771643\n",
      "\n",
      "Epoch %d: train loss %f 144 0.24410691586407748\n",
      "Epoch 144: val loss 0.786456\n",
      "\n",
      "Epoch %d: train loss %f 145 0.23824101551012558\n",
      "Epoch 145: val loss 0.799977\n",
      "\n",
      "Epoch %d: train loss %f 146 0.2540839111263102\n",
      "Epoch 146: val loss 0.815118\n",
      "\n",
      "Epoch %d: train loss %f 147 0.28542808375575324\n",
      "Epoch 147: val loss 0.775267\n",
      "\n",
      "Epoch %d: train loss %f 148 0.26445747911930084\n",
      "Epoch 148: val loss 0.728243\n",
      "\n",
      "Epoch %d: train loss %f 149 0.24530539864843542\n",
      "Epoch 149: val loss 0.766374\n",
      "\n",
      "Epoch %d: train loss %f 150 0.24415018612688238\n",
      "Epoch 150: val loss 0.800719\n",
      "\n",
      "Epoch %d: train loss %f 151 0.2823786207220771\n",
      "Epoch 151: val loss 0.806866\n",
      "\n",
      "Epoch %d: train loss %f 152 0.2725555923852054\n",
      "Epoch 152: val loss 0.797144\n",
      "\n",
      "Epoch %d: train loss %f 153 0.22353629361499439\n",
      "Epoch 153: val loss 0.816702\n",
      "\n",
      "Epoch %d: train loss %f 154 0.24693830040368167\n",
      "Epoch 154: val loss 0.784191\n",
      "\n",
      "Epoch %d: train loss %f 155 0.2517344585873864\n",
      "Epoch 155: val loss 0.789279\n",
      "\n",
      "Epoch %d: train loss %f 156 0.2554205330935391\n",
      "Epoch 156: val loss 0.786460\n",
      "\n",
      "Epoch %d: train loss %f 157 0.2497042105956511\n",
      "Epoch 157: val loss 0.768618\n",
      "\n",
      "Epoch %d: train loss %f 158 0.2551142248240384\n",
      "Epoch 158: val loss 0.841136\n",
      "\n",
      "Epoch %d: train loss %f 159 0.2673896693370559\n",
      "Epoch 159: val loss 0.819621\n",
      "\n",
      "Epoch %d: train loss %f 160 0.26337760348211636\n",
      "Epoch 160: val loss 0.782568\n",
      "\n",
      "Epoch %d: train loss %f 161 0.26415418630296533\n",
      "Epoch 161: val loss 0.794111\n",
      "\n",
      "Epoch %d: train loss %f 162 0.26407175714319403\n",
      "Epoch 162: val loss 0.803678\n",
      "\n",
      "Epoch %d: train loss %f 163 0.25884515995329077\n",
      "Epoch 163: val loss 0.800424\n",
      "\n",
      "Epoch %d: train loss %f 164 0.25186787274750794\n",
      "Epoch 164: val loss 0.795300\n",
      "\n",
      "Epoch %d: train loss %f 165 0.23592871088873257\n",
      "Epoch 165: val loss 0.773866\n",
      "\n",
      "Epoch %d: train loss %f 166 0.2540356930006634\n",
      "Epoch 166: val loss 0.793696\n",
      "\n",
      "Epoch %d: train loss %f 167 0.262862504883246\n",
      "Epoch 167: val loss 0.833230\n",
      "\n",
      "Epoch %d: train loss %f 168 0.24471938068216498\n",
      "Epoch 168: val loss 0.809643\n",
      "\n",
      "Epoch %d: train loss %f 169 0.21825178103013473\n",
      "Epoch 169: val loss 0.818462\n",
      "\n",
      "Epoch %d: train loss %f 170 0.27779959819533606\n",
      "Epoch 170: val loss 0.759280\n",
      "\n",
      "Epoch %d: train loss %f 171 0.2616766786033457\n",
      "Epoch 171: val loss 0.820438\n",
      "\n",
      "Epoch %d: train loss %f 172 0.2648275745185939\n",
      "Epoch 172: val loss 0.835019\n",
      "\n",
      "Epoch %d: train loss %f 173 0.2700334313240918\n",
      "Epoch 173: val loss 0.829776\n",
      "\n",
      "Epoch %d: train loss %f 174 0.27067983421412384\n",
      "Epoch 174: val loss 0.749222\n",
      "\n",
      "Epoch %d: train loss %f 175 0.2414878335866061\n",
      "Epoch 175: val loss 0.767056\n",
      "\n",
      "Epoch %d: train loss %f 176 0.2295011281967163\n",
      "Epoch 176: val loss 0.840351\n",
      "\n",
      "Epoch %d: train loss %f 177 0.24837192351167853\n",
      "Epoch 177: val loss 0.827200\n",
      "\n",
      "Epoch %d: train loss %f 178 0.3109622584147887\n",
      "Epoch 178: val loss 0.829402\n",
      "\n",
      "Epoch %d: train loss %f 179 0.2189363261515444\n",
      "Epoch 179: val loss 0.825195\n",
      "\n",
      "Epoch %d: train loss %f 180 0.2621805315667933\n",
      "Epoch 180: val loss 0.825942\n",
      "\n",
      "Epoch %d: train loss %f 181 0.2848502140153538\n",
      "Epoch 181: val loss 0.830062\n",
      "\n",
      "Epoch %d: train loss %f 182 0.25461498173800384\n",
      "Epoch 182: val loss 0.792610\n",
      "\n",
      "Epoch %d: train loss %f 183 0.2060624977404421\n",
      "Epoch 183: val loss 0.788092\n",
      "\n",
      "Epoch %d: train loss %f 184 0.2344890981912613\n",
      "Epoch 184: val loss 0.786374\n",
      "\n",
      "Epoch %d: train loss %f 185 0.25645088201219385\n",
      "Epoch 185: val loss 0.815880\n",
      "\n",
      "Epoch %d: train loss %f 186 0.24657899683172052\n",
      "Epoch 186: val loss 0.791679\n",
      "\n",
      "Epoch %d: train loss %f 187 0.24865636094049973\n",
      "Epoch 187: val loss 0.806004\n",
      "\n",
      "Epoch %d: train loss %f 188 0.2849461504004218\n",
      "Epoch 188: val loss 0.783503\n",
      "\n",
      "Epoch %d: train loss %f 189 0.21803866530006583\n",
      "Epoch 189: val loss 0.781396\n",
      "\n",
      "Epoch %d: train loss %f 190 0.21572100032459607\n",
      "Epoch 190: val loss 0.796740\n",
      "\n",
      "Epoch %d: train loss %f 191 0.24973564594984055\n",
      "Epoch 191: val loss 0.819128\n",
      "\n",
      "Epoch %d: train loss %f 192 0.22638330947269092\n",
      "Epoch 192: val loss 0.794514\n",
      "\n",
      "Epoch %d: train loss %f 193 0.24222605133598502\n",
      "Epoch 193: val loss 0.799100\n",
      "\n",
      "Epoch %d: train loss %f 194 0.20105377516963266\n",
      "Epoch 194: val loss 0.807119\n",
      "\n",
      "Epoch %d: train loss %f 195 0.24851943281563846\n",
      "Epoch 195: val loss 0.808171\n",
      "\n",
      "Epoch %d: train loss %f 196 0.27359820021824405\n",
      "Epoch 196: val loss 0.823495\n",
      "\n",
      "Epoch %d: train loss %f 197 0.20160365917465903\n",
      "Epoch 197: val loss 0.837057\n",
      "\n",
      "Epoch %d: train loss %f 198 0.24425394155762412\n",
      "Epoch 198: val loss 0.818104\n",
      "\n",
      "Epoch %d: train loss %f 199 0.2256038419224999\n",
      "Epoch 199: val loss 0.828730\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6949414451917012\n",
      "Epoch 0: val loss 0.695211\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6831430951754253\n",
      "Epoch 1: val loss 0.685255\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6667452732721965\n",
      "Epoch 2: val loss 0.661653\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6406344493230184\n",
      "Epoch 3: val loss 0.624970\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6134632110595704\n",
      "Epoch 4: val loss 0.603811\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6087679465611776\n",
      "Epoch 5: val loss 0.580656\n",
      "\n",
      "Epoch %d: train loss %f 6 0.5822527448336283\n",
      "Epoch 6: val loss 0.586133\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5822072406609853\n",
      "Epoch 7: val loss 0.565686\n",
      "\n",
      "Epoch %d: train loss %f 8 0.578600287437439\n",
      "Epoch 8: val loss 0.553188\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5633802731831868\n",
      "Epoch 9: val loss 0.544995\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5505444725354512\n",
      "Epoch 10: val loss 0.551962\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5578238745530446\n",
      "Epoch 11: val loss 0.540840\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5456181466579437\n",
      "Epoch 12: val loss 0.541145\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5394077082475026\n",
      "Epoch 13: val loss 0.533716\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5494119127591451\n",
      "Epoch 14: val loss 0.543058\n",
      "\n",
      "Epoch %d: train loss %f 15 0.52221373518308\n",
      "Epoch 15: val loss 0.528634\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5382123788197836\n",
      "Epoch 16: val loss 0.534327\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5287304878234863\n",
      "Epoch 17: val loss 0.522847\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5056363801161449\n",
      "Epoch 18: val loss 0.530095\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5208535552024841\n",
      "Epoch 19: val loss 0.521853\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5100420951843262\n",
      "Epoch 20: val loss 0.524266\n",
      "\n",
      "Epoch %d: train loss %f 21 0.4984815359115601\n",
      "Epoch 21: val loss 0.528421\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5182420750459035\n",
      "Epoch 22: val loss 0.525152\n",
      "\n",
      "Epoch %d: train loss %f 23 0.480171932776769\n",
      "Epoch 23: val loss 0.505763\n",
      "\n",
      "Epoch %d: train loss %f 24 0.5026402095953624\n",
      "Epoch 24: val loss 0.510210\n",
      "\n",
      "Epoch %d: train loss %f 25 0.5147977451483409\n",
      "Epoch 25: val loss 0.518586\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4900506317615509\n",
      "Epoch 26: val loss 0.518511\n",
      "\n",
      "Epoch %d: train loss %f 27 0.48632925351460776\n",
      "Epoch 27: val loss 0.514547\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4870883027712504\n",
      "Epoch 28: val loss 0.515310\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4825083295504252\n",
      "Epoch 29: val loss 0.514674\n",
      "\n",
      "Epoch %d: train loss %f 30 0.4891749322414398\n",
      "Epoch 30: val loss 0.506058\n",
      "\n",
      "Epoch %d: train loss %f 31 0.502859338124593\n",
      "Epoch 31: val loss 0.504208\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4812754909197489\n",
      "Epoch 32: val loss 0.519554\n",
      "\n",
      "Epoch %d: train loss %f 33 0.4594550410906474\n",
      "Epoch 33: val loss 0.511863\n",
      "\n",
      "Epoch %d: train loss %f 34 0.4807998816172282\n",
      "Epoch 34: val loss 0.498819\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4654316842556\n",
      "Epoch 35: val loss 0.500700\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4761674483617147\n",
      "Epoch 36: val loss 0.502829\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4657098650932312\n",
      "Epoch 37: val loss 0.507895\n",
      "\n",
      "Epoch %d: train loss %f 38 0.4777077694733938\n",
      "Epoch 38: val loss 0.508790\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4929201861222585\n",
      "Epoch 39: val loss 0.502705\n",
      "\n",
      "Epoch %d: train loss %f 40 0.4430096626281738\n",
      "Epoch 40: val loss 0.513890\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4861124118169149\n",
      "Epoch 41: val loss 0.520344\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4545324722925822\n",
      "Epoch 42: val loss 0.510614\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4409181217352549\n",
      "Epoch 43: val loss 0.498174\n",
      "\n",
      "Epoch %d: train loss %f 44 0.4542237937450409\n",
      "Epoch 44: val loss 0.497178\n",
      "\n",
      "Epoch %d: train loss %f 45 0.47462671001752216\n",
      "Epoch 45: val loss 0.490989\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4376651108264923\n",
      "Epoch 46: val loss 0.510216\n",
      "\n",
      "Epoch %d: train loss %f 47 0.45615381797154747\n",
      "Epoch 47: val loss 0.488719\n",
      "\n",
      "Epoch %d: train loss %f 48 0.46308770775794983\n",
      "Epoch 48: val loss 0.503912\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4527164320151011\n",
      "Epoch 49: val loss 0.491504\n",
      "\n",
      "Epoch %d: train loss %f 50 0.46814828912417095\n",
      "Epoch 50: val loss 0.496543\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4524715820948283\n",
      "Epoch 51: val loss 0.501723\n",
      "\n",
      "Epoch %d: train loss %f 52 0.47082815766334535\n",
      "Epoch 52: val loss 0.500601\n",
      "\n",
      "Epoch %d: train loss %f 53 0.45764001210530597\n",
      "Epoch 53: val loss 0.484479\n",
      "\n",
      "Epoch %d: train loss %f 54 0.45248769323031107\n",
      "Epoch 54: val loss 0.496120\n",
      "\n",
      "Epoch %d: train loss %f 55 0.45792798002560936\n",
      "Epoch 55: val loss 0.512724\n",
      "\n",
      "Epoch %d: train loss %f 56 0.45123280088106793\n",
      "Epoch 56: val loss 0.504812\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4383281389872233\n",
      "Epoch 57: val loss 0.507949\n",
      "\n",
      "Epoch %d: train loss %f 58 0.44582571387290953\n",
      "Epoch 58: val loss 0.508866\n",
      "\n",
      "Epoch %d: train loss %f 59 0.4424382964769999\n",
      "Epoch 59: val loss 0.492820\n",
      "\n",
      "Epoch %d: train loss %f 60 0.41397777597109475\n",
      "Epoch 60: val loss 0.492471\n",
      "\n",
      "Epoch %d: train loss %f 61 0.42498431404431664\n",
      "Epoch 61: val loss 0.504809\n",
      "\n",
      "Epoch %d: train loss %f 62 0.44324349959691367\n",
      "Epoch 62: val loss 0.496184\n",
      "\n",
      "Epoch %d: train loss %f 63 0.43414344986279807\n",
      "Epoch 63: val loss 0.489882\n",
      "\n",
      "Epoch %d: train loss %f 64 0.4416133840878805\n",
      "Epoch 64: val loss 0.506532\n",
      "\n",
      "Epoch %d: train loss %f 65 0.47619696060816447\n",
      "Epoch 65: val loss 0.530752\n",
      "\n",
      "Epoch %d: train loss %f 66 0.4288713117440542\n",
      "Epoch 66: val loss 0.507909\n",
      "\n",
      "Epoch %d: train loss %f 67 0.41433348854382834\n",
      "Epoch 67: val loss 0.511623\n",
      "\n",
      "Epoch %d: train loss %f 68 0.4516511956850688\n",
      "Epoch 68: val loss 0.503049\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4224022150039673\n",
      "Epoch 69: val loss 0.494770\n",
      "\n",
      "Epoch %d: train loss %f 70 0.4189645826816559\n",
      "Epoch 70: val loss 0.492227\n",
      "\n",
      "Epoch %d: train loss %f 71 0.4283244093259176\n",
      "Epoch 71: val loss 0.497663\n",
      "\n",
      "Epoch %d: train loss %f 72 0.45585452218850453\n",
      "Epoch 72: val loss 0.489378\n",
      "\n",
      "Epoch %d: train loss %f 73 0.4192086319128672\n",
      "Epoch 73: val loss 0.489335\n",
      "\n",
      "Epoch %d: train loss %f 74 0.4536264588435491\n",
      "Epoch 74: val loss 0.476049\n",
      "\n",
      "Epoch %d: train loss %f 75 0.42887916366259254\n",
      "Epoch 75: val loss 0.478683\n",
      "\n",
      "Epoch %d: train loss %f 76 0.4297209958235423\n",
      "Epoch 76: val loss 0.480195\n",
      "\n",
      "Epoch %d: train loss %f 77 0.4219838241736094\n",
      "Epoch 77: val loss 0.482464\n",
      "\n",
      "Epoch %d: train loss %f 78 0.43479743401209514\n",
      "Epoch 78: val loss 0.483584\n",
      "\n",
      "Epoch %d: train loss %f 79 0.41814071337381997\n",
      "Epoch 79: val loss 0.497275\n",
      "\n",
      "Epoch %d: train loss %f 80 0.4060328920682271\n",
      "Epoch 80: val loss 0.495057\n",
      "\n",
      "Epoch %d: train loss %f 81 0.43055927952130635\n",
      "Epoch 81: val loss 0.481253\n",
      "\n",
      "Epoch %d: train loss %f 82 0.416646150747935\n",
      "Epoch 82: val loss 0.500955\n",
      "\n",
      "Epoch %d: train loss %f 83 0.4227121959129969\n",
      "Epoch 83: val loss 0.509609\n",
      "\n",
      "Epoch %d: train loss %f 84 0.4169222017129262\n",
      "Epoch 84: val loss 0.473183\n",
      "\n",
      "Epoch %d: train loss %f 85 0.4193972130616506\n",
      "Epoch 85: val loss 0.476070\n",
      "\n",
      "Epoch %d: train loss %f 86 0.4392742375532786\n",
      "Epoch 86: val loss 0.481587\n",
      "\n",
      "Epoch %d: train loss %f 87 0.39313363631566367\n",
      "Epoch 87: val loss 0.480169\n",
      "\n",
      "Epoch %d: train loss %f 88 0.4011577953894933\n",
      "Epoch 88: val loss 0.488408\n",
      "\n",
      "Epoch %d: train loss %f 89 0.40814463595549266\n",
      "Epoch 89: val loss 0.485098\n",
      "\n",
      "Epoch %d: train loss %f 90 0.38753583033879596\n",
      "Epoch 90: val loss 0.485039\n",
      "\n",
      "Epoch %d: train loss %f 91 0.402039560675621\n",
      "Epoch 91: val loss 0.498318\n",
      "\n",
      "Epoch %d: train loss %f 92 0.3848631272713343\n",
      "Epoch 92: val loss 0.494073\n",
      "\n",
      "Epoch %d: train loss %f 93 0.3840729941924413\n",
      "Epoch 93: val loss 0.491016\n",
      "\n",
      "Epoch %d: train loss %f 94 0.3867822617292404\n",
      "Epoch 94: val loss 0.495424\n",
      "\n",
      "Epoch %d: train loss %f 95 0.4060267289479574\n",
      "Epoch 95: val loss 0.495819\n",
      "\n",
      "Epoch %d: train loss %f 96 0.4010147253672282\n",
      "Epoch 96: val loss 0.500104\n",
      "\n",
      "Epoch %d: train loss %f 97 0.39810017347335813\n",
      "Epoch 97: val loss 0.488417\n",
      "\n",
      "Epoch %d: train loss %f 98 0.38678814669450123\n",
      "Epoch 98: val loss 0.489027\n",
      "\n",
      "Epoch %d: train loss %f 99 0.4051310439904531\n",
      "Epoch 99: val loss 0.523859\n",
      "\n",
      "Epoch %d: train loss %f 100 0.3793890118598938\n",
      "Epoch 100: val loss 0.495949\n",
      "\n",
      "Epoch %d: train loss %f 101 0.38307773172855375\n",
      "Epoch 101: val loss 0.494785\n",
      "\n",
      "Epoch %d: train loss %f 102 0.3818775882323583\n",
      "Epoch 102: val loss 0.498167\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3705946743488312\n",
      "Epoch 103: val loss 0.482379\n",
      "\n",
      "Epoch %d: train loss %f 104 0.41318100889523823\n",
      "Epoch 104: val loss 0.525476\n",
      "\n",
      "Epoch %d: train loss %f 105 0.37967559496561687\n",
      "Epoch 105: val loss 0.484605\n",
      "\n",
      "Epoch %d: train loss %f 106 0.3919850846131643\n",
      "Epoch 106: val loss 0.518235\n",
      "\n",
      "Epoch %d: train loss %f 107 0.38893122573693595\n",
      "Epoch 107: val loss 0.505601\n",
      "\n",
      "Epoch %d: train loss %f 108 0.37738005121548973\n",
      "Epoch 108: val loss 0.524444\n",
      "\n",
      "Epoch %d: train loss %f 109 0.3689893662929535\n",
      "Epoch 109: val loss 0.506606\n",
      "\n",
      "Epoch %d: train loss %f 110 0.36698068380355836\n",
      "Epoch 110: val loss 0.514532\n",
      "\n",
      "Epoch %d: train loss %f 111 0.40870618522167207\n",
      "Epoch 111: val loss 0.518791\n",
      "\n",
      "Epoch %d: train loss %f 112 0.4060856839021047\n",
      "Epoch 112: val loss 0.530234\n",
      "\n",
      "Epoch %d: train loss %f 113 0.38192667762438454\n",
      "Epoch 113: val loss 0.508772\n",
      "\n",
      "Epoch %d: train loss %f 114 0.363068030277888\n",
      "Epoch 114: val loss 0.508578\n",
      "\n",
      "Epoch %d: train loss %f 115 0.3509554654359818\n",
      "Epoch 115: val loss 0.524171\n",
      "\n",
      "Epoch %d: train loss %f 116 0.3634939690430959\n",
      "Epoch 116: val loss 0.509160\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3848991056283315\n",
      "Epoch 117: val loss 0.516224\n",
      "\n",
      "Epoch %d: train loss %f 118 0.3405247370402018\n",
      "Epoch 118: val loss 0.487985\n",
      "\n",
      "Epoch %d: train loss %f 119 0.36082879900932313\n",
      "Epoch 119: val loss 0.516788\n",
      "\n",
      "Epoch %d: train loss %f 120 0.368472013870875\n",
      "Epoch 120: val loss 0.521606\n",
      "\n",
      "Epoch %d: train loss %f 121 0.3896854857603709\n",
      "Epoch 121: val loss 0.513885\n",
      "\n",
      "Epoch %d: train loss %f 122 0.38741838137308754\n",
      "Epoch 122: val loss 0.516935\n",
      "\n",
      "Epoch %d: train loss %f 123 0.3991590549548467\n",
      "Epoch 123: val loss 0.520971\n",
      "\n",
      "Epoch %d: train loss %f 124 0.39707881212234497\n",
      "Epoch 124: val loss 0.508453\n",
      "\n",
      "Epoch %d: train loss %f 125 0.37812552054723103\n",
      "Epoch 125: val loss 0.527166\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3673974772294362\n",
      "Epoch 126: val loss 0.507019\n",
      "\n",
      "Epoch %d: train loss %f 127 0.36886593302090964\n",
      "Epoch 127: val loss 0.512924\n",
      "\n",
      "Epoch %d: train loss %f 128 0.3453310638666153\n",
      "Epoch 128: val loss 0.530759\n",
      "\n",
      "Epoch %d: train loss %f 129 0.36778327624003093\n",
      "Epoch 129: val loss 0.533985\n",
      "\n",
      "Epoch %d: train loss %f 130 0.3377125630776087\n",
      "Epoch 130: val loss 0.508638\n",
      "\n",
      "Epoch %d: train loss %f 131 0.3543143212795258\n",
      "Epoch 131: val loss 0.511354\n",
      "\n",
      "Epoch %d: train loss %f 132 0.33902479012807213\n",
      "Epoch 132: val loss 0.526587\n",
      "\n",
      "Epoch %d: train loss %f 133 0.35240890085697174\n",
      "Epoch 133: val loss 0.526951\n",
      "\n",
      "Epoch %d: train loss %f 134 0.35312404930591584\n",
      "Epoch 134: val loss 0.509844\n",
      "\n",
      "Epoch %d: train loss %f 135 0.36662864685058594\n",
      "Epoch 135: val loss 0.530567\n",
      "\n",
      "Epoch %d: train loss %f 136 0.36843913396199546\n",
      "Epoch 136: val loss 0.507525\n",
      "\n",
      "Epoch %d: train loss %f 137 0.3253769040107727\n",
      "Epoch 137: val loss 0.510313\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6920358786980311\n",
      "Epoch 0: val loss 0.693831\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6868706345558167\n",
      "Epoch 1: val loss 0.692667\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6785875310500463\n",
      "Epoch 2: val loss 0.688745\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6681106040875117\n",
      "Epoch 3: val loss 0.679240\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6427516937255859\n",
      "Epoch 4: val loss 0.664267\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6310703406731287\n",
      "Epoch 5: val loss 0.641140\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6095238029956818\n",
      "Epoch 6: val loss 0.625543\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5929995278517405\n",
      "Epoch 7: val loss 0.621294\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5999750271439552\n",
      "Epoch 8: val loss 0.649043\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5792486493786176\n",
      "Epoch 9: val loss 0.605175\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5755316043893496\n",
      "Epoch 10: val loss 0.619957\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5639689316352209\n",
      "Epoch 11: val loss 0.630170\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5587325493494669\n",
      "Epoch 12: val loss 0.613999\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5623621866106987\n",
      "Epoch 13: val loss 0.617539\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5638061513503393\n",
      "Epoch 14: val loss 0.607082\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5404041608174642\n",
      "Epoch 15: val loss 0.610987\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5385060881574949\n",
      "Epoch 16: val loss 0.623493\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5206025838851929\n",
      "Epoch 17: val loss 0.618911\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5493848323822021\n",
      "Epoch 18: val loss 0.609370\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5286439458529154\n",
      "Epoch 19: val loss 0.626396\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5075370570023855\n",
      "Epoch 20: val loss 0.618860\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5174251173933347\n",
      "Epoch 21: val loss 0.659470\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5094020714362463\n",
      "Epoch 22: val loss 0.624786\n",
      "\n",
      "Epoch %d: train loss %f 23 0.49789467205603916\n",
      "Epoch 23: val loss 0.668687\n",
      "\n",
      "Epoch %d: train loss %f 24 0.5175987780094147\n",
      "Epoch 24: val loss 0.598449\n",
      "\n",
      "Epoch %d: train loss %f 25 0.510525127251943\n",
      "Epoch 25: val loss 0.632316\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4730653042594592\n",
      "Epoch 26: val loss 0.625828\n",
      "\n",
      "Epoch %d: train loss %f 27 0.47918155292669934\n",
      "Epoch 27: val loss 0.628344\n",
      "\n",
      "Epoch %d: train loss %f 28 0.47758792092402774\n",
      "Epoch 28: val loss 0.603268\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4747025246421496\n",
      "Epoch 29: val loss 0.677666\n",
      "\n",
      "Epoch %d: train loss %f 30 0.4547676717241605\n",
      "Epoch 30: val loss 0.630129\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4562567050258319\n",
      "Epoch 31: val loss 0.663952\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4778697316845258\n",
      "Epoch 32: val loss 0.614858\n",
      "\n",
      "Epoch %d: train loss %f 33 0.4625547453761101\n",
      "Epoch 33: val loss 0.672987\n",
      "\n",
      "Epoch %d: train loss %f 34 0.43179211268822354\n",
      "Epoch 34: val loss 0.613199\n",
      "\n",
      "Epoch %d: train loss %f 35 0.46851056069135666\n",
      "Epoch 35: val loss 0.662559\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4525074288249016\n",
      "Epoch 36: val loss 0.654440\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4620024959246318\n",
      "Epoch 37: val loss 0.629640\n",
      "\n",
      "Epoch %d: train loss %f 38 0.462345985074838\n",
      "Epoch 38: val loss 0.672779\n",
      "\n",
      "Epoch %d: train loss %f 39 0.43812014410893124\n",
      "Epoch 39: val loss 0.619360\n",
      "\n",
      "Epoch %d: train loss %f 40 0.4605112249652545\n",
      "Epoch 40: val loss 0.665278\n",
      "\n",
      "Epoch %d: train loss %f 41 0.44067761053641635\n",
      "Epoch 41: val loss 0.619195\n",
      "\n",
      "Epoch %d: train loss %f 42 0.44312313944101334\n",
      "Epoch 42: val loss 0.676343\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4550960585474968\n",
      "Epoch 43: val loss 0.632985\n",
      "\n",
      "Epoch %d: train loss %f 44 0.4279549966255824\n",
      "Epoch 44: val loss 0.647774\n",
      "\n",
      "Epoch %d: train loss %f 45 0.4416108975807826\n",
      "Epoch 45: val loss 0.620967\n",
      "\n",
      "Epoch %d: train loss %f 46 0.44394425054391223\n",
      "Epoch 46: val loss 0.633557\n",
      "\n",
      "Epoch %d: train loss %f 47 0.4403703510761261\n",
      "Epoch 47: val loss 0.652139\n",
      "\n",
      "Epoch %d: train loss %f 48 0.4157215232650439\n",
      "Epoch 48: val loss 0.676563\n",
      "\n",
      "Epoch %d: train loss %f 49 0.43656955907742184\n",
      "Epoch 49: val loss 0.641046\n",
      "\n",
      "Epoch %d: train loss %f 50 0.46467553824186325\n",
      "Epoch 50: val loss 0.675425\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4367004558444023\n",
      "Epoch 51: val loss 0.636994\n",
      "\n",
      "Epoch %d: train loss %f 52 0.4081321607033412\n",
      "Epoch 52: val loss 0.633200\n",
      "\n",
      "Epoch %d: train loss %f 53 0.4178628424803416\n",
      "Epoch 53: val loss 0.636131\n",
      "\n",
      "Epoch %d: train loss %f 54 0.40933466454346973\n",
      "Epoch 54: val loss 0.662498\n",
      "\n",
      "Epoch %d: train loss %f 55 0.4095415348807971\n",
      "Epoch 55: val loss 0.637121\n",
      "\n",
      "Epoch %d: train loss %f 56 0.3831655482451121\n",
      "Epoch 56: val loss 0.627877\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4048382267355919\n",
      "Epoch 57: val loss 0.638492\n",
      "\n",
      "Epoch %d: train loss %f 58 0.40977007399002713\n",
      "Epoch 58: val loss 0.676726\n",
      "\n",
      "Epoch %d: train loss %f 59 0.4405423973997434\n",
      "Epoch 59: val loss 0.663982\n",
      "\n",
      "Epoch %d: train loss %f 60 0.41260775178670883\n",
      "Epoch 60: val loss 0.689794\n",
      "\n",
      "Epoch %d: train loss %f 61 0.43296685069799423\n",
      "Epoch 61: val loss 0.648978\n",
      "\n",
      "Epoch %d: train loss %f 62 0.38386762142181396\n",
      "Epoch 62: val loss 0.681119\n",
      "\n",
      "Epoch %d: train loss %f 63 0.4015752896666527\n",
      "Epoch 63: val loss 0.658688\n",
      "\n",
      "Epoch %d: train loss %f 64 0.412474458416303\n",
      "Epoch 64: val loss 0.732082\n",
      "\n",
      "Epoch %d: train loss %f 65 0.3992019755144914\n",
      "Epoch 65: val loss 0.629852\n",
      "\n",
      "Epoch %d: train loss %f 66 0.3878613238533338\n",
      "Epoch 66: val loss 0.700882\n",
      "\n",
      "Epoch %d: train loss %f 67 0.4243367041150729\n",
      "Epoch 67: val loss 0.650234\n",
      "\n",
      "Epoch %d: train loss %f 68 0.401100921134154\n",
      "Epoch 68: val loss 0.703255\n",
      "\n",
      "Epoch %d: train loss %f 69 0.39866608877976734\n",
      "Epoch 69: val loss 0.672178\n",
      "\n",
      "Epoch %d: train loss %f 70 0.42369384070237476\n",
      "Epoch 70: val loss 0.642423\n",
      "\n",
      "Epoch %d: train loss %f 71 0.4018256018559138\n",
      "Epoch 71: val loss 0.652527\n",
      "\n",
      "Epoch %d: train loss %f 72 0.3855601002772649\n",
      "Epoch 72: val loss 0.678682\n",
      "\n",
      "Epoch %d: train loss %f 73 0.38858253757158917\n",
      "Epoch 73: val loss 0.628133\n",
      "\n",
      "Epoch %d: train loss %f 74 0.3737846960624059\n",
      "Epoch 74: val loss 0.671741\n",
      "\n",
      "Epoch %d: train loss %f 75 0.4300795992215474\n",
      "Epoch 75: val loss 0.645699\n",
      "\n",
      "Epoch %d: train loss %f 76 0.3932490323980649\n",
      "Epoch 76: val loss 0.674823\n",
      "\n",
      "Epoch %d: train loss %f 77 0.37501147389411926\n",
      "Epoch 77: val loss 0.664258\n",
      "\n",
      "Epoch %d: train loss %f 78 0.3918663064638774\n",
      "Epoch 78: val loss 0.657183\n",
      "\n",
      "Epoch %d: train loss %f 79 0.3735240027308464\n",
      "Epoch 79: val loss 0.641468\n",
      "\n",
      "Epoch %d: train loss %f 80 0.4046531443794568\n",
      "Epoch 80: val loss 0.697270\n",
      "\n",
      "Epoch %d: train loss %f 81 0.3940097838640213\n",
      "Epoch 81: val loss 0.634662\n",
      "\n",
      "Epoch %d: train loss %f 82 0.35319771741827327\n",
      "Epoch 82: val loss 0.701615\n",
      "\n",
      "Epoch %d: train loss %f 83 0.3767699959377448\n",
      "Epoch 83: val loss 0.676397\n",
      "\n",
      "Epoch %d: train loss %f 84 0.35334040969610214\n",
      "Epoch 84: val loss 0.636292\n",
      "\n",
      "Epoch %d: train loss %f 85 0.37676266580820084\n",
      "Epoch 85: val loss 0.665036\n",
      "\n",
      "Epoch %d: train loss %f 86 0.3910692557692528\n",
      "Epoch 86: val loss 0.647753\n",
      "\n",
      "Epoch %d: train loss %f 87 0.3678591052691142\n",
      "Epoch 87: val loss 0.697619\n",
      "\n",
      "Epoch %d: train loss %f 88 0.3856197843949\n",
      "Epoch 88: val loss 0.712571\n",
      "\n",
      "Epoch %d: train loss %f 89 0.3757457882165909\n",
      "Epoch 89: val loss 0.723266\n",
      "\n",
      "Epoch %d: train loss %f 90 0.3794601336121559\n",
      "Epoch 90: val loss 0.693811\n",
      "\n",
      "Epoch %d: train loss %f 91 0.3816751850148042\n",
      "Epoch 91: val loss 0.673193\n",
      "\n",
      "Epoch %d: train loss %f 92 0.39278890440861386\n",
      "Epoch 92: val loss 0.707813\n",
      "\n",
      "Epoch %d: train loss %f 93 0.38351239264011383\n",
      "Epoch 93: val loss 0.664712\n",
      "\n",
      "Epoch %d: train loss %f 94 0.3670908759037654\n",
      "Epoch 94: val loss 0.685433\n",
      "\n",
      "Epoch %d: train loss %f 95 0.378008467455705\n",
      "Epoch 95: val loss 0.701392\n",
      "\n",
      "Epoch %d: train loss %f 96 0.373137004673481\n",
      "Epoch 96: val loss 0.608148\n",
      "\n",
      "Epoch %d: train loss %f 97 0.42226140946149826\n",
      "Epoch 97: val loss 0.757134\n",
      "\n",
      "Epoch %d: train loss %f 98 0.36685991038878757\n",
      "Epoch 98: val loss 0.628594\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3898024608691533\n",
      "Epoch 99: val loss 0.734141\n",
      "\n",
      "Epoch %d: train loss %f 100 0.3679496683180332\n",
      "Epoch 100: val loss 0.648259\n",
      "\n",
      "Epoch %d: train loss %f 101 0.35825494552652043\n",
      "Epoch 101: val loss 0.717689\n",
      "\n",
      "Epoch %d: train loss %f 102 0.3550288639962673\n",
      "Epoch 102: val loss 0.635031\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3387517121930917\n",
      "Epoch 103: val loss 0.623392\n",
      "\n",
      "Epoch %d: train loss %f 104 0.34824998552600545\n",
      "Epoch 104: val loss 0.769271\n",
      "\n",
      "Epoch %d: train loss %f 105 0.3607529364526272\n",
      "Epoch 105: val loss 0.627118\n",
      "\n",
      "Epoch %d: train loss %f 106 0.337723933160305\n",
      "Epoch 106: val loss 0.676221\n",
      "\n",
      "Epoch %d: train loss %f 107 0.3332880636056264\n",
      "Epoch 107: val loss 0.715511\n",
      "\n",
      "Epoch %d: train loss %f 108 0.3310422897338867\n",
      "Epoch 108: val loss 0.652636\n",
      "\n",
      "Epoch %d: train loss %f 109 0.34462570399045944\n",
      "Epoch 109: val loss 0.683338\n",
      "\n",
      "Epoch %d: train loss %f 110 0.33895524094502133\n",
      "Epoch 110: val loss 0.633002\n",
      "\n",
      "Epoch %d: train loss %f 111 0.37331795940796536\n",
      "Epoch 111: val loss 0.705547\n",
      "\n",
      "Epoch %d: train loss %f 112 0.3692905778686206\n",
      "Epoch 112: val loss 0.636958\n",
      "\n",
      "Epoch %d: train loss %f 113 0.318792000412941\n",
      "Epoch 113: val loss 0.678469\n",
      "\n",
      "Epoch %d: train loss %f 114 0.36786940693855286\n",
      "Epoch 114: val loss 0.732495\n",
      "\n",
      "Epoch %d: train loss %f 115 0.393184798459212\n",
      "Epoch 115: val loss 0.716058\n",
      "\n",
      "Epoch %d: train loss %f 116 0.34743138402700424\n",
      "Epoch 116: val loss 0.674952\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3505145348608494\n",
      "Epoch 117: val loss 0.706564\n",
      "\n",
      "Epoch %d: train loss %f 118 0.3830864168703556\n",
      "Epoch 118: val loss 0.688163\n",
      "\n",
      "Epoch %d: train loss %f 119 0.3783548101782799\n",
      "Epoch 119: val loss 0.681177\n",
      "\n",
      "Epoch %d: train loss %f 120 0.35030857970317203\n",
      "Epoch 120: val loss 0.682203\n",
      "\n",
      "Epoch %d: train loss %f 121 0.36019859711329144\n",
      "Epoch 121: val loss 0.661410\n",
      "\n",
      "Epoch %d: train loss %f 122 0.3565704623858134\n",
      "Epoch 122: val loss 0.769242\n",
      "\n",
      "Epoch %d: train loss %f 123 0.32359518234928447\n",
      "Epoch 123: val loss 0.663199\n",
      "\n",
      "Epoch %d: train loss %f 124 0.32397738471627235\n",
      "Epoch 124: val loss 0.706734\n",
      "\n",
      "Epoch %d: train loss %f 125 0.37746769562363625\n",
      "Epoch 125: val loss 0.785602\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3423510591189067\n",
      "Epoch 126: val loss 0.683723\n",
      "\n",
      "Epoch %d: train loss %f 127 0.3617266888419787\n",
      "Epoch 127: val loss 0.700644\n",
      "\n",
      "Epoch %d: train loss %f 128 0.35420698920885724\n",
      "Epoch 128: val loss 0.690307\n",
      "\n",
      "Epoch %d: train loss %f 129 0.30649268875519436\n",
      "Epoch 129: val loss 0.649488\n",
      "\n",
      "Epoch %d: train loss %f 130 0.29056597501039505\n",
      "Epoch 130: val loss 0.672800\n",
      "\n",
      "Epoch %d: train loss %f 131 0.33692243819435436\n",
      "Epoch 131: val loss 0.682865\n",
      "\n",
      "Epoch %d: train loss %f 132 0.30996424828966457\n",
      "Epoch 132: val loss 0.677854\n",
      "\n",
      "Epoch %d: train loss %f 133 0.328989344338576\n",
      "Epoch 133: val loss 0.692547\n",
      "\n",
      "Epoch %d: train loss %f 134 0.3243616657952468\n",
      "Epoch 134: val loss 0.777356\n",
      "\n",
      "Epoch %d: train loss %f 135 0.35412319377064705\n",
      "Epoch 135: val loss 0.734769\n",
      "\n",
      "Epoch %d: train loss %f 136 0.30369114503264427\n",
      "Epoch 136: val loss 0.695785\n",
      "\n",
      "Epoch %d: train loss %f 137 0.36205241084098816\n",
      "Epoch 137: val loss 0.668977\n",
      "\n",
      "Epoch %d: train loss %f 138 0.37392638623714447\n",
      "Epoch 138: val loss 0.766797\n",
      "\n",
      "Epoch %d: train loss %f 139 0.32422242561976117\n",
      "Epoch 139: val loss 0.738257\n",
      "\n",
      "Epoch %d: train loss %f 140 0.30594557772080105\n",
      "Epoch 140: val loss 0.688100\n",
      "\n",
      "Epoch %d: train loss %f 141 0.3335820473730564\n",
      "Epoch 141: val loss 0.703647\n",
      "\n",
      "Epoch %d: train loss %f 142 0.3535033365090688\n",
      "Epoch 142: val loss 0.687952\n",
      "\n",
      "Epoch %d: train loss %f 143 0.33531727890173596\n",
      "Epoch 143: val loss 0.786525\n",
      "\n",
      "Epoch %d: train loss %f 144 0.32459796965122223\n",
      "Epoch 144: val loss 0.672631\n",
      "\n",
      "Epoch %d: train loss %f 145 0.3592488144834836\n",
      "Epoch 145: val loss 0.684493\n",
      "\n",
      "Epoch %d: train loss %f 146 0.30068525299429893\n",
      "Epoch 146: val loss 0.737290\n",
      "\n",
      "Epoch %d: train loss %f 147 0.3142572194337845\n",
      "Epoch 147: val loss 0.718013\n",
      "\n",
      "Epoch %d: train loss %f 148 0.30912745371460915\n",
      "Epoch 148: val loss 0.743948\n",
      "\n",
      "Epoch %d: train loss %f 149 0.3551545354227225\n",
      "Epoch 149: val loss 0.750437\n",
      "\n",
      "Epoch %d: train loss %f 150 0.3166653700172901\n",
      "Epoch 150: val loss 0.706849\n",
      "\n",
      "Epoch %d: train loss %f 151 0.3202810510993004\n",
      "Epoch 151: val loss 0.711438\n",
      "\n",
      "Epoch %d: train loss %f 152 0.3331418459614118\n",
      "Epoch 152: val loss 0.764178\n",
      "\n",
      "Epoch %d: train loss %f 153 0.30448632563153905\n",
      "Epoch 153: val loss 0.684259\n",
      "\n",
      "Epoch %d: train loss %f 154 0.35278288399179775\n",
      "Epoch 154: val loss 0.733542\n",
      "\n",
      "Epoch %d: train loss %f 155 0.33022696773211163\n",
      "Epoch 155: val loss 0.766038\n",
      "\n",
      "Epoch %d: train loss %f 156 0.30180420602361363\n",
      "Epoch 156: val loss 0.713192\n",
      "\n",
      "Epoch %d: train loss %f 157 0.32612961406509083\n",
      "Epoch 157: val loss 0.718639\n",
      "\n",
      "Epoch %d: train loss %f 158 0.31559347609678906\n",
      "Epoch 158: val loss 0.730059\n",
      "\n",
      "Epoch %d: train loss %f 159 0.30921464537580806\n",
      "Epoch 159: val loss 0.758338\n",
      "\n",
      "Epoch %d: train loss %f 160 0.3369373840590318\n",
      "Epoch 160: val loss 0.638916\n",
      "\n",
      "Epoch %d: train loss %f 161 0.3405109296242396\n",
      "Epoch 161: val loss 0.818070\n",
      "\n",
      "Epoch %d: train loss %f 162 0.32594340791304904\n",
      "Epoch 162: val loss 0.731983\n",
      "\n",
      "Epoch %d: train loss %f 163 0.354216484973828\n",
      "Epoch 163: val loss 0.869158\n",
      "\n",
      "Epoch %d: train loss %f 164 0.3305231270690759\n",
      "Epoch 164: val loss 0.731423\n",
      "\n",
      "Epoch %d: train loss %f 165 0.3195517224570115\n",
      "Epoch 165: val loss 0.796733\n",
      "\n",
      "Epoch %d: train loss %f 166 0.3402125996847947\n",
      "Epoch 166: val loss 0.737766\n",
      "\n",
      "Epoch %d: train loss %f 167 0.3204428131381671\n",
      "Epoch 167: val loss 0.780980\n",
      "\n",
      "Epoch %d: train loss %f 168 0.30026861776908237\n",
      "Epoch 168: val loss 0.703952\n",
      "\n",
      "Epoch %d: train loss %f 169 0.2977348727484544\n",
      "Epoch 169: val loss 0.750745\n",
      "\n",
      "Epoch %d: train loss %f 170 0.3362196646630764\n",
      "Epoch 170: val loss 0.714712\n",
      "\n",
      "Epoch %d: train loss %f 171 0.30374711379408836\n",
      "Epoch 171: val loss 0.785810\n",
      "\n",
      "Epoch %d: train loss %f 172 0.3281955805917581\n",
      "Epoch 172: val loss 0.803585\n",
      "\n",
      "Epoch %d: train loss %f 173 0.3088691396017869\n",
      "Epoch 173: val loss 0.764710\n",
      "\n",
      "Epoch %d: train loss %f 174 0.34422904004653293\n",
      "Epoch 174: val loss 0.722980\n",
      "\n",
      "Epoch %d: train loss %f 175 0.3202160932123661\n",
      "Epoch 175: val loss 0.871132\n",
      "\n",
      "Epoch %d: train loss %f 176 0.33568715676665306\n",
      "Epoch 176: val loss 0.669498\n",
      "\n",
      "Epoch %d: train loss %f 177 0.314111119757096\n",
      "Epoch 177: val loss 0.815812\n",
      "\n",
      "Epoch %d: train loss %f 178 0.3023865595459938\n",
      "Epoch 178: val loss 0.763929\n",
      "\n",
      "Epoch %d: train loss %f 179 0.2956395087142785\n",
      "Epoch 179: val loss 0.805278\n",
      "\n",
      "Epoch %d: train loss %f 180 0.3287511517604192\n",
      "Epoch 180: val loss 0.742161\n",
      "\n",
      "Epoch %d: train loss %f 181 0.2943778870006402\n",
      "Epoch 181: val loss 0.762402\n",
      "\n",
      "Epoch %d: train loss %f 182 0.297468364238739\n",
      "Epoch 182: val loss 0.809421\n",
      "\n",
      "Epoch %d: train loss %f 183 0.28324640666445094\n",
      "Epoch 183: val loss 0.806781\n",
      "\n",
      "Epoch %d: train loss %f 184 0.2974111506094535\n",
      "Epoch 184: val loss 0.788294\n",
      "\n",
      "Epoch %d: train loss %f 185 0.314158125470082\n",
      "Epoch 185: val loss 0.795973\n",
      "\n",
      "Epoch %d: train loss %f 186 0.25601898816724616\n",
      "Epoch 186: val loss 0.777769\n",
      "\n",
      "Epoch %d: train loss %f 187 0.3048800776402156\n",
      "Epoch 187: val loss 0.797789\n",
      "\n",
      "Epoch %d: train loss %f 188 0.33432693282763165\n",
      "Epoch 188: val loss 0.762317\n",
      "\n",
      "Epoch %d: train loss %f 189 0.3143714765707652\n",
      "Epoch 189: val loss 0.763161\n",
      "\n",
      "Epoch %d: train loss %f 190 0.2930992916226387\n",
      "Epoch 190: val loss 0.826729\n",
      "\n",
      "Epoch %d: train loss %f 191 0.3334031843890746\n",
      "Epoch 191: val loss 0.745872\n",
      "\n",
      "Epoch %d: train loss %f 192 0.3817535613973935\n",
      "Epoch 192: val loss 0.902963\n",
      "\n",
      "Epoch %d: train loss %f 193 0.29723524550596875\n",
      "Epoch 193: val loss 0.776084\n",
      "\n",
      "Epoch %d: train loss %f 194 0.3166322236259778\n",
      "Epoch 194: val loss 0.807115\n",
      "\n",
      "Epoch %d: train loss %f 195 0.2825519914428393\n",
      "Epoch 195: val loss 0.737889\n",
      "\n",
      "Epoch %d: train loss %f 196 0.2945198640227318\n",
      "Epoch 196: val loss 0.792333\n",
      "\n",
      "Epoch %d: train loss %f 197 0.2936452602346738\n",
      "Epoch 197: val loss 0.732826\n",
      "\n",
      "Epoch %d: train loss %f 198 0.29426747436324757\n",
      "Epoch 198: val loss 0.786186\n",
      "\n",
      "Epoch %d: train loss %f 199 0.34287329763174057\n",
      "Epoch 199: val loss 0.865741\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6929368873437246\n",
      "Epoch 0: val loss 0.692987\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6899679352839788\n",
      "Epoch 1: val loss 0.691531\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6813904295365015\n",
      "Epoch 2: val loss 0.685817\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6641122400760651\n",
      "Epoch 3: val loss 0.666189\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6421391814947128\n",
      "Epoch 4: val loss 0.638076\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6263775378465652\n",
      "Epoch 5: val loss 0.640537\n",
      "\n",
      "Epoch %d: train loss %f 6 0.5935788849989573\n",
      "Epoch 6: val loss 0.624281\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5910034030675888\n",
      "Epoch 7: val loss 0.629461\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5652998437484106\n",
      "Epoch 8: val loss 0.628863\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5625094523032507\n",
      "Epoch 9: val loss 0.655612\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5579652537902197\n",
      "Epoch 10: val loss 0.640557\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5584351172049841\n",
      "Epoch 11: val loss 0.652236\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5689140806595484\n",
      "Epoch 12: val loss 0.666703\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5411119510730108\n",
      "Epoch 13: val loss 0.653419\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5303497885664304\n",
      "Epoch 14: val loss 0.665300\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5356837660074234\n",
      "Epoch 15: val loss 0.648518\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5421152263879776\n",
      "Epoch 16: val loss 0.649871\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5323665539423624\n",
      "Epoch 17: val loss 0.657449\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5272777527570724\n",
      "Epoch 18: val loss 0.660553\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5229361479481062\n",
      "Epoch 19: val loss 0.650446\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5180947308739027\n",
      "Epoch 20: val loss 0.661874\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5219054520130157\n",
      "Epoch 21: val loss 0.650727\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5181398068865141\n",
      "Epoch 22: val loss 0.648231\n",
      "\n",
      "Epoch %d: train loss %f 23 0.503838874399662\n",
      "Epoch 23: val loss 0.628337\n",
      "\n",
      "Epoch %d: train loss %f 24 0.4983493636051814\n",
      "Epoch 24: val loss 0.641647\n",
      "\n",
      "Epoch %d: train loss %f 25 0.5087298105160395\n",
      "Epoch 25: val loss 0.633465\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4978468840320905\n",
      "Epoch 26: val loss 0.638038\n",
      "\n",
      "Epoch %d: train loss %f 27 0.5005835220217705\n",
      "Epoch 27: val loss 0.641439\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4881170690059662\n",
      "Epoch 28: val loss 0.636829\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4793178414305051\n",
      "Epoch 29: val loss 0.610603\n",
      "\n",
      "Epoch %d: train loss %f 30 0.4841281324625015\n",
      "Epoch 30: val loss 0.641420\n",
      "\n",
      "Epoch %d: train loss %f 31 0.4654126192132632\n",
      "Epoch 31: val loss 0.635346\n",
      "\n",
      "Epoch %d: train loss %f 32 0.48189520090818405\n",
      "Epoch 32: val loss 0.620595\n",
      "\n",
      "Epoch %d: train loss %f 33 0.46030255655447644\n",
      "Epoch 33: val loss 0.632734\n",
      "\n",
      "Epoch %d: train loss %f 34 0.4649440571665764\n",
      "Epoch 34: val loss 0.625620\n",
      "\n",
      "Epoch %d: train loss %f 35 0.45021584381659824\n",
      "Epoch 35: val loss 0.628067\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4376898395518462\n",
      "Epoch 36: val loss 0.663903\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4346817235151927\n",
      "Epoch 37: val loss 0.632421\n",
      "\n",
      "Epoch %d: train loss %f 38 0.44232525676488876\n",
      "Epoch 38: val loss 0.655283\n",
      "\n",
      "Epoch %d: train loss %f 39 0.44351136932770413\n",
      "Epoch 39: val loss 0.634299\n",
      "\n",
      "Epoch %d: train loss %f 40 0.45238828162352246\n",
      "Epoch 40: val loss 0.642872\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4446619898080826\n",
      "Epoch 41: val loss 0.660738\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4601868266860644\n",
      "Epoch 42: val loss 0.644393\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4326274295647939\n",
      "Epoch 43: val loss 0.663116\n",
      "\n",
      "Epoch %d: train loss %f 44 0.43324921528498334\n",
      "Epoch 44: val loss 0.637267\n",
      "\n",
      "Epoch %d: train loss %f 45 0.43143830945094425\n",
      "Epoch 45: val loss 0.651435\n",
      "\n",
      "Epoch %d: train loss %f 46 0.431698489934206\n",
      "Epoch 46: val loss 0.669839\n",
      "\n",
      "Epoch %d: train loss %f 47 0.45407455166180927\n",
      "Epoch 47: val loss 0.645919\n",
      "\n",
      "Epoch %d: train loss %f 48 0.43670280277729034\n",
      "Epoch 48: val loss 0.639475\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4339510028560956\n",
      "Epoch 49: val loss 0.633945\n",
      "\n",
      "Epoch %d: train loss %f 50 0.42742683241764706\n",
      "Epoch 50: val loss 0.638626\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4515061353643735\n",
      "Epoch 51: val loss 0.626274\n",
      "\n",
      "Epoch %d: train loss %f 52 0.4316513290007909\n",
      "Epoch 52: val loss 0.632516\n",
      "\n",
      "Epoch %d: train loss %f 53 0.4279771496852239\n",
      "Epoch 53: val loss 0.630502\n",
      "\n",
      "Epoch %d: train loss %f 54 0.41375521818796795\n",
      "Epoch 54: val loss 0.659323\n",
      "\n",
      "Epoch %d: train loss %f 55 0.42623066405455273\n",
      "Epoch 55: val loss 0.632836\n",
      "\n",
      "Epoch %d: train loss %f 56 0.41496769338846207\n",
      "Epoch 56: val loss 0.655368\n",
      "\n",
      "Epoch %d: train loss %f 57 0.41939523567756015\n",
      "Epoch 57: val loss 0.646599\n",
      "\n",
      "Epoch %d: train loss %f 58 0.4312288388609886\n",
      "Epoch 58: val loss 0.638971\n",
      "\n",
      "Epoch %d: train loss %f 59 0.41808151205380756\n",
      "Epoch 59: val loss 0.667177\n",
      "\n",
      "Epoch %d: train loss %f 60 0.43361959358056384\n",
      "Epoch 60: val loss 0.660507\n",
      "\n",
      "Epoch %d: train loss %f 61 0.39903071771065396\n",
      "Epoch 61: val loss 0.652946\n",
      "\n",
      "Epoch %d: train loss %f 62 0.3986647625764211\n",
      "Epoch 62: val loss 0.659691\n",
      "\n",
      "Epoch %d: train loss %f 63 0.4310838431119919\n",
      "Epoch 63: val loss 0.654238\n",
      "\n",
      "Epoch %d: train loss %f 64 0.39426372945308685\n",
      "Epoch 64: val loss 0.639894\n",
      "\n",
      "Epoch %d: train loss %f 65 0.4178433145085971\n",
      "Epoch 65: val loss 0.660786\n",
      "\n",
      "Epoch %d: train loss %f 66 0.42385610193014145\n",
      "Epoch 66: val loss 0.667671\n",
      "\n",
      "Epoch %d: train loss %f 67 0.4037104944388072\n",
      "Epoch 67: val loss 0.660070\n",
      "\n",
      "Epoch %d: train loss %f 68 0.4016595780849457\n",
      "Epoch 68: val loss 0.644013\n",
      "\n",
      "Epoch %d: train loss %f 69 0.40068017194668454\n",
      "Epoch 69: val loss 0.641180\n",
      "\n",
      "Epoch %d: train loss %f 70 0.3968261467913787\n",
      "Epoch 70: val loss 0.643373\n",
      "\n",
      "Epoch %d: train loss %f 71 0.4093574782212575\n",
      "Epoch 71: val loss 0.649751\n",
      "\n",
      "Epoch %d: train loss %f 72 0.37528924147288006\n",
      "Epoch 72: val loss 0.648647\n",
      "\n",
      "Epoch %d: train loss %f 73 0.4064079349239667\n",
      "Epoch 73: val loss 0.668946\n",
      "\n",
      "Epoch %d: train loss %f 74 0.39312834044297534\n",
      "Epoch 74: val loss 0.645307\n",
      "\n",
      "Epoch %d: train loss %f 75 0.38238079473376274\n",
      "Epoch 75: val loss 0.651327\n",
      "\n",
      "Epoch %d: train loss %f 76 0.4097008667886257\n",
      "Epoch 76: val loss 0.675577\n",
      "\n",
      "Epoch %d: train loss %f 77 0.39348094910383224\n",
      "Epoch 77: val loss 0.657841\n",
      "\n",
      "Epoch %d: train loss %f 78 0.42780887087186176\n",
      "Epoch 78: val loss 0.650858\n",
      "\n",
      "Epoch %d: train loss %f 79 0.37116240958372754\n",
      "Epoch 79: val loss 0.645183\n",
      "\n",
      "Epoch %d: train loss %f 80 0.39497313400109607\n",
      "Epoch 80: val loss 0.623398\n",
      "\n",
      "Epoch %d: train loss %f 81 0.3753126338124275\n",
      "Epoch 81: val loss 0.633560\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3772694642345111\n",
      "Epoch 82: val loss 0.639375\n",
      "\n",
      "Epoch %d: train loss %f 83 0.39455240964889526\n",
      "Epoch 83: val loss 0.643862\n",
      "\n",
      "Epoch %d: train loss %f 84 0.38569414615631104\n",
      "Epoch 84: val loss 0.663911\n",
      "\n",
      "Epoch %d: train loss %f 85 0.38845087960362434\n",
      "Epoch 85: val loss 0.658204\n",
      "\n",
      "Epoch %d: train loss %f 86 0.3863484288255374\n",
      "Epoch 86: val loss 0.689987\n",
      "\n",
      "Epoch %d: train loss %f 87 0.3762668122847875\n",
      "Epoch 87: val loss 0.659562\n",
      "\n",
      "Epoch %d: train loss %f 88 0.3801674197117488\n",
      "Epoch 88: val loss 0.673280\n",
      "\n",
      "Epoch %d: train loss %f 89 0.40048886835575104\n",
      "Epoch 89: val loss 0.655069\n",
      "\n",
      "Epoch %d: train loss %f 90 0.3659030335644881\n",
      "Epoch 90: val loss 0.654669\n",
      "\n",
      "Epoch %d: train loss %f 91 0.37902157629529637\n",
      "Epoch 91: val loss 0.653105\n",
      "\n",
      "Epoch %d: train loss %f 92 0.3805963049332301\n",
      "Epoch 92: val loss 0.649245\n",
      "\n",
      "Epoch %d: train loss %f 93 0.4079720750451088\n",
      "Epoch 93: val loss 0.627383\n",
      "\n",
      "Epoch %d: train loss %f 94 0.35832486922542256\n",
      "Epoch 94: val loss 0.660879\n",
      "\n",
      "Epoch %d: train loss %f 95 0.3663285933434963\n",
      "Epoch 95: val loss 0.651564\n",
      "\n",
      "Epoch %d: train loss %f 96 0.352028784652551\n",
      "Epoch 96: val loss 0.661653\n",
      "\n",
      "Epoch %d: train loss %f 97 0.37901746109128\n",
      "Epoch 97: val loss 0.665494\n",
      "\n",
      "Epoch %d: train loss %f 98 0.3522825837135315\n",
      "Epoch 98: val loss 0.664438\n",
      "\n",
      "Epoch %d: train loss %f 99 0.38583643982807797\n",
      "Epoch 99: val loss 0.647558\n",
      "\n",
      "Epoch %d: train loss %f 100 0.3655208187798659\n",
      "Epoch 100: val loss 0.675570\n",
      "\n",
      "Epoch %d: train loss %f 101 0.3627718488375346\n",
      "Epoch 101: val loss 0.670935\n",
      "\n",
      "Epoch %d: train loss %f 102 0.36425822228193283\n",
      "Epoch 102: val loss 0.662444\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3695419530073802\n",
      "Epoch 103: val loss 0.657133\n",
      "\n",
      "Epoch %d: train loss %f 104 0.3491479667524497\n",
      "Epoch 104: val loss 0.653494\n",
      "\n",
      "Epoch %d: train loss %f 105 0.3622483089566231\n",
      "Epoch 105: val loss 0.655512\n",
      "\n",
      "Epoch %d: train loss %f 106 0.38339126606782276\n",
      "Epoch 106: val loss 0.656997\n",
      "\n",
      "Epoch %d: train loss %f 107 0.3634108913441499\n",
      "Epoch 107: val loss 0.673786\n",
      "\n",
      "Epoch %d: train loss %f 108 0.34515905131896335\n",
      "Epoch 108: val loss 0.665927\n",
      "\n",
      "Epoch %d: train loss %f 109 0.3321709508697192\n",
      "Epoch 109: val loss 0.673593\n",
      "\n",
      "Epoch %d: train loss %f 110 0.36756501098473865\n",
      "Epoch 110: val loss 0.674286\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3667425215244293\n",
      "Epoch 111: val loss 0.673845\n",
      "\n",
      "Epoch %d: train loss %f 112 0.3386122186978658\n",
      "Epoch 112: val loss 0.670926\n",
      "\n",
      "Epoch %d: train loss %f 113 0.35533079753319424\n",
      "Epoch 113: val loss 0.646458\n",
      "\n",
      "Epoch %d: train loss %f 114 0.36068221802512807\n",
      "Epoch 114: val loss 0.641672\n",
      "\n",
      "Epoch %d: train loss %f 115 0.38148203616340953\n",
      "Epoch 115: val loss 0.643608\n",
      "\n",
      "Epoch %d: train loss %f 116 0.3631950778265794\n",
      "Epoch 116: val loss 0.637352\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3597637340426445\n",
      "Epoch 117: val loss 0.683185\n",
      "\n",
      "Epoch %d: train loss %f 118 0.38984562704960507\n",
      "Epoch 118: val loss 0.636141\n",
      "\n",
      "Epoch %d: train loss %f 119 0.36564049993952114\n",
      "Epoch 119: val loss 0.626042\n",
      "\n",
      "Epoch %d: train loss %f 120 0.3672369569540024\n",
      "Epoch 120: val loss 0.628943\n",
      "\n",
      "Epoch %d: train loss %f 121 0.3676384588082631\n",
      "Epoch 121: val loss 0.639760\n",
      "\n",
      "Epoch %d: train loss %f 122 0.35511063784360886\n",
      "Epoch 122: val loss 0.634494\n",
      "\n",
      "Epoch %d: train loss %f 123 0.38850561777750653\n",
      "Epoch 123: val loss 0.664741\n",
      "\n",
      "Epoch %d: train loss %f 124 0.38372981548309326\n",
      "Epoch 124: val loss 0.674289\n",
      "\n",
      "Epoch %d: train loss %f 125 0.35690781598289806\n",
      "Epoch 125: val loss 0.674103\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3930293396115303\n",
      "Epoch 126: val loss 0.678077\n",
      "\n",
      "Epoch %d: train loss %f 127 0.32260559499263763\n",
      "Epoch 127: val loss 0.681113\n",
      "\n",
      "Epoch %d: train loss %f 128 0.35913147777318954\n",
      "Epoch 128: val loss 0.677330\n",
      "\n",
      "Epoch %d: train loss %f 129 0.3330027026434739\n",
      "Epoch 129: val loss 0.663937\n",
      "\n",
      "Epoch %d: train loss %f 130 0.3305594051877658\n",
      "Epoch 130: val loss 0.673323\n",
      "\n",
      "Epoch %d: train loss %f 131 0.39568181335926056\n",
      "Epoch 131: val loss 0.648402\n",
      "\n",
      "Epoch %d: train loss %f 132 0.36007855584224063\n",
      "Epoch 132: val loss 0.656100\n",
      "\n",
      "Epoch %d: train loss %f 133 0.30090117702881497\n",
      "Epoch 133: val loss 0.652851\n",
      "\n",
      "Epoch %d: train loss %f 134 0.35228394716978073\n",
      "Epoch 134: val loss 0.665896\n",
      "\n",
      "Epoch %d: train loss %f 135 0.3273136379818122\n",
      "Epoch 135: val loss 0.665678\n",
      "\n",
      "Epoch %d: train loss %f 136 0.3359842250744502\n",
      "Epoch 136: val loss 0.668926\n",
      "\n",
      "Epoch %d: train loss %f 137 0.30169226105014485\n",
      "Epoch 137: val loss 0.677315\n",
      "\n",
      "Epoch %d: train loss %f 138 0.36780256157120067\n",
      "Epoch 138: val loss 0.674831\n",
      "\n",
      "Epoch %d: train loss %f 139 0.34118475516637164\n",
      "Epoch 139: val loss 0.671444\n",
      "\n",
      "Epoch %d: train loss %f 140 0.31850406775871914\n",
      "Epoch 140: val loss 0.671683\n",
      "\n",
      "Epoch %d: train loss %f 141 0.34864800175031024\n",
      "Epoch 141: val loss 0.661775\n",
      "\n",
      "Epoch %d: train loss %f 142 0.3260931794842084\n",
      "Epoch 142: val loss 0.653535\n",
      "\n",
      "Epoch %d: train loss %f 143 0.33288925016919774\n",
      "Epoch 143: val loss 0.657497\n",
      "\n",
      "Epoch %d: train loss %f 144 0.3199293948709965\n",
      "Epoch 144: val loss 0.662233\n",
      "\n",
      "Epoch %d: train loss %f 145 0.3402470139165719\n",
      "Epoch 145: val loss 0.674890\n",
      "\n",
      "Epoch %d: train loss %f 146 0.35260792697469395\n",
      "Epoch 146: val loss 0.683546\n",
      "\n",
      "Epoch %d: train loss %f 147 0.31639207775394124\n",
      "Epoch 147: val loss 0.662910\n",
      "\n",
      "Epoch %d: train loss %f 148 0.30531783153613407\n",
      "Epoch 148: val loss 0.677795\n",
      "\n",
      "Epoch %d: train loss %f 149 0.2887466909984748\n",
      "Epoch 149: val loss 0.696738\n",
      "\n",
      "Epoch %d: train loss %f 150 0.32261595005790394\n",
      "Epoch 150: val loss 0.680776\n",
      "\n",
      "Epoch %d: train loss %f 151 0.2995322172840436\n",
      "Epoch 151: val loss 0.679369\n",
      "\n",
      "Epoch %d: train loss %f 152 0.3115207434942325\n",
      "Epoch 152: val loss 0.696138\n",
      "\n",
      "Epoch %d: train loss %f 153 0.32608605300386745\n",
      "Epoch 153: val loss 0.687641\n",
      "\n",
      "Epoch %d: train loss %f 154 0.3248361684381962\n",
      "Epoch 154: val loss 0.706538\n",
      "\n",
      "Epoch %d: train loss %f 155 0.3659344141681989\n",
      "Epoch 155: val loss 0.674917\n",
      "\n",
      "Epoch %d: train loss %f 156 0.32453254113594693\n",
      "Epoch 156: val loss 0.664578\n",
      "\n",
      "Epoch %d: train loss %f 157 0.29633456220229465\n",
      "Epoch 157: val loss 0.679009\n",
      "\n",
      "Epoch %d: train loss %f 158 0.308939869205157\n",
      "Epoch 158: val loss 0.653680\n",
      "\n",
      "Epoch %d: train loss %f 159 0.30524494126439095\n",
      "Epoch 159: val loss 0.648828\n",
      "\n",
      "Epoch %d: train loss %f 160 0.340148205558459\n",
      "Epoch 160: val loss 0.648328\n",
      "\n",
      "Epoch %d: train loss %f 161 0.32050370549162227\n",
      "Epoch 161: val loss 0.652277\n",
      "\n",
      "Epoch %d: train loss %f 162 0.30409449835618335\n",
      "Epoch 162: val loss 0.669578\n",
      "\n",
      "Epoch %d: train loss %f 163 0.31760261207818985\n",
      "Epoch 163: val loss 0.663794\n",
      "\n",
      "Epoch %d: train loss %f 164 0.34102002034584683\n",
      "Epoch 164: val loss 0.668361\n",
      "\n",
      "Epoch %d: train loss %f 165 0.3224775505562623\n",
      "Epoch 165: val loss 0.704403\n",
      "\n",
      "Epoch %d: train loss %f 166 0.3211013662318389\n",
      "Epoch 166: val loss 0.691470\n",
      "\n",
      "Epoch %d: train loss %f 167 0.29797037442525226\n",
      "Epoch 167: val loss 0.707368\n",
      "\n",
      "Epoch %d: train loss %f 168 0.3400814135869344\n",
      "Epoch 168: val loss 0.676292\n",
      "\n",
      "Epoch %d: train loss %f 169 0.2986563021938006\n",
      "Epoch 169: val loss 0.695237\n",
      "\n",
      "Epoch %d: train loss %f 170 0.3087381881972154\n",
      "Epoch 170: val loss 0.689069\n",
      "\n",
      "Epoch %d: train loss %f 171 0.3374261458714803\n",
      "Epoch 171: val loss 0.672408\n",
      "\n",
      "Epoch %d: train loss %f 172 0.307912219936649\n",
      "Epoch 172: val loss 0.672303\n",
      "\n",
      "Epoch %d: train loss %f 173 0.31082270418604213\n",
      "Epoch 173: val loss 0.655623\n",
      "\n",
      "Epoch %d: train loss %f 174 0.3176830547551314\n",
      "Epoch 174: val loss 0.686534\n",
      "\n",
      "Epoch %d: train loss %f 175 0.3227720533808072\n",
      "Epoch 175: val loss 0.699652\n",
      "\n",
      "Epoch %d: train loss %f 176 0.3690096251666546\n",
      "Epoch 176: val loss 0.710736\n",
      "\n",
      "Epoch %d: train loss %f 177 0.3043954906364282\n",
      "Epoch 177: val loss 0.673406\n",
      "\n",
      "Epoch %d: train loss %f 178 0.32390062386790913\n",
      "Epoch 178: val loss 0.680498\n",
      "\n",
      "Epoch %d: train loss %f 179 0.3135848554472129\n",
      "Epoch 179: val loss 0.701893\n",
      "\n",
      "Epoch %d: train loss %f 180 0.32780804733435315\n",
      "Epoch 180: val loss 0.672976\n",
      "\n",
      "Epoch %d: train loss %f 181 0.3051873656610648\n",
      "Epoch 181: val loss 0.680404\n",
      "\n",
      "Epoch %d: train loss %f 182 0.3054809992512067\n",
      "Epoch 182: val loss 0.684058\n",
      "\n",
      "Epoch %d: train loss %f 183 0.3209141120314598\n",
      "Epoch 183: val loss 0.665951\n",
      "\n",
      "Epoch %d: train loss %f 184 0.28440053885181743\n",
      "Epoch 184: val loss 0.664439\n",
      "\n",
      "Epoch %d: train loss %f 185 0.26186272129416466\n",
      "Epoch 185: val loss 0.663235\n",
      "\n",
      "Epoch %d: train loss %f 186 0.29361507296562195\n",
      "Epoch 186: val loss 0.681752\n",
      "\n",
      "Epoch %d: train loss %f 187 0.30075140049060184\n",
      "Epoch 187: val loss 0.700879\n",
      "\n",
      "Epoch %d: train loss %f 188 0.3312755525112152\n",
      "Epoch 188: val loss 0.695676\n",
      "\n",
      "Epoch %d: train loss %f 189 0.2746880513926347\n",
      "Epoch 189: val loss 0.705989\n",
      "\n",
      "Epoch %d: train loss %f 190 0.2792824978629748\n",
      "Epoch 190: val loss 0.710487\n",
      "\n",
      "Epoch %d: train loss %f 191 0.30059974019726116\n",
      "Epoch 191: val loss 0.705981\n",
      "\n",
      "Epoch %d: train loss %f 192 0.3103477917611599\n",
      "Epoch 192: val loss 0.714932\n",
      "\n",
      "Epoch %d: train loss %f 193 0.3137070046116908\n",
      "Epoch 193: val loss 0.740880\n",
      "\n",
      "Epoch %d: train loss %f 194 0.2780247616271178\n",
      "Epoch 194: val loss 0.713378\n",
      "\n",
      "Epoch %d: train loss %f 195 0.2923499445120494\n",
      "Epoch 195: val loss 0.695183\n",
      "\n",
      "Epoch %d: train loss %f 196 0.3167278841137886\n",
      "Epoch 196: val loss 0.690864\n",
      "\n",
      "Epoch %d: train loss %f 197 0.2719848162184159\n",
      "Epoch 197: val loss 0.698961\n",
      "\n",
      "Epoch %d: train loss %f 198 0.3030659668147564\n",
      "Epoch 198: val loss 0.707501\n",
      "\n",
      "Epoch %d: train loss %f 199 0.2925899376471837\n",
      "Epoch 199: val loss 0.710125\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6917442381381989\n",
      "Epoch 0: val loss 0.692671\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6878927747408549\n",
      "Epoch 1: val loss 0.691753\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6852457722028097\n",
      "Epoch 2: val loss 0.688158\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6701184809207916\n",
      "Epoch 3: val loss 0.675256\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6496703227361044\n",
      "Epoch 4: val loss 0.644331\n",
      "\n",
      "Epoch %d: train loss %f 5 0.623128816485405\n",
      "Epoch 5: val loss 0.603840\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6075460960467657\n",
      "Epoch 6: val loss 0.588034\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6080245872338613\n",
      "Epoch 7: val loss 0.578040\n",
      "\n",
      "Epoch %d: train loss %f 8 0.598244865735372\n",
      "Epoch 8: val loss 0.591763\n",
      "\n",
      "Epoch %d: train loss %f 9 0.596846322218577\n",
      "Epoch 9: val loss 0.572788\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5845461388429006\n",
      "Epoch 10: val loss 0.584019\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5947172492742538\n",
      "Epoch 11: val loss 0.568164\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5703679521878561\n",
      "Epoch 12: val loss 0.581249\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5826051731904348\n",
      "Epoch 13: val loss 0.564028\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5736227035522461\n",
      "Epoch 14: val loss 0.581140\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5656484663486481\n",
      "Epoch 15: val loss 0.570422\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5532983218630155\n",
      "Epoch 16: val loss 0.573128\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5510837659239769\n",
      "Epoch 17: val loss 0.566057\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5610629618167877\n",
      "Epoch 18: val loss 0.565246\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5466638455788294\n",
      "Epoch 19: val loss 0.569008\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5647419095039368\n",
      "Epoch 20: val loss 0.582726\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5532839645942053\n",
      "Epoch 21: val loss 0.569308\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5468077932794889\n",
      "Epoch 22: val loss 0.566355\n",
      "\n",
      "Epoch %d: train loss %f 23 0.5589320783813795\n",
      "Epoch 23: val loss 0.586525\n",
      "\n",
      "Epoch %d: train loss %f 24 0.5501914968093237\n",
      "Epoch 24: val loss 0.559746\n",
      "\n",
      "Epoch %d: train loss %f 25 0.5221347957849503\n",
      "Epoch 25: val loss 0.570861\n",
      "\n",
      "Epoch %d: train loss %f 26 0.5321776469548544\n",
      "Epoch 26: val loss 0.555550\n",
      "\n",
      "Epoch %d: train loss %f 27 0.5314300308624903\n",
      "Epoch 27: val loss 0.573546\n",
      "\n",
      "Epoch %d: train loss %f 28 0.537821168700854\n",
      "Epoch 28: val loss 0.555429\n",
      "\n",
      "Epoch %d: train loss %f 29 0.5108040223519007\n",
      "Epoch 29: val loss 0.553851\n",
      "\n",
      "Epoch %d: train loss %f 30 0.5158716340859731\n",
      "Epoch 30: val loss 0.558255\n",
      "\n",
      "Epoch %d: train loss %f 31 0.49840035537878674\n",
      "Epoch 31: val loss 0.550843\n",
      "\n",
      "Epoch %d: train loss %f 32 0.5127535983920097\n",
      "Epoch 32: val loss 0.553734\n",
      "\n",
      "Epoch %d: train loss %f 33 0.5065965503454208\n",
      "Epoch 33: val loss 0.557920\n",
      "\n",
      "Epoch %d: train loss %f 34 0.5024689411123594\n",
      "Epoch 34: val loss 0.547213\n",
      "\n",
      "Epoch %d: train loss %f 35 0.49428708851337433\n",
      "Epoch 35: val loss 0.554002\n",
      "\n",
      "Epoch %d: train loss %f 36 0.48666248718897503\n",
      "Epoch 36: val loss 0.564039\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4850075567762057\n",
      "Epoch 37: val loss 0.565860\n",
      "\n",
      "Epoch %d: train loss %f 38 0.4657697429259618\n",
      "Epoch 38: val loss 0.537541\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4901309882601102\n",
      "Epoch 39: val loss 0.580013\n",
      "\n",
      "Epoch %d: train loss %f 40 0.5084512258569399\n",
      "Epoch 40: val loss 0.557473\n",
      "\n",
      "Epoch %d: train loss %f 41 0.46678704768419266\n",
      "Epoch 41: val loss 0.553183\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4736887713273366\n",
      "Epoch 42: val loss 0.557032\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4833460946877797\n",
      "Epoch 43: val loss 0.570700\n",
      "\n",
      "Epoch %d: train loss %f 44 0.46765226870775223\n",
      "Epoch 44: val loss 0.551203\n",
      "\n",
      "Epoch %d: train loss %f 45 0.47850889960924786\n",
      "Epoch 45: val loss 0.562650\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4826716457804044\n",
      "Epoch 46: val loss 0.540366\n",
      "\n",
      "Epoch %d: train loss %f 47 0.4614479218920072\n",
      "Epoch 47: val loss 0.561831\n",
      "\n",
      "Epoch %d: train loss %f 48 0.46183469394842785\n",
      "Epoch 48: val loss 0.550869\n",
      "\n",
      "Epoch %d: train loss %f 49 0.4641464203596115\n",
      "Epoch 49: val loss 0.556575\n",
      "\n",
      "Epoch %d: train loss %f 50 0.4714827612042427\n",
      "Epoch 50: val loss 0.540779\n",
      "\n",
      "Epoch %d: train loss %f 51 0.46987402190764743\n",
      "Epoch 51: val loss 0.574418\n",
      "\n",
      "Epoch %d: train loss %f 52 0.46694836765527725\n",
      "Epoch 52: val loss 0.540657\n",
      "\n",
      "Epoch %d: train loss %f 53 0.45756855607032776\n",
      "Epoch 53: val loss 0.559784\n",
      "\n",
      "Epoch %d: train loss %f 54 0.4635378246506055\n",
      "Epoch 54: val loss 0.540070\n",
      "\n",
      "Epoch %d: train loss %f 55 0.47411564737558365\n",
      "Epoch 55: val loss 0.548263\n",
      "\n",
      "Epoch %d: train loss %f 56 0.43340788781642914\n",
      "Epoch 56: val loss 0.556041\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4545498490333557\n",
      "Epoch 57: val loss 0.561661\n",
      "\n",
      "Epoch %d: train loss %f 58 0.42642083019018173\n",
      "Epoch 58: val loss 0.545081\n",
      "\n",
      "Epoch %d: train loss %f 59 0.4486854722102483\n",
      "Epoch 59: val loss 0.566612\n",
      "\n",
      "Epoch %d: train loss %f 60 0.4499075065056483\n",
      "Epoch 60: val loss 0.547280\n",
      "\n",
      "Epoch %d: train loss %f 61 0.42660584424932796\n",
      "Epoch 61: val loss 0.556479\n",
      "\n",
      "Epoch %d: train loss %f 62 0.44287216911713284\n",
      "Epoch 62: val loss 0.557819\n",
      "\n",
      "Epoch %d: train loss %f 63 0.4313155512015025\n",
      "Epoch 63: val loss 0.548380\n",
      "\n",
      "Epoch %d: train loss %f 64 0.451776626209418\n",
      "Epoch 64: val loss 0.564572\n",
      "\n",
      "Epoch %d: train loss %f 65 0.40525760253270465\n",
      "Epoch 65: val loss 0.548745\n",
      "\n",
      "Epoch %d: train loss %f 66 0.4354544977347056\n",
      "Epoch 66: val loss 0.557262\n",
      "\n",
      "Epoch %d: train loss %f 67 0.42807911336421967\n",
      "Epoch 67: val loss 0.551144\n",
      "\n",
      "Epoch %d: train loss %f 68 0.45474054167668027\n",
      "Epoch 68: val loss 0.561929\n",
      "\n",
      "Epoch %d: train loss %f 69 0.43718787282705307\n",
      "Epoch 69: val loss 0.541465\n",
      "\n",
      "Epoch %d: train loss %f 70 0.4312356064716975\n",
      "Epoch 70: val loss 0.562467\n",
      "\n",
      "Epoch %d: train loss %f 71 0.4293786784013112\n",
      "Epoch 71: val loss 0.565211\n",
      "\n",
      "Epoch %d: train loss %f 72 0.41178375979264575\n",
      "Epoch 72: val loss 0.578127\n",
      "\n",
      "Epoch %d: train loss %f 73 0.4170595159133275\n",
      "Epoch 73: val loss 0.547425\n",
      "\n",
      "Epoch %d: train loss %f 74 0.4222942565878232\n",
      "Epoch 74: val loss 0.583260\n",
      "\n",
      "Epoch %d: train loss %f 75 0.42602023233970004\n",
      "Epoch 75: val loss 0.545056\n",
      "\n",
      "Epoch %d: train loss %f 76 0.4545124222834905\n",
      "Epoch 76: val loss 0.565628\n",
      "\n",
      "Epoch %d: train loss %f 77 0.41569291551907855\n",
      "Epoch 77: val loss 0.586031\n",
      "\n",
      "Epoch %d: train loss %f 78 0.4151276672879855\n",
      "Epoch 78: val loss 0.573321\n",
      "\n",
      "Epoch %d: train loss %f 79 0.40382522344589233\n",
      "Epoch 79: val loss 0.562171\n",
      "\n",
      "Epoch %d: train loss %f 80 0.45819392303625744\n",
      "Epoch 80: val loss 0.590547\n",
      "\n",
      "Epoch %d: train loss %f 81 0.39633610596259433\n",
      "Epoch 81: val loss 0.560155\n",
      "\n",
      "Epoch %d: train loss %f 82 0.42404955873886746\n",
      "Epoch 82: val loss 0.605718\n",
      "\n",
      "Epoch %d: train loss %f 83 0.41313257813453674\n",
      "Epoch 83: val loss 0.540785\n",
      "\n",
      "Epoch %d: train loss %f 84 0.40087543924649555\n",
      "Epoch 84: val loss 0.582329\n",
      "\n",
      "Epoch %d: train loss %f 85 0.425673005481561\n",
      "Epoch 85: val loss 0.560072\n",
      "\n",
      "Epoch %d: train loss %f 86 0.41892892370621365\n",
      "Epoch 86: val loss 0.599932\n",
      "\n",
      "Epoch %d: train loss %f 87 0.4002249464392662\n",
      "Epoch 87: val loss 0.565141\n",
      "\n",
      "Epoch %d: train loss %f 88 0.41688243051369983\n",
      "Epoch 88: val loss 0.578047\n",
      "\n",
      "Epoch %d: train loss %f 89 0.4055905044078827\n",
      "Epoch 89: val loss 0.559636\n",
      "\n",
      "Epoch %d: train loss %f 90 0.4062575201193492\n",
      "Epoch 90: val loss 0.570298\n",
      "\n",
      "Epoch %d: train loss %f 91 0.3848712245623271\n",
      "Epoch 91: val loss 0.579943\n",
      "\n",
      "Epoch %d: train loss %f 92 0.37867550055185956\n",
      "Epoch 92: val loss 0.582102\n",
      "\n",
      "Epoch %d: train loss %f 93 0.3985467255115509\n",
      "Epoch 93: val loss 0.590778\n",
      "\n",
      "Epoch %d: train loss %f 94 0.38254381716251373\n",
      "Epoch 94: val loss 0.593334\n",
      "\n",
      "Epoch %d: train loss %f 95 0.42284482220808667\n",
      "Epoch 95: val loss 0.582311\n",
      "\n",
      "Epoch %d: train loss %f 96 0.3913489952683449\n",
      "Epoch 96: val loss 0.578543\n",
      "\n",
      "Epoch %d: train loss %f 97 0.39942646274964017\n",
      "Epoch 97: val loss 0.621781\n",
      "\n",
      "Epoch %d: train loss %f 98 0.3921925475200017\n",
      "Epoch 98: val loss 0.589003\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3822696829835574\n",
      "Epoch 99: val loss 0.637068\n",
      "\n",
      "Epoch %d: train loss %f 100 0.39346731702486676\n",
      "Epoch 100: val loss 0.578073\n",
      "\n",
      "Epoch %d: train loss %f 101 0.3981212116777897\n",
      "Epoch 101: val loss 0.619333\n",
      "\n",
      "Epoch %d: train loss %f 102 0.37260854865113896\n",
      "Epoch 102: val loss 0.580833\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3945273620386918\n",
      "Epoch 103: val loss 0.596022\n",
      "\n",
      "Epoch %d: train loss %f 104 0.39033997307221097\n",
      "Epoch 104: val loss 0.629958\n",
      "\n",
      "Epoch %d: train loss %f 105 0.3999980067213376\n",
      "Epoch 105: val loss 0.588326\n",
      "\n",
      "Epoch %d: train loss %f 106 0.4048191321392854\n",
      "Epoch 106: val loss 0.608220\n",
      "\n",
      "Epoch %d: train loss %f 107 0.3921460037430127\n",
      "Epoch 107: val loss 0.591240\n",
      "\n",
      "Epoch %d: train loss %f 108 0.38300597046812374\n",
      "Epoch 108: val loss 0.602272\n",
      "\n",
      "Epoch %d: train loss %f 109 0.3793797269463539\n",
      "Epoch 109: val loss 0.600992\n",
      "\n",
      "Epoch %d: train loss %f 110 0.3935657913486163\n",
      "Epoch 110: val loss 0.618504\n",
      "\n",
      "Epoch %d: train loss %f 111 0.41810552030801773\n",
      "Epoch 111: val loss 0.619410\n",
      "\n",
      "Epoch %d: train loss %f 112 0.39844832321008045\n",
      "Epoch 112: val loss 0.586963\n",
      "\n",
      "Epoch %d: train loss %f 113 0.3833141525586446\n",
      "Epoch 113: val loss 0.613166\n",
      "\n",
      "Epoch %d: train loss %f 114 0.3872608269254367\n",
      "Epoch 114: val loss 0.598580\n",
      "\n",
      "Epoch %d: train loss %f 115 0.3865121230483055\n",
      "Epoch 115: val loss 0.614194\n",
      "\n",
      "Epoch %d: train loss %f 116 0.3615277235706647\n",
      "Epoch 116: val loss 0.585477\n",
      "\n",
      "Epoch %d: train loss %f 117 0.40675252427657443\n",
      "Epoch 117: val loss 0.609652\n",
      "\n",
      "Epoch %d: train loss %f 118 0.3565290495753288\n",
      "Epoch 118: val loss 0.593639\n",
      "\n",
      "Epoch %d: train loss %f 119 0.41432418674230576\n",
      "Epoch 119: val loss 0.628971\n",
      "\n",
      "Epoch %d: train loss %f 120 0.3767113871872425\n",
      "Epoch 120: val loss 0.588154\n",
      "\n",
      "Epoch %d: train loss %f 121 0.3540430913368861\n",
      "Epoch 121: val loss 0.647428\n",
      "\n",
      "Epoch %d: train loss %f 122 0.4172748252749443\n",
      "Epoch 122: val loss 0.588983\n",
      "\n",
      "Epoch %d: train loss %f 123 0.36335937678813934\n",
      "Epoch 123: val loss 0.610547\n",
      "\n",
      "Epoch %d: train loss %f 124 0.3532783513267835\n",
      "Epoch 124: val loss 0.584393\n",
      "\n",
      "Epoch %d: train loss %f 125 0.3626236816247304\n",
      "Epoch 125: val loss 0.592108\n",
      "\n",
      "Epoch %d: train loss %f 126 0.33579489464561146\n",
      "Epoch 126: val loss 0.615071\n",
      "\n",
      "Epoch %d: train loss %f 127 0.38464101652304333\n",
      "Epoch 127: val loss 0.592026\n",
      "\n",
      "Epoch %d: train loss %f 128 0.376422422627608\n",
      "Epoch 128: val loss 0.628313\n",
      "\n",
      "Epoch %d: train loss %f 129 0.3744581565260887\n",
      "Epoch 129: val loss 0.620867\n",
      "\n",
      "Epoch %d: train loss %f 130 0.3660932444036007\n",
      "Epoch 130: val loss 0.585501\n",
      "\n",
      "Epoch %d: train loss %f 131 0.41158161063989\n",
      "Epoch 131: val loss 0.631611\n",
      "\n",
      "Epoch %d: train loss %f 132 0.38471316546201706\n",
      "Epoch 132: val loss 0.606617\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3755536178747813\n",
      "Epoch 133: val loss 0.595991\n",
      "\n",
      "Epoch %d: train loss %f 134 0.3684384897351265\n",
      "Epoch 134: val loss 0.604613\n",
      "\n",
      "Epoch %d: train loss %f 135 0.3594393307964007\n",
      "Epoch 135: val loss 0.612403\n",
      "\n",
      "Epoch %d: train loss %f 136 0.35820579901337624\n",
      "Epoch 136: val loss 0.613385\n",
      "\n",
      "Epoch %d: train loss %f 137 0.4036169871687889\n",
      "Epoch 137: val loss 0.679782\n",
      "\n",
      "Epoch %d: train loss %f 138 0.36632516731818515\n",
      "Epoch 138: val loss 0.619921\n",
      "\n",
      "Epoch %d: train loss %f 139 0.36561279992262524\n",
      "Epoch 139: val loss 0.604081\n",
      "\n",
      "Epoch %d: train loss %f 140 0.34959598258137703\n",
      "Epoch 140: val loss 0.608650\n",
      "\n",
      "Epoch %d: train loss %f 141 0.3399825456241767\n",
      "Epoch 141: val loss 0.626980\n",
      "\n",
      "Epoch %d: train loss %f 142 0.35981037964423496\n",
      "Epoch 142: val loss 0.615327\n",
      "\n",
      "Epoch %d: train loss %f 143 0.33842405676841736\n",
      "Epoch 143: val loss 0.664792\n",
      "\n",
      "Epoch %d: train loss %f 144 0.37869395439823467\n",
      "Epoch 144: val loss 0.627307\n",
      "\n",
      "Epoch %d: train loss %f 145 0.3653826030592124\n",
      "Epoch 145: val loss 0.638326\n",
      "\n",
      "Epoch %d: train loss %f 146 0.3646348665157954\n",
      "Epoch 146: val loss 0.619510\n",
      "\n",
      "Epoch %d: train loss %f 147 0.34219524388511974\n",
      "Epoch 147: val loss 0.610397\n",
      "\n",
      "Epoch %d: train loss %f 148 0.4129335694015026\n",
      "Epoch 148: val loss 0.640689\n",
      "\n",
      "Epoch %d: train loss %f 149 0.3433787574370702\n",
      "Epoch 149: val loss 0.604971\n",
      "\n",
      "Epoch %d: train loss %f 150 0.33721013988057774\n",
      "Epoch 150: val loss 0.619421\n",
      "\n",
      "Epoch %d: train loss %f 151 0.31763337800900143\n",
      "Epoch 151: val loss 0.616944\n",
      "\n",
      "Epoch %d: train loss %f 152 0.3320832997560501\n",
      "Epoch 152: val loss 0.640946\n",
      "\n",
      "Epoch %d: train loss %f 153 0.33733803654710454\n",
      "Epoch 153: val loss 0.609800\n",
      "\n",
      "Epoch %d: train loss %f 154 0.3484517199297746\n",
      "Epoch 154: val loss 0.653206\n",
      "\n",
      "Epoch %d: train loss %f 155 0.33191963161031407\n",
      "Epoch 155: val loss 0.628580\n",
      "\n",
      "Epoch %d: train loss %f 156 0.3682730980217457\n",
      "Epoch 156: val loss 0.638060\n",
      "\n",
      "Epoch %d: train loss %f 157 0.3490196814139684\n",
      "Epoch 157: val loss 0.635163\n",
      "\n",
      "Epoch %d: train loss %f 158 0.36535316084822017\n",
      "Epoch 158: val loss 0.622159\n",
      "\n",
      "Epoch %d: train loss %f 159 0.34296637028455734\n",
      "Epoch 159: val loss 0.645855\n",
      "\n",
      "Epoch %d: train loss %f 160 0.3440178992847602\n",
      "Epoch 160: val loss 0.614776\n",
      "\n",
      "Epoch %d: train loss %f 161 0.31643958141406375\n",
      "Epoch 161: val loss 0.665254\n",
      "\n",
      "Epoch %d: train loss %f 162 0.33375955497225124\n",
      "Epoch 162: val loss 0.638129\n",
      "\n",
      "Epoch %d: train loss %f 163 0.34457487612962723\n",
      "Epoch 163: val loss 0.643824\n",
      "\n",
      "Epoch %d: train loss %f 164 0.38220925504962605\n",
      "Epoch 164: val loss 0.629470\n",
      "\n",
      "Epoch %d: train loss %f 165 0.34634220351775485\n",
      "Epoch 165: val loss 0.641285\n",
      "\n",
      "Epoch %d: train loss %f 166 0.3644808369378249\n",
      "Epoch 166: val loss 0.617026\n",
      "\n",
      "Epoch %d: train loss %f 167 0.3405364342033863\n",
      "Epoch 167: val loss 0.641284\n",
      "\n",
      "Epoch %d: train loss %f 168 0.3196948654949665\n",
      "Epoch 168: val loss 0.653972\n",
      "\n",
      "Epoch %d: train loss %f 169 0.3463532601793607\n",
      "Epoch 169: val loss 0.665901\n",
      "\n",
      "Epoch %d: train loss %f 170 0.34657613188028336\n",
      "Epoch 170: val loss 0.649278\n",
      "\n",
      "Epoch %d: train loss %f 171 0.31724340096116066\n",
      "Epoch 171: val loss 0.671159\n",
      "\n",
      "Epoch %d: train loss %f 172 0.3501819173494975\n",
      "Epoch 172: val loss 0.640564\n",
      "\n",
      "Epoch %d: train loss %f 173 0.326813622067372\n",
      "Epoch 173: val loss 0.655033\n",
      "\n",
      "Epoch %d: train loss %f 174 0.311822393288215\n",
      "Epoch 174: val loss 0.699274\n",
      "\n",
      "Epoch %d: train loss %f 175 0.3592236911257108\n",
      "Epoch 175: val loss 0.657324\n",
      "\n",
      "Epoch %d: train loss %f 176 0.32905300209919613\n",
      "Epoch 176: val loss 0.663703\n",
      "\n",
      "Epoch %d: train loss %f 177 0.3289643240471681\n",
      "Epoch 177: val loss 0.659482\n",
      "\n",
      "Epoch %d: train loss %f 178 0.3037664083143075\n",
      "Epoch 178: val loss 0.682342\n",
      "\n",
      "Epoch %d: train loss %f 179 0.33114999160170555\n",
      "Epoch 179: val loss 0.668261\n",
      "\n",
      "Epoch %d: train loss %f 180 0.32732510566711426\n",
      "Epoch 180: val loss 0.687322\n",
      "\n",
      "Epoch %d: train loss %f 181 0.3384268494943778\n",
      "Epoch 181: val loss 0.668695\n",
      "\n",
      "Epoch %d: train loss %f 182 0.3250098129113515\n",
      "Epoch 182: val loss 0.660704\n",
      "\n",
      "Epoch %d: train loss %f 183 0.3585760009785493\n",
      "Epoch 183: val loss 0.646420\n",
      "\n",
      "Epoch %d: train loss %f 184 0.3173363109429677\n",
      "Epoch 184: val loss 0.638038\n",
      "\n",
      "Epoch %d: train loss %f 185 0.3037172456582387\n",
      "Epoch 185: val loss 0.684739\n",
      "\n",
      "Epoch %d: train loss %f 186 0.35299105818072957\n",
      "Epoch 186: val loss 0.678943\n",
      "\n",
      "Epoch %d: train loss %f 187 0.36049820606907207\n",
      "Epoch 187: val loss 0.693477\n",
      "\n",
      "Epoch %d: train loss %f 188 0.3172518437107404\n",
      "Epoch 188: val loss 0.673608\n",
      "\n",
      "Epoch %d: train loss %f 189 0.35812580461303395\n",
      "Epoch 189: val loss 0.655314\n",
      "\n",
      "Epoch %d: train loss %f 190 0.32557614892721176\n",
      "Epoch 190: val loss 0.697159\n",
      "\n",
      "Epoch %d: train loss %f 191 0.31552429621418315\n",
      "Epoch 191: val loss 0.688373\n",
      "\n",
      "Epoch %d: train loss %f 192 0.301211620370547\n",
      "Epoch 192: val loss 0.666859\n",
      "\n",
      "Epoch %d: train loss %f 193 0.33629490559299785\n",
      "Epoch 193: val loss 0.681156\n",
      "\n",
      "Epoch %d: train loss %f 194 0.31393983711798984\n",
      "Epoch 194: val loss 0.659460\n",
      "\n",
      "Epoch %d: train loss %f 195 0.3511242891351382\n",
      "Epoch 195: val loss 0.680054\n",
      "\n",
      "Epoch %d: train loss %f 196 0.32293011372288066\n",
      "Epoch 196: val loss 0.672729\n",
      "\n",
      "Epoch %d: train loss %f 197 0.2654972566912572\n",
      "Epoch 197: val loss 0.691363\n",
      "\n",
      "Epoch %d: train loss %f 198 0.35164449488123256\n",
      "Epoch 198: val loss 0.695629\n",
      "\n",
      "Epoch %d: train loss %f 199 0.3107505850493908\n",
      "Epoch 199: val loss 0.670988\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6928335378567377\n",
      "Epoch 0: val loss 0.692909\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6878691414992014\n",
      "Epoch 1: val loss 0.690726\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6778748581806818\n",
      "Epoch 2: val loss 0.683160\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6627184102932612\n",
      "Epoch 3: val loss 0.659872\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6294133613506953\n",
      "Epoch 4: val loss 0.628115\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6086485683917999\n",
      "Epoch 5: val loss 0.619910\n",
      "\n",
      "Epoch %d: train loss %f 6 0.5864414349198341\n",
      "Epoch 6: val loss 0.616625\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5881850719451904\n",
      "Epoch 7: val loss 0.611215\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5812699422240257\n",
      "Epoch 8: val loss 0.614823\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5753147974610329\n",
      "Epoch 9: val loss 0.608726\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5567153717080752\n",
      "Epoch 10: val loss 0.608389\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5602378522356352\n",
      "Epoch 11: val loss 0.603484\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5482551281650861\n",
      "Epoch 12: val loss 0.605052\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5613583748539289\n",
      "Epoch 13: val loss 0.610717\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5537148912747701\n",
      "Epoch 14: val loss 0.597855\n",
      "\n",
      "Epoch %d: train loss %f 15 0.536128560702006\n",
      "Epoch 15: val loss 0.597145\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5133825987577438\n",
      "Epoch 16: val loss 0.610429\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5258879686395327\n",
      "Epoch 17: val loss 0.600150\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5287718797723452\n",
      "Epoch 18: val loss 0.608828\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5234724978605906\n",
      "Epoch 19: val loss 0.604333\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5142601331075033\n",
      "Epoch 20: val loss 0.599720\n",
      "\n",
      "Epoch %d: train loss %f 21 0.48965561389923096\n",
      "Epoch 21: val loss 0.601628\n",
      "\n",
      "Epoch %d: train loss %f 22 0.493761104842027\n",
      "Epoch 22: val loss 0.590958\n",
      "\n",
      "Epoch %d: train loss %f 23 0.4918016890684764\n",
      "Epoch 23: val loss 0.592620\n",
      "\n",
      "Epoch %d: train loss %f 24 0.48724007854859036\n",
      "Epoch 24: val loss 0.606609\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4911491746703784\n",
      "Epoch 25: val loss 0.586251\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4722545345624288\n",
      "Epoch 26: val loss 0.606657\n",
      "\n",
      "Epoch %d: train loss %f 27 0.49167395134766895\n",
      "Epoch 27: val loss 0.597592\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4757234180967013\n",
      "Epoch 28: val loss 0.590075\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4787935217221578\n",
      "Epoch 29: val loss 0.569589\n",
      "\n",
      "Epoch %d: train loss %f 30 0.4752218797802925\n",
      "Epoch 30: val loss 0.583377\n",
      "\n",
      "Epoch %d: train loss %f 31 0.46501801908016205\n",
      "Epoch 31: val loss 0.584894\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4407593831419945\n",
      "Epoch 32: val loss 0.591189\n",
      "\n",
      "Epoch %d: train loss %f 33 0.46186261127392453\n",
      "Epoch 33: val loss 0.603271\n",
      "\n",
      "Epoch %d: train loss %f 34 0.4699396938085556\n",
      "Epoch 34: val loss 0.585256\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4570469856262207\n",
      "Epoch 35: val loss 0.576410\n",
      "\n",
      "Epoch %d: train loss %f 36 0.4424006740252177\n",
      "Epoch 36: val loss 0.569552\n",
      "\n",
      "Epoch %d: train loss %f 37 0.4422898267706235\n",
      "Epoch 37: val loss 0.574803\n",
      "\n",
      "Epoch %d: train loss %f 38 0.46412167449792224\n",
      "Epoch 38: val loss 0.593083\n",
      "\n",
      "Epoch %d: train loss %f 39 0.44213351607322693\n",
      "Epoch 39: val loss 0.579340\n",
      "\n",
      "Epoch %d: train loss %f 40 0.46322035044431686\n",
      "Epoch 40: val loss 0.576881\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4388221750656764\n",
      "Epoch 41: val loss 0.573910\n",
      "\n",
      "Epoch %d: train loss %f 42 0.41887354850769043\n",
      "Epoch 42: val loss 0.572460\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4566447585821152\n",
      "Epoch 43: val loss 0.583679\n",
      "\n",
      "Epoch %d: train loss %f 44 0.4342336754004161\n",
      "Epoch 44: val loss 0.585396\n",
      "\n",
      "Epoch %d: train loss %f 45 0.45754751314719516\n",
      "Epoch 45: val loss 0.571857\n",
      "\n",
      "Epoch %d: train loss %f 46 0.4254678909977277\n",
      "Epoch 46: val loss 0.601773\n",
      "\n",
      "Epoch %d: train loss %f 47 0.4479585786660512\n",
      "Epoch 47: val loss 0.593535\n",
      "\n",
      "Epoch %d: train loss %f 48 0.42498794694741565\n",
      "Epoch 48: val loss 0.572008\n",
      "\n",
      "Epoch %d: train loss %f 49 0.43998705223202705\n",
      "Epoch 49: val loss 0.592341\n",
      "\n",
      "Epoch %d: train loss %f 50 0.4397707059979439\n",
      "Epoch 50: val loss 0.603374\n",
      "\n",
      "Epoch %d: train loss %f 51 0.40725618849198025\n",
      "Epoch 51: val loss 0.616551\n",
      "\n",
      "Epoch %d: train loss %f 52 0.4108622993032138\n",
      "Epoch 52: val loss 0.612540\n",
      "\n",
      "Epoch %d: train loss %f 53 0.4297548606991768\n",
      "Epoch 53: val loss 0.589000\n",
      "\n",
      "Epoch %d: train loss %f 54 0.4274862731496493\n",
      "Epoch 54: val loss 0.608359\n",
      "\n",
      "Epoch %d: train loss %f 55 0.4117133567730586\n",
      "Epoch 55: val loss 0.593317\n",
      "\n",
      "Epoch %d: train loss %f 56 0.42827849338452023\n",
      "Epoch 56: val loss 0.608440\n",
      "\n",
      "Epoch %d: train loss %f 57 0.42913159479697544\n",
      "Epoch 57: val loss 0.601962\n",
      "\n",
      "Epoch %d: train loss %f 58 0.4064115198949973\n",
      "Epoch 58: val loss 0.599040\n",
      "\n",
      "Epoch %d: train loss %f 59 0.41787245124578476\n",
      "Epoch 59: val loss 0.591605\n",
      "\n",
      "Epoch %d: train loss %f 60 0.40788550426562625\n",
      "Epoch 60: val loss 0.594114\n",
      "\n",
      "Epoch %d: train loss %f 61 0.3929958293835322\n",
      "Epoch 61: val loss 0.590911\n",
      "\n",
      "Epoch %d: train loss %f 62 0.3907889525095622\n",
      "Epoch 62: val loss 0.595731\n",
      "\n",
      "Epoch %d: train loss %f 63 0.4219056194027265\n",
      "Epoch 63: val loss 0.644128\n",
      "\n",
      "Epoch %d: train loss %f 64 0.3942445827027162\n",
      "Epoch 64: val loss 0.605180\n",
      "\n",
      "Epoch %d: train loss %f 65 0.403744213283062\n",
      "Epoch 65: val loss 0.616568\n",
      "\n",
      "Epoch %d: train loss %f 66 0.4074924687544505\n",
      "Epoch 66: val loss 0.617672\n",
      "\n",
      "Epoch %d: train loss %f 67 0.4310514231522878\n",
      "Epoch 67: val loss 0.600596\n",
      "\n",
      "Epoch %d: train loss %f 68 0.42644044508536655\n",
      "Epoch 68: val loss 0.621424\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4183993637561798\n",
      "Epoch 69: val loss 0.611136\n",
      "\n",
      "Epoch %d: train loss %f 70 0.39916277925173443\n",
      "Epoch 70: val loss 0.624980\n",
      "\n",
      "Epoch %d: train loss %f 71 0.4103796606262525\n",
      "Epoch 71: val loss 0.592409\n",
      "\n",
      "Epoch %d: train loss %f 72 0.38821997741858166\n",
      "Epoch 72: val loss 0.616570\n",
      "\n",
      "Epoch %d: train loss %f 73 0.4145197197794914\n",
      "Epoch 73: val loss 0.617635\n",
      "\n",
      "Epoch %d: train loss %f 74 0.39787005136410397\n",
      "Epoch 74: val loss 0.630382\n",
      "\n",
      "Epoch %d: train loss %f 75 0.36931464821100235\n",
      "Epoch 75: val loss 0.625987\n",
      "\n",
      "Epoch %d: train loss %f 76 0.3673011710246404\n",
      "Epoch 76: val loss 0.623751\n",
      "\n",
      "Epoch %d: train loss %f 77 0.38383642584085464\n",
      "Epoch 77: val loss 0.594235\n",
      "\n",
      "Epoch %d: train loss %f 78 0.3750540092587471\n",
      "Epoch 78: val loss 0.598823\n",
      "\n",
      "Epoch %d: train loss %f 79 0.3611799677213033\n",
      "Epoch 79: val loss 0.627154\n",
      "\n",
      "Epoch %d: train loss %f 80 0.39639807616670925\n",
      "Epoch 80: val loss 0.626703\n",
      "\n",
      "Epoch %d: train loss %f 81 0.36282648021976155\n",
      "Epoch 81: val loss 0.604004\n",
      "\n",
      "Epoch %d: train loss %f 82 0.3943371909360091\n",
      "Epoch 82: val loss 0.661996\n",
      "\n",
      "Epoch %d: train loss %f 83 0.38049590587615967\n",
      "Epoch 83: val loss 0.596748\n",
      "\n",
      "Epoch %d: train loss %f 84 0.38941169033447903\n",
      "Epoch 84: val loss 0.617579\n",
      "\n",
      "Epoch %d: train loss %f 85 0.36861276378234226\n",
      "Epoch 85: val loss 0.606915\n",
      "\n",
      "Epoch %d: train loss %f 86 0.39168545852104825\n",
      "Epoch 86: val loss 0.602448\n",
      "\n",
      "Epoch %d: train loss %f 87 0.38539084295431775\n",
      "Epoch 87: val loss 0.620887\n",
      "\n",
      "Epoch %d: train loss %f 88 0.4015846873323123\n",
      "Epoch 88: val loss 0.624548\n",
      "\n",
      "Epoch %d: train loss %f 89 0.3954075003663699\n",
      "Epoch 89: val loss 0.663405\n",
      "\n",
      "Epoch %d: train loss %f 90 0.3729618266224861\n",
      "Epoch 90: val loss 0.638772\n",
      "\n",
      "Epoch %d: train loss %f 91 0.40460426608721417\n",
      "Epoch 91: val loss 0.633887\n",
      "\n",
      "Epoch %d: train loss %f 92 0.3690024676422278\n",
      "Epoch 92: val loss 0.630885\n",
      "\n",
      "Epoch %d: train loss %f 93 0.36364155262708664\n",
      "Epoch 93: val loss 0.635711\n",
      "\n",
      "Epoch %d: train loss %f 94 0.37253690014282864\n",
      "Epoch 94: val loss 0.645217\n",
      "\n",
      "Epoch %d: train loss %f 95 0.3636641092598438\n",
      "Epoch 95: val loss 0.654494\n",
      "\n",
      "Epoch %d: train loss %f 96 0.368216576675574\n",
      "Epoch 96: val loss 0.638721\n",
      "\n",
      "Epoch %d: train loss %f 97 0.38366461793581647\n",
      "Epoch 97: val loss 0.630304\n",
      "\n",
      "Epoch %d: train loss %f 98 0.36176561191678047\n",
      "Epoch 98: val loss 0.653540\n",
      "\n",
      "Epoch %d: train loss %f 99 0.37349944810072583\n",
      "Epoch 99: val loss 0.644942\n",
      "\n",
      "Epoch %d: train loss %f 100 0.35215899720788\n",
      "Epoch 100: val loss 0.642914\n",
      "\n",
      "Epoch %d: train loss %f 101 0.36003229518731433\n",
      "Epoch 101: val loss 0.626447\n",
      "\n",
      "Epoch %d: train loss %f 102 0.39500317598382634\n",
      "Epoch 102: val loss 0.676497\n",
      "\n",
      "Epoch %d: train loss %f 103 0.4354633018374443\n",
      "Epoch 103: val loss 0.642716\n",
      "\n",
      "Epoch %d: train loss %f 104 0.3546151282886664\n",
      "Epoch 104: val loss 0.647125\n",
      "\n",
      "Epoch %d: train loss %f 105 0.3562261362870534\n",
      "Epoch 105: val loss 0.651082\n",
      "\n",
      "Epoch %d: train loss %f 106 0.3750695362687111\n",
      "Epoch 106: val loss 0.651570\n",
      "\n",
      "Epoch %d: train loss %f 107 0.33525950461626053\n",
      "Epoch 107: val loss 0.638668\n",
      "\n",
      "Epoch %d: train loss %f 108 0.38816411296526593\n",
      "Epoch 108: val loss 0.648904\n",
      "\n",
      "Epoch %d: train loss %f 109 0.362601804236571\n",
      "Epoch 109: val loss 0.647370\n",
      "\n",
      "Epoch %d: train loss %f 110 0.3394171744585037\n",
      "Epoch 110: val loss 0.646437\n",
      "\n",
      "Epoch %d: train loss %f 111 0.38127759595712024\n",
      "Epoch 111: val loss 0.614863\n",
      "\n",
      "Epoch %d: train loss %f 112 0.3396279998123646\n",
      "Epoch 112: val loss 0.639074\n",
      "\n",
      "Epoch %d: train loss %f 113 0.3474348187446594\n",
      "Epoch 113: val loss 0.642013\n",
      "\n",
      "Epoch %d: train loss %f 114 0.357519722233216\n",
      "Epoch 114: val loss 0.627713\n",
      "\n",
      "Epoch %d: train loss %f 115 0.3585541769862175\n",
      "Epoch 115: val loss 0.634619\n",
      "\n",
      "Epoch %d: train loss %f 116 0.3669239307443301\n",
      "Epoch 116: val loss 0.642937\n",
      "\n",
      "Epoch %d: train loss %f 117 0.3409016281366348\n",
      "Epoch 117: val loss 0.642327\n",
      "\n",
      "Epoch %d: train loss %f 118 0.3475545719265938\n",
      "Epoch 118: val loss 0.670170\n",
      "\n",
      "Epoch %d: train loss %f 119 0.3442567040522893\n",
      "Epoch 119: val loss 0.655748\n",
      "\n",
      "Epoch %d: train loss %f 120 0.3359683578213056\n",
      "Epoch 120: val loss 0.666558\n",
      "\n",
      "Epoch %d: train loss %f 121 0.3532883549729983\n",
      "Epoch 121: val loss 0.693790\n",
      "\n",
      "Epoch %d: train loss %f 122 0.3755626032749812\n",
      "Epoch 122: val loss 0.669237\n",
      "\n",
      "Epoch %d: train loss %f 123 0.3539350579182307\n",
      "Epoch 123: val loss 0.668674\n",
      "\n",
      "Epoch %d: train loss %f 124 0.3152067835132281\n",
      "Epoch 124: val loss 0.667427\n",
      "\n",
      "Epoch %d: train loss %f 125 0.3483501896262169\n",
      "Epoch 125: val loss 0.676906\n",
      "\n",
      "Epoch %d: train loss %f 126 0.3241792470216751\n",
      "Epoch 126: val loss 0.665563\n",
      "\n",
      "Epoch %d: train loss %f 127 0.3800063853462537\n",
      "Epoch 127: val loss 0.747835\n",
      "\n",
      "Epoch %d: train loss %f 128 0.3316480169693629\n",
      "Epoch 128: val loss 0.724039\n",
      "\n",
      "Epoch %d: train loss %f 129 0.34377963468432426\n",
      "Epoch 129: val loss 0.709174\n",
      "\n",
      "Epoch %d: train loss %f 130 0.33225217709938687\n",
      "Epoch 130: val loss 0.637420\n",
      "\n",
      "Epoch %d: train loss %f 131 0.36772943908969563\n",
      "Epoch 131: val loss 0.669367\n",
      "\n",
      "Epoch %d: train loss %f 132 0.34251010169585544\n",
      "Epoch 132: val loss 0.657098\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3420833262304465\n",
      "Epoch 133: val loss 0.696622\n",
      "\n",
      "Epoch %d: train loss %f 134 0.2987699583172798\n",
      "Epoch 134: val loss 0.670577\n",
      "\n",
      "Epoch %d: train loss %f 135 0.3714703929920991\n",
      "Epoch 135: val loss 0.654136\n",
      "\n",
      "Epoch %d: train loss %f 136 0.30545273050665855\n",
      "Epoch 136: val loss 0.648549\n",
      "\n",
      "Epoch %d: train loss %f 137 0.3038642505804698\n",
      "Epoch 137: val loss 0.652779\n",
      "\n",
      "Epoch %d: train loss %f 138 0.3676573584477107\n",
      "Epoch 138: val loss 0.675331\n",
      "\n",
      "Epoch %d: train loss %f 139 0.3415401726961136\n",
      "Epoch 139: val loss 0.693503\n",
      "\n",
      "Epoch %d: train loss %f 140 0.3049294240772724\n",
      "Epoch 140: val loss 0.726344\n",
      "\n",
      "Epoch %d: train loss %f 141 0.3230520524084568\n",
      "Epoch 141: val loss 0.718799\n",
      "\n",
      "Epoch %d: train loss %f 142 0.3289247403542201\n",
      "Epoch 142: val loss 0.718047\n",
      "\n",
      "Epoch %d: train loss %f 143 0.3490535418192546\n",
      "Epoch 143: val loss 0.702417\n",
      "\n",
      "Epoch %d: train loss %f 144 0.30967315658926964\n",
      "Epoch 144: val loss 0.724745\n",
      "\n",
      "Epoch %d: train loss %f 145 0.37545526400208473\n",
      "Epoch 145: val loss 0.714561\n",
      "\n",
      "Epoch %d: train loss %f 146 0.3433364623536666\n",
      "Epoch 146: val loss 0.726870\n",
      "\n",
      "Epoch %d: train loss %f 147 0.30824004858732224\n",
      "Epoch 147: val loss 0.697699\n",
      "\n",
      "Epoch %d: train loss %f 148 0.326176006346941\n",
      "Epoch 148: val loss 0.694030\n",
      "\n",
      "Epoch %d: train loss %f 149 0.31874751175443333\n",
      "Epoch 149: val loss 0.704371\n",
      "\n",
      "Epoch %d: train loss %f 150 0.3439513680835565\n",
      "Epoch 150: val loss 0.671603\n",
      "\n",
      "Epoch %d: train loss %f 151 0.31326849386096\n",
      "Epoch 151: val loss 0.649641\n",
      "\n",
      "Epoch %d: train loss %f 152 0.2993472218513489\n",
      "Epoch 152: val loss 0.691680\n",
      "\n",
      "Epoch %d: train loss %f 153 0.3343746451040109\n",
      "Epoch 153: val loss 0.696048\n",
      "\n",
      "Epoch %d: train loss %f 154 0.32393528893589973\n",
      "Epoch 154: val loss 0.721246\n",
      "\n",
      "Epoch %d: train loss %f 155 0.3287684681514899\n",
      "Epoch 155: val loss 0.675652\n",
      "\n",
      "Epoch %d: train loss %f 156 0.31917672604322433\n",
      "Epoch 156: val loss 0.665053\n",
      "\n",
      "Epoch %d: train loss %f 157 0.32148593043287593\n",
      "Epoch 157: val loss 0.692921\n",
      "\n",
      "Epoch %d: train loss %f 158 0.32206186403830844\n",
      "Epoch 158: val loss 0.687306\n",
      "\n",
      "Epoch %d: train loss %f 159 0.3431759923696518\n",
      "Epoch 159: val loss 0.702108\n",
      "\n",
      "Epoch %d: train loss %f 160 0.34806256368756294\n",
      "Epoch 160: val loss 0.709503\n",
      "\n",
      "Epoch %d: train loss %f 161 0.31465962529182434\n",
      "Epoch 161: val loss 0.706846\n",
      "\n",
      "Epoch %d: train loss %f 162 0.3085280905167262\n",
      "Epoch 162: val loss 0.733602\n",
      "\n",
      "Epoch %d: train loss %f 163 0.34051746999224025\n",
      "Epoch 163: val loss 0.705912\n",
      "\n",
      "Epoch %d: train loss %f 164 0.32027508815129596\n",
      "Epoch 164: val loss 0.703949\n",
      "\n",
      "Epoch %d: train loss %f 165 0.319364200035731\n",
      "Epoch 165: val loss 0.720674\n",
      "\n",
      "Epoch %d: train loss %f 166 0.33447841679056484\n",
      "Epoch 166: val loss 0.704664\n",
      "\n",
      "Epoch %d: train loss %f 167 0.3235563337802887\n",
      "Epoch 167: val loss 0.702935\n",
      "\n",
      "Epoch %d: train loss %f 168 0.3354648066063722\n",
      "Epoch 168: val loss 0.684720\n",
      "\n",
      "Epoch %d: train loss %f 169 0.2867317882676919\n",
      "Epoch 169: val loss 0.708834\n",
      "\n",
      "Epoch %d: train loss %f 170 0.2907513715326786\n",
      "Epoch 170: val loss 0.711949\n",
      "\n",
      "Epoch %d: train loss %f 171 0.33373529215653736\n",
      "Epoch 171: val loss 0.700219\n",
      "\n",
      "Epoch %d: train loss %f 172 0.2912953806420167\n",
      "Epoch 172: val loss 0.729993\n",
      "\n",
      "Epoch %d: train loss %f 173 0.33817996084690094\n",
      "Epoch 173: val loss 0.786682\n",
      "\n",
      "Epoch %d: train loss %f 174 0.3241513905425866\n",
      "Epoch 174: val loss 0.725854\n",
      "\n",
      "Epoch %d: train loss %f 175 0.34141117831071216\n",
      "Epoch 175: val loss 0.708871\n",
      "\n",
      "Epoch %d: train loss %f 176 0.3291813222070535\n",
      "Epoch 176: val loss 0.684317\n",
      "\n",
      "Epoch %d: train loss %f 177 0.3256237991154194\n",
      "Epoch 177: val loss 0.696659\n",
      "\n",
      "Epoch %d: train loss %f 178 0.30746560047070187\n",
      "Epoch 178: val loss 0.712250\n",
      "\n",
      "Epoch %d: train loss %f 179 0.29888031631708145\n",
      "Epoch 179: val loss 0.705448\n",
      "\n",
      "Epoch %d: train loss %f 180 0.30356817568341893\n",
      "Epoch 180: val loss 0.709151\n",
      "\n",
      "Epoch %d: train loss %f 181 0.33742636690537137\n",
      "Epoch 181: val loss 0.716366\n",
      "\n",
      "Epoch %d: train loss %f 182 0.27887000516057014\n",
      "Epoch 182: val loss 0.715238\n",
      "\n",
      "Epoch %d: train loss %f 183 0.31444355597098667\n",
      "Epoch 183: val loss 0.707025\n",
      "\n",
      "Epoch %d: train loss %f 184 0.2791865902642409\n",
      "Epoch 184: val loss 0.709182\n",
      "\n",
      "Epoch %d: train loss %f 185 0.3310837832589944\n",
      "Epoch 185: val loss 0.689610\n",
      "\n",
      "Epoch %d: train loss %f 186 0.30955137809117633\n",
      "Epoch 186: val loss 0.751813\n",
      "\n",
      "Epoch %d: train loss %f 187 0.28830915441115695\n",
      "Epoch 187: val loss 0.734595\n",
      "\n",
      "Epoch %d: train loss %f 188 0.292190108448267\n",
      "Epoch 188: val loss 0.799922\n",
      "\n",
      "Epoch %d: train loss %f 189 0.3406872662405173\n",
      "Epoch 189: val loss 0.730475\n",
      "\n",
      "Epoch %d: train loss %f 190 0.2881709026793639\n",
      "Epoch 190: val loss 0.710739\n",
      "\n",
      "Epoch %d: train loss %f 191 0.2981172315776348\n",
      "Epoch 191: val loss 0.733384\n",
      "\n",
      "Epoch %d: train loss %f 192 0.28086980370183784\n",
      "Epoch 192: val loss 0.714535\n",
      "\n",
      "Epoch %d: train loss %f 193 0.3094923235476017\n",
      "Epoch 193: val loss 0.727132\n",
      "\n",
      "Epoch %d: train loss %f 194 0.2976275372008483\n",
      "Epoch 194: val loss 0.719575\n",
      "\n",
      "Epoch %d: train loss %f 195 0.31670896833141643\n",
      "Epoch 195: val loss 0.722066\n",
      "\n",
      "Epoch %d: train loss %f 196 0.30501458297173184\n",
      "Epoch 196: val loss 0.713804\n",
      "\n",
      "Epoch %d: train loss %f 197 0.3024829092125098\n",
      "Epoch 197: val loss 0.704982\n",
      "\n",
      "Epoch %d: train loss %f 198 0.29696037371953327\n",
      "Epoch 198: val loss 0.719705\n",
      "\n",
      "Epoch %d: train loss %f 199 0.29318029433488846\n",
      "Epoch 199: val loss 0.733950\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6925774589180946\n",
      "Epoch 0: val loss 0.692567\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6885990612208843\n",
      "Epoch 1: val loss 0.691574\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6819456405937672\n",
      "Epoch 2: val loss 0.685914\n",
      "\n",
      "Epoch %d: train loss %f 3 0.668212067335844\n",
      "Epoch 3: val loss 0.658976\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6294752880930901\n",
      "Epoch 4: val loss 0.637408\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6012388374656439\n",
      "Epoch 5: val loss 0.593355\n",
      "\n",
      "Epoch %d: train loss %f 6 0.5713719576597214\n",
      "Epoch 6: val loss 0.611854\n",
      "\n",
      "Epoch %d: train loss %f 7 0.5774644613265991\n",
      "Epoch 7: val loss 0.583547\n",
      "\n",
      "Epoch %d: train loss %f 8 0.5772058479487896\n",
      "Epoch 8: val loss 0.592760\n",
      "\n",
      "Epoch %d: train loss %f 9 0.5619617458432913\n",
      "Epoch 9: val loss 0.587530\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5627983175218105\n",
      "Epoch 10: val loss 0.589540\n",
      "\n",
      "Epoch %d: train loss %f 11 0.5443120207637548\n",
      "Epoch 11: val loss 0.580870\n",
      "\n",
      "Epoch %d: train loss %f 12 0.538914643228054\n",
      "Epoch 12: val loss 0.574584\n",
      "\n",
      "Epoch %d: train loss %f 13 0.527349142357707\n",
      "Epoch 13: val loss 0.617099\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5131455883383751\n",
      "Epoch 14: val loss 0.572424\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5076837185770273\n",
      "Epoch 15: val loss 0.586547\n",
      "\n",
      "Epoch %d: train loss %f 16 0.49510172940790653\n",
      "Epoch 16: val loss 0.587850\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5037709791213274\n",
      "Epoch 17: val loss 0.581663\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5065503977239132\n",
      "Epoch 18: val loss 0.582294\n",
      "\n",
      "Epoch %d: train loss %f 19 0.519529590383172\n",
      "Epoch 19: val loss 0.564967\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5001439712941647\n",
      "Epoch 20: val loss 0.579207\n",
      "\n",
      "Epoch %d: train loss %f 21 0.4916126597672701\n",
      "Epoch 21: val loss 0.576727\n",
      "\n",
      "Epoch %d: train loss %f 22 0.4808815438300371\n",
      "Epoch 22: val loss 0.578940\n",
      "\n",
      "Epoch %d: train loss %f 23 0.4919165354222059\n",
      "Epoch 23: val loss 0.560231\n",
      "\n",
      "Epoch %d: train loss %f 24 0.4902924634516239\n",
      "Epoch 24: val loss 0.563029\n",
      "\n",
      "Epoch %d: train loss %f 25 0.4891465287655592\n",
      "Epoch 25: val loss 0.564923\n",
      "\n",
      "Epoch %d: train loss %f 26 0.4859443362802267\n",
      "Epoch 26: val loss 0.571416\n",
      "\n",
      "Epoch %d: train loss %f 27 0.4804230146110058\n",
      "Epoch 27: val loss 0.565275\n",
      "\n",
      "Epoch %d: train loss %f 28 0.4749562907963991\n",
      "Epoch 28: val loss 0.562256\n",
      "\n",
      "Epoch %d: train loss %f 29 0.4670232404023409\n",
      "Epoch 29: val loss 0.590408\n",
      "\n",
      "Epoch %d: train loss %f 30 0.47978021390736103\n",
      "Epoch 30: val loss 0.557590\n",
      "\n",
      "Epoch %d: train loss %f 31 0.467864403501153\n",
      "Epoch 31: val loss 0.557715\n",
      "\n",
      "Epoch %d: train loss %f 32 0.4754620660096407\n",
      "Epoch 32: val loss 0.586140\n",
      "\n",
      "Epoch %d: train loss %f 33 0.4596127327531576\n",
      "Epoch 33: val loss 0.547112\n",
      "\n",
      "Epoch %d: train loss %f 34 0.46943535283207893\n",
      "Epoch 34: val loss 0.584341\n",
      "\n",
      "Epoch %d: train loss %f 35 0.4688519053161144\n",
      "Epoch 35: val loss 0.559095\n",
      "\n",
      "Epoch %d: train loss %f 36 0.48185318894684315\n",
      "Epoch 36: val loss 0.571230\n",
      "\n",
      "Epoch %d: train loss %f 37 0.44761644676327705\n",
      "Epoch 37: val loss 0.550553\n",
      "\n",
      "Epoch %d: train loss %f 38 0.4603915587067604\n",
      "Epoch 38: val loss 0.556620\n",
      "\n",
      "Epoch %d: train loss %f 39 0.4721506740897894\n",
      "Epoch 39: val loss 0.558653\n",
      "\n",
      "Epoch %d: train loss %f 40 0.44195964373648167\n",
      "Epoch 40: val loss 0.551469\n",
      "\n",
      "Epoch %d: train loss %f 41 0.4509800113737583\n",
      "Epoch 41: val loss 0.562231\n",
      "\n",
      "Epoch %d: train loss %f 42 0.4436329808086157\n",
      "Epoch 42: val loss 0.567910\n",
      "\n",
      "Epoch %d: train loss %f 43 0.4617721978574991\n",
      "Epoch 43: val loss 0.552971\n",
      "\n",
      "Epoch %d: train loss %f 44 0.46172034926712513\n",
      "Epoch 44: val loss 0.591607\n",
      "\n",
      "Epoch %d: train loss %f 45 0.45433027669787407\n",
      "Epoch 45: val loss 0.549590\n",
      "\n",
      "Epoch %d: train loss %f 46 0.44123324006795883\n",
      "Epoch 46: val loss 0.544407\n",
      "\n",
      "Epoch %d: train loss %f 47 0.44952845573425293\n",
      "Epoch 47: val loss 0.572568\n",
      "\n",
      "Epoch %d: train loss %f 48 0.43945492058992386\n",
      "Epoch 48: val loss 0.538527\n",
      "\n",
      "Epoch %d: train loss %f 49 0.41802780982106924\n",
      "Epoch 49: val loss 0.558699\n",
      "\n",
      "Epoch %d: train loss %f 50 0.44593787007033825\n",
      "Epoch 50: val loss 0.544567\n",
      "\n",
      "Epoch %d: train loss %f 51 0.4489658623933792\n",
      "Epoch 51: val loss 0.534168\n",
      "\n",
      "Epoch %d: train loss %f 52 0.44847219809889793\n",
      "Epoch 52: val loss 0.555134\n",
      "\n",
      "Epoch %d: train loss %f 53 0.44036402739584446\n",
      "Epoch 53: val loss 0.548353\n",
      "\n",
      "Epoch %d: train loss %f 54 0.42047863733023405\n",
      "Epoch 54: val loss 0.568696\n",
      "\n",
      "Epoch %d: train loss %f 55 0.4556785672903061\n",
      "Epoch 55: val loss 0.575713\n",
      "\n",
      "Epoch %d: train loss %f 56 0.42934804782271385\n",
      "Epoch 56: val loss 0.553933\n",
      "\n",
      "Epoch %d: train loss %f 57 0.44285355508327484\n",
      "Epoch 57: val loss 0.563013\n",
      "\n",
      "Epoch %d: train loss %f 58 0.4430943299084902\n",
      "Epoch 58: val loss 0.560096\n",
      "\n",
      "Epoch %d: train loss %f 59 0.4146183794364333\n",
      "Epoch 59: val loss 0.577935\n",
      "\n",
      "Epoch %d: train loss %f 60 0.4098506346344948\n",
      "Epoch 60: val loss 0.550971\n",
      "\n",
      "Epoch %d: train loss %f 61 0.4360543880611658\n",
      "Epoch 61: val loss 0.584316\n",
      "\n",
      "Epoch %d: train loss %f 62 0.42255845852196217\n",
      "Epoch 62: val loss 0.584315\n",
      "\n",
      "Epoch %d: train loss %f 63 0.41918379068374634\n",
      "Epoch 63: val loss 0.592226\n",
      "\n",
      "Epoch %d: train loss %f 64 0.4267397504299879\n",
      "Epoch 64: val loss 0.588189\n",
      "\n",
      "Epoch %d: train loss %f 65 0.4530685022473335\n",
      "Epoch 65: val loss 0.592124\n",
      "\n",
      "Epoch %d: train loss %f 66 0.4064400512725115\n",
      "Epoch 66: val loss 0.599190\n",
      "\n",
      "Epoch %d: train loss %f 67 0.44960073195397854\n",
      "Epoch 67: val loss 0.555454\n",
      "\n",
      "Epoch %d: train loss %f 68 0.41810994409024715\n",
      "Epoch 68: val loss 0.587944\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4205826986581087\n",
      "Epoch 69: val loss 0.563875\n",
      "\n",
      "Epoch %d: train loss %f 70 0.41331591829657555\n",
      "Epoch 70: val loss 0.585413\n",
      "\n",
      "Epoch %d: train loss %f 71 0.4267761204391718\n",
      "Epoch 71: val loss 0.583648\n",
      "\n",
      "Epoch %d: train loss %f 72 0.41152513585984707\n",
      "Epoch 72: val loss 0.587083\n",
      "\n",
      "Epoch %d: train loss %f 73 0.41305091604590416\n",
      "Epoch 73: val loss 0.564242\n",
      "\n",
      "Epoch %d: train loss %f 74 0.4054388739168644\n",
      "Epoch 74: val loss 0.583791\n",
      "\n",
      "Epoch %d: train loss %f 75 0.3958447836339474\n",
      "Epoch 75: val loss 0.588075\n",
      "\n",
      "Epoch %d: train loss %f 76 0.3889720384031534\n",
      "Epoch 76: val loss 0.571743\n",
      "\n",
      "Epoch %d: train loss %f 77 0.40569863095879555\n",
      "Epoch 77: val loss 0.597624\n",
      "\n",
      "Epoch %d: train loss %f 78 0.4242639299482107\n",
      "Epoch 78: val loss 0.586294\n",
      "\n",
      "Epoch %d: train loss %f 79 0.4046621238812804\n",
      "Epoch 79: val loss 0.580817\n",
      "\n",
      "Epoch %d: train loss %f 80 0.43588106893002987\n",
      "Epoch 80: val loss 0.579137\n",
      "\n",
      "Epoch %d: train loss %f 81 0.43201665207743645\n",
      "Epoch 81: val loss 0.574404\n",
      "\n",
      "Epoch %d: train loss %f 82 0.4117668326944113\n",
      "Epoch 82: val loss 0.610148\n",
      "\n",
      "Epoch %d: train loss %f 83 0.4165804563090205\n",
      "Epoch 83: val loss 0.577721\n",
      "\n",
      "Epoch %d: train loss %f 84 0.41648899391293526\n",
      "Epoch 84: val loss 0.573015\n",
      "\n",
      "Epoch %d: train loss %f 85 0.4099732078611851\n",
      "Epoch 85: val loss 0.580577\n",
      "\n",
      "Epoch %d: train loss %f 86 0.42038370855152607\n",
      "Epoch 86: val loss 0.588050\n",
      "\n",
      "Epoch %d: train loss %f 87 0.3976362245157361\n",
      "Epoch 87: val loss 0.583509\n",
      "\n",
      "Epoch %d: train loss %f 88 0.3931431844830513\n",
      "Epoch 88: val loss 0.563459\n",
      "\n",
      "Epoch %d: train loss %f 89 0.41341938078403473\n",
      "Epoch 89: val loss 0.570197\n",
      "\n",
      "Epoch %d: train loss %f 90 0.3979895990341902\n",
      "Epoch 90: val loss 0.578656\n",
      "\n",
      "Epoch %d: train loss %f 91 0.39341017603874207\n",
      "Epoch 91: val loss 0.582922\n",
      "\n",
      "Epoch %d: train loss %f 92 0.39605590142309666\n",
      "Epoch 92: val loss 0.608864\n",
      "\n",
      "Epoch %d: train loss %f 93 0.42714451160281897\n",
      "Epoch 93: val loss 0.605499\n",
      "\n",
      "Epoch %d: train loss %f 94 0.43803444504737854\n",
      "Epoch 94: val loss 0.575344\n",
      "\n",
      "Epoch %d: train loss %f 95 0.39118912257254124\n",
      "Epoch 95: val loss 0.575425\n",
      "\n",
      "Epoch %d: train loss %f 96 0.38302321918308735\n",
      "Epoch 96: val loss 0.576651\n",
      "\n",
      "Epoch %d: train loss %f 97 0.39621955528855324\n",
      "Epoch 97: val loss 0.589643\n",
      "\n",
      "Epoch %d: train loss %f 98 0.38728369399905205\n",
      "Epoch 98: val loss 0.576320\n",
      "\n",
      "Epoch %d: train loss %f 99 0.3735889922827482\n",
      "Epoch 99: val loss 0.599314\n",
      "\n",
      "Epoch %d: train loss %f 100 0.38330122642219067\n",
      "Epoch 100: val loss 0.585974\n",
      "\n",
      "Epoch %d: train loss %f 101 0.3827196341007948\n",
      "Epoch 101: val loss 0.589833\n",
      "\n",
      "Epoch %d: train loss %f 102 0.396396042779088\n",
      "Epoch 102: val loss 0.604407\n",
      "\n",
      "Epoch %d: train loss %f 103 0.3998582400381565\n",
      "Epoch 103: val loss 0.574636\n",
      "\n",
      "Epoch %d: train loss %f 104 0.3934875149279833\n",
      "Epoch 104: val loss 0.630895\n",
      "\n",
      "Epoch %d: train loss %f 105 0.39788574166595936\n",
      "Epoch 105: val loss 0.580885\n",
      "\n",
      "Epoch %d: train loss %f 106 0.4025585036724806\n",
      "Epoch 106: val loss 0.599160\n",
      "\n",
      "Epoch %d: train loss %f 107 0.40548294223845005\n",
      "Epoch 107: val loss 0.579822\n",
      "\n",
      "Epoch %d: train loss %f 108 0.39491856284439564\n",
      "Epoch 108: val loss 0.577767\n",
      "\n",
      "Epoch %d: train loss %f 109 0.37860035989433527\n",
      "Epoch 109: val loss 0.598107\n",
      "\n",
      "Epoch %d: train loss %f 110 0.38954075891524553\n",
      "Epoch 110: val loss 0.589718\n",
      "\n",
      "Epoch %d: train loss %f 111 0.3949554245918989\n",
      "Epoch 111: val loss 0.589749\n",
      "\n",
      "Epoch %d: train loss %f 112 0.4094173274934292\n",
      "Epoch 112: val loss 0.610181\n",
      "\n",
      "Epoch %d: train loss %f 113 0.39465815667063\n",
      "Epoch 113: val loss 0.613842\n",
      "\n",
      "Epoch %d: train loss %f 114 0.4041043920442462\n",
      "Epoch 114: val loss 0.574838\n",
      "\n",
      "Epoch %d: train loss %f 115 0.3869217624887824\n",
      "Epoch 115: val loss 0.578296\n",
      "\n",
      "Epoch %d: train loss %f 116 0.3852279856801033\n",
      "Epoch 116: val loss 0.575969\n",
      "\n",
      "Epoch %d: train loss %f 117 0.39182164799422026\n",
      "Epoch 117: val loss 0.583832\n",
      "\n",
      "Epoch %d: train loss %f 118 0.41149179078638554\n",
      "Epoch 118: val loss 0.595786\n",
      "\n",
      "Epoch %d: train loss %f 119 0.36454668920487165\n",
      "Epoch 119: val loss 0.593316\n",
      "\n",
      "Epoch %d: train loss %f 120 0.388565493747592\n",
      "Epoch 120: val loss 0.591737\n",
      "\n",
      "Epoch %d: train loss %f 121 0.3598715737462044\n",
      "Epoch 121: val loss 0.605778\n",
      "\n",
      "Epoch %d: train loss %f 122 0.37762244418263435\n",
      "Epoch 122: val loss 0.600341\n",
      "\n",
      "Epoch %d: train loss %f 123 0.3752708435058594\n",
      "Epoch 123: val loss 0.617993\n",
      "\n",
      "Epoch %d: train loss %f 124 0.3734648358076811\n",
      "Epoch 124: val loss 0.615628\n",
      "\n",
      "Epoch %d: train loss %f 125 0.34784995950758457\n",
      "Epoch 125: val loss 0.619546\n",
      "\n",
      "Epoch %d: train loss %f 126 0.39148304145783186\n",
      "Epoch 126: val loss 0.596923\n",
      "\n",
      "Epoch %d: train loss %f 127 0.4031793288886547\n",
      "Epoch 127: val loss 0.590589\n",
      "\n",
      "Epoch %d: train loss %f 128 0.3882401417940855\n",
      "Epoch 128: val loss 0.604439\n",
      "\n",
      "Epoch %d: train loss %f 129 0.37155916169285774\n",
      "Epoch 129: val loss 0.587056\n",
      "\n",
      "Epoch %d: train loss %f 130 0.35966415982693434\n",
      "Epoch 130: val loss 0.600625\n",
      "\n",
      "Epoch %d: train loss %f 131 0.3668544553220272\n",
      "Epoch 131: val loss 0.610934\n",
      "\n",
      "Epoch %d: train loss %f 132 0.3488511713221669\n",
      "Epoch 132: val loss 0.619930\n",
      "\n",
      "Epoch %d: train loss %f 133 0.38107809517532587\n",
      "Epoch 133: val loss 0.611766\n",
      "\n",
      "Epoch %d: train loss %f 134 0.39128830283880234\n",
      "Epoch 134: val loss 0.605386\n",
      "\n",
      "Epoch %d: train loss %f 135 0.3323607351630926\n",
      "Epoch 135: val loss 0.593281\n",
      "\n",
      "Epoch %d: train loss %f 136 0.37481018900871277\n",
      "Epoch 136: val loss 0.587560\n",
      "\n",
      "Epoch %d: train loss %f 137 0.3745727464556694\n",
      "Epoch 137: val loss 0.603827\n",
      "\n",
      "Epoch %d: train loss %f 138 0.367181614972651\n",
      "Epoch 138: val loss 0.626742\n",
      "\n",
      "Epoch %d: train loss %f 139 0.3901860471814871\n",
      "Epoch 139: val loss 0.596004\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6917924731969833\n",
      "Epoch 0: val loss 0.693690\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6888782580693563\n",
      "Epoch 1: val loss 0.693695\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6845798492431641\n",
      "Epoch 2: val loss 0.693503\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6818389644225439\n",
      "Epoch 3: val loss 0.692994\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6648721247911453\n",
      "Epoch 4: val loss 0.687595\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6493424673875173\n",
      "Epoch 5: val loss 0.691319\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6348496427138647\n",
      "Epoch 6: val loss 0.718153\n",
      "\n",
      "Epoch %d: train loss %f 7 0.625643347700437\n",
      "Epoch 7: val loss 0.721189\n",
      "\n",
      "Epoch %d: train loss %f 8 0.610433022181193\n",
      "Epoch 8: val loss 0.699618\n",
      "\n",
      "Epoch %d: train loss %f 9 0.6091336111227671\n",
      "Epoch 9: val loss 0.718276\n",
      "\n",
      "Epoch %d: train loss %f 10 0.5950325007239977\n",
      "Epoch 10: val loss 0.727616\n",
      "\n",
      "Epoch %d: train loss %f 11 0.6107576961318651\n",
      "Epoch 11: val loss 0.713198\n",
      "\n",
      "Epoch %d: train loss %f 12 0.5881890108187994\n",
      "Epoch 12: val loss 0.716097\n",
      "\n",
      "Epoch %d: train loss %f 13 0.5880983496705691\n",
      "Epoch 13: val loss 0.718391\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5938742309808731\n",
      "Epoch 14: val loss 0.734874\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5714463988939921\n",
      "Epoch 15: val loss 0.718049\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5925402541955312\n",
      "Epoch 16: val loss 0.724502\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5837288772066435\n",
      "Epoch 17: val loss 0.729568\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5781473169724146\n",
      "Epoch 18: val loss 0.700736\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5740786294142405\n",
      "Epoch 19: val loss 0.744586\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5671850095192591\n",
      "Epoch 20: val loss 0.729097\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5563538471857706\n",
      "Epoch 21: val loss 0.708785\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5570136780540148\n",
      "Epoch 22: val loss 0.723321\n",
      "\n",
      "Epoch %d: train loss %f 23 0.5486959293484688\n",
      "Epoch 23: val loss 0.742125\n",
      "\n",
      "Epoch %d: train loss %f 24 0.5548097888628641\n",
      "Epoch 24: val loss 0.722169\n",
      "\n",
      "Epoch %d: train loss %f 25 0.5601553022861481\n",
      "Epoch 25: val loss 0.728985\n",
      "\n",
      "Epoch %d: train loss %f 26 0.5639230608940125\n",
      "Epoch 26: val loss 0.723197\n",
      "\n",
      "Epoch %d: train loss %f 27 0.5436919008692106\n",
      "Epoch 27: val loss 0.734118\n",
      "\n",
      "Epoch %d: train loss %f 28 0.55878713230292\n",
      "Epoch 28: val loss 0.742755\n",
      "\n",
      "Epoch %d: train loss %f 29 0.5424168606599172\n",
      "Epoch 29: val loss 0.719011\n",
      "\n",
      "Epoch %d: train loss %f 30 0.5632644221186638\n",
      "Epoch 30: val loss 0.729842\n",
      "\n",
      "Epoch %d: train loss %f 31 0.5433404073119164\n",
      "Epoch 31: val loss 0.726164\n",
      "\n",
      "Epoch %d: train loss %f 32 0.5376062865058581\n",
      "Epoch 32: val loss 0.733139\n",
      "\n",
      "Epoch %d: train loss %f 33 0.5583889906605085\n",
      "Epoch 33: val loss 0.697079\n",
      "\n",
      "Epoch %d: train loss %f 34 0.5446219369769096\n",
      "Epoch 34: val loss 0.728810\n",
      "\n",
      "Epoch %d: train loss %f 35 0.516031210621198\n",
      "Epoch 35: val loss 0.749430\n",
      "\n",
      "Epoch %d: train loss %f 36 0.5197084049383799\n",
      "Epoch 36: val loss 0.739935\n",
      "\n",
      "Epoch %d: train loss %f 37 0.5122385248541832\n",
      "Epoch 37: val loss 0.760327\n",
      "\n",
      "Epoch %d: train loss %f 38 0.501751221716404\n",
      "Epoch 38: val loss 0.757635\n",
      "\n",
      "Epoch %d: train loss %f 39 0.5112708956003189\n",
      "Epoch 39: val loss 0.747543\n",
      "\n",
      "Epoch %d: train loss %f 40 0.5100703959663709\n",
      "Epoch 40: val loss 0.740985\n",
      "\n",
      "Epoch %d: train loss %f 41 0.5307681361834208\n",
      "Epoch 41: val loss 0.792087\n",
      "\n",
      "Epoch %d: train loss %f 42 0.5109306847055753\n",
      "Epoch 42: val loss 0.730155\n",
      "\n",
      "Epoch %d: train loss %f 43 0.5184544722239176\n",
      "Epoch 43: val loss 0.725802\n",
      "\n",
      "Epoch %d: train loss %f 44 0.5074961160620054\n",
      "Epoch 44: val loss 0.751163\n",
      "\n",
      "Epoch %d: train loss %f 45 0.5244498426715533\n",
      "Epoch 45: val loss 0.742282\n",
      "\n",
      "Epoch %d: train loss %f 46 0.5024610236287117\n",
      "Epoch 46: val loss 0.763417\n",
      "\n",
      "Epoch %d: train loss %f 47 0.5108208109935125\n",
      "Epoch 47: val loss 0.744703\n",
      "\n",
      "Epoch %d: train loss %f 48 0.5150880018870035\n",
      "Epoch 48: val loss 0.734347\n",
      "\n",
      "Epoch %d: train loss %f 49 0.48938461641470593\n",
      "Epoch 49: val loss 0.726713\n",
      "\n",
      "Epoch %d: train loss %f 50 0.5068508212765058\n",
      "Epoch 50: val loss 0.745180\n",
      "\n",
      "Epoch %d: train loss %f 51 0.5034483994046847\n",
      "Epoch 51: val loss 0.741381\n",
      "\n",
      "Epoch %d: train loss %f 52 0.47723503907521564\n",
      "Epoch 52: val loss 0.756949\n",
      "\n",
      "Epoch %d: train loss %f 53 0.5007630536953608\n",
      "Epoch 53: val loss 0.743072\n",
      "\n",
      "Epoch %d: train loss %f 54 0.5118968263268471\n",
      "Epoch 54: val loss 0.741931\n",
      "\n",
      "Epoch %d: train loss %f 55 0.5056351547439893\n",
      "Epoch 55: val loss 0.727289\n",
      "\n",
      "Epoch %d: train loss %f 56 0.5027104169130325\n",
      "Epoch 56: val loss 0.727842\n",
      "\n",
      "Epoch %d: train loss %f 57 0.4896744290987651\n",
      "Epoch 57: val loss 0.745836\n",
      "\n",
      "Epoch %d: train loss %f 58 0.48089898626009625\n",
      "Epoch 58: val loss 0.728133\n",
      "\n",
      "Epoch %d: train loss %f 59 0.5044584001104037\n",
      "Epoch 59: val loss 0.734674\n",
      "\n",
      "Epoch %d: train loss %f 60 0.46558047582705814\n",
      "Epoch 60: val loss 0.737081\n",
      "\n",
      "Epoch %d: train loss %f 61 0.48536205540100735\n",
      "Epoch 61: val loss 0.749862\n",
      "\n",
      "Epoch %d: train loss %f 62 0.47798823068539303\n",
      "Epoch 62: val loss 0.770241\n",
      "\n",
      "Epoch %d: train loss %f 63 0.46611467003822327\n",
      "Epoch 63: val loss 0.751361\n",
      "\n",
      "Epoch %d: train loss %f 64 0.43756607671578723\n",
      "Epoch 64: val loss 0.720748\n",
      "\n",
      "Epoch %d: train loss %f 65 0.4696603665749232\n",
      "Epoch 65: val loss 0.751119\n",
      "\n",
      "Epoch %d: train loss %f 66 0.4754477913180987\n",
      "Epoch 66: val loss 0.769250\n",
      "\n",
      "Epoch %d: train loss %f 67 0.44979555904865265\n",
      "Epoch 67: val loss 0.735086\n",
      "\n",
      "Epoch %d: train loss %f 68 0.4680767705043157\n",
      "Epoch 68: val loss 0.742814\n",
      "\n",
      "Epoch %d: train loss %f 69 0.45368490119775134\n",
      "Epoch 69: val loss 0.743659\n",
      "\n",
      "Epoch %d: train loss %f 70 0.4762829393148422\n",
      "Epoch 70: val loss 0.745228\n",
      "\n",
      "Epoch %d: train loss %f 71 0.45425548156102497\n",
      "Epoch 71: val loss 0.759369\n",
      "\n",
      "Epoch %d: train loss %f 72 0.46640990177790326\n",
      "Epoch 72: val loss 0.719060\n",
      "\n",
      "Epoch %d: train loss %f 73 0.46891741702953976\n",
      "Epoch 73: val loss 0.757172\n",
      "\n",
      "Epoch %d: train loss %f 74 0.45389359692732495\n",
      "Epoch 74: val loss 0.770678\n",
      "\n",
      "Epoch %d: train loss %f 75 0.4766673917571704\n",
      "Epoch 75: val loss 0.751814\n",
      "\n",
      "Epoch %d: train loss %f 76 0.43919282406568527\n",
      "Epoch 76: val loss 0.741397\n",
      "\n",
      "Epoch %d: train loss %f 77 0.44019510596990585\n",
      "Epoch 77: val loss 0.743792\n",
      "\n",
      "Epoch %d: train loss %f 78 0.4307621990640958\n",
      "Epoch 78: val loss 0.717206\n",
      "\n",
      "Epoch %d: train loss %f 79 0.43273641417423886\n",
      "Epoch 79: val loss 0.763547\n",
      "\n",
      "Epoch %d: train loss %f 80 0.4443289289871852\n",
      "Epoch 80: val loss 0.773011\n",
      "\n",
      "Epoch %d: train loss %f 81 0.4764511634906133\n",
      "Epoch 81: val loss 0.764507\n",
      "\n",
      "Epoch %d: train loss %f 82 0.4521164000034332\n",
      "Epoch 82: val loss 0.750991\n",
      "\n",
      "Epoch %d: train loss %f 83 0.41143276045719784\n",
      "Epoch 83: val loss 0.777017\n",
      "\n",
      "Epoch %d: train loss %f 84 0.42434647182623547\n",
      "Epoch 84: val loss 0.772332\n",
      "\n",
      "Epoch %d: train loss %f 85 0.432838295896848\n",
      "Epoch 85: val loss 0.753951\n",
      "\n",
      "Epoch %d: train loss %f 86 0.4108358869949977\n",
      "Epoch 86: val loss 0.753385\n",
      "\n",
      "Epoch %d: train loss %f 87 0.4252059335509936\n",
      "Epoch 87: val loss 0.780771\n",
      "\n",
      "Epoch %d: train loss %f 88 0.451817365984122\n",
      "Epoch 88: val loss 0.743073\n",
      "\n",
      "Epoch %d: train loss %f 89 0.42652862643202144\n",
      "Epoch 89: val loss 0.753597\n",
      "\n",
      "Epoch %d: train loss %f 90 0.44870157291491825\n",
      "Epoch 90: val loss 0.741411\n",
      "\n",
      "Epoch %d: train loss %f 91 0.4383738761146863\n",
      "Epoch 91: val loss 0.771454\n",
      "\n",
      "Epoch %d: train loss %f 92 0.4188233862320582\n",
      "Epoch 92: val loss 0.784041\n",
      "\n",
      "Epoch %d: train loss %f 93 0.4530576169490814\n",
      "Epoch 93: val loss 0.736137\n",
      "\n",
      "Epoch %d: train loss %f 94 0.44401135047276813\n",
      "Epoch 94: val loss 0.780891\n",
      "\n",
      "Epoch %d: train loss %f 95 0.40839727222919464\n",
      "Epoch 95: val loss 0.770776\n",
      "\n",
      "Epoch %d: train loss %f 96 0.4791952768961589\n",
      "Epoch 96: val loss 0.774538\n",
      "\n",
      "Epoch %d: train loss %f 97 0.4215528815984726\n",
      "Epoch 97: val loss 0.762635\n",
      "\n",
      "Epoch %d: train loss %f 98 0.4256598949432373\n",
      "Epoch 98: val loss 0.766686\n",
      "\n",
      "Epoch %d: train loss %f 99 0.4236264203985532\n",
      "Epoch 99: val loss 0.742869\n",
      "\n",
      "Epoch %d: train loss %f 100 0.4003482982516289\n",
      "Epoch 100: val loss 0.796437\n",
      "\n",
      "Epoch %d: train loss %f 101 0.45534223566452664\n",
      "Epoch 101: val loss 0.784657\n",
      "\n",
      "Epoch %d: train loss %f 102 0.38426827639341354\n",
      "Epoch 102: val loss 0.742580\n",
      "\n",
      "Epoch %d: train loss %f 103 0.42560628553231555\n",
      "Epoch 103: val loss 0.779441\n",
      "\n",
      "Epoch %d: train loss %f 104 0.4378076493740082\n",
      "Epoch 104: val loss 0.812140\n",
      "\n",
      "Epoch %d: train loss %f 105 0.3873208463191986\n",
      "Epoch 105: val loss 0.753954\n",
      "\n",
      "Epoch %d: train loss %f 106 0.44809579228361446\n",
      "Epoch 106: val loss 0.771275\n",
      "\n",
      "Epoch %d: train loss %f 107 0.4152454286813736\n",
      "Epoch 107: val loss 0.757620\n",
      "\n",
      "Epoch %d: train loss %f 108 0.4376444121201833\n",
      "Epoch 108: val loss 0.749857\n",
      "\n",
      "Epoch %d: train loss %f 109 0.43738224109013873\n",
      "Epoch 109: val loss 0.769591\n",
      "\n",
      "Epoch %d: train loss %f 110 0.4255705426136653\n",
      "Epoch 110: val loss 0.766909\n",
      "\n",
      "Epoch %d: train loss %f 111 0.4112541402379672\n",
      "Epoch 111: val loss 0.774575\n",
      "\n",
      "Epoch %d: train loss %f 112 0.4423487534125646\n",
      "Epoch 112: val loss 0.750268\n",
      "\n",
      "Epoch %d: train loss %f 113 0.39708589886625606\n",
      "Epoch 113: val loss 0.774444\n",
      "\n",
      "Epoch %d: train loss %f 114 0.41481484721104306\n",
      "Epoch 114: val loss 0.740683\n",
      "\n",
      "Epoch %d: train loss %f 115 0.43014271805683774\n",
      "Epoch 115: val loss 0.757441\n",
      "\n",
      "Epoch %d: train loss %f 116 0.3784187858303388\n",
      "Epoch 116: val loss 0.827183\n",
      "\n",
      "Epoch %d: train loss %f 117 0.40420368562142056\n",
      "Epoch 117: val loss 0.801824\n",
      "\n",
      "Epoch %d: train loss %f 118 0.40831731011470157\n",
      "Epoch 118: val loss 0.759632\n",
      "\n",
      "Epoch %d: train loss %f 119 0.39891299108664197\n",
      "Epoch 119: val loss 0.762933\n",
      "\n",
      "Epoch %d: train loss %f 120 0.3865425189336141\n",
      "Epoch 120: val loss 0.836235\n",
      "\n",
      "Epoch %d: train loss %f 121 0.3757797529300054\n",
      "Epoch 121: val loss 0.786898\n",
      "\n",
      "Epoch %d: train loss %f 122 0.41014596074819565\n",
      "Epoch 122: val loss 0.790780\n",
      "\n",
      "Epoch %d: train loss %f 123 0.39534225563208264\n",
      "Epoch 123: val loss 0.863909\n",
      "\n",
      "Epoch %d: train loss %f 124 0.4503298376997312\n",
      "Epoch 124: val loss 0.785854\n",
      "\n",
      "Epoch %d: train loss %f 125 0.4135731483499209\n",
      "Epoch 125: val loss 0.778987\n",
      "\n",
      "Epoch %d: train loss %f 126 0.455773505071799\n",
      "Epoch 126: val loss 0.810060\n",
      "\n",
      "Epoch %d: train loss %f 127 0.3988748913009961\n",
      "Epoch 127: val loss 0.810974\n",
      "\n",
      "Epoch %d: train loss %f 128 0.3905799339214961\n",
      "Epoch 128: val loss 0.815844\n",
      "\n",
      "Epoch %d: train loss %f 129 0.39901018639405567\n",
      "Epoch 129: val loss 0.815336\n",
      "\n",
      "Epoch %d: train loss %f 130 0.394246111313502\n",
      "Epoch 130: val loss 0.785166\n",
      "\n",
      "Epoch %d: train loss %f 131 0.4409620488683383\n",
      "Epoch 131: val loss 0.765161\n",
      "\n",
      "Epoch %d: train loss %f 132 0.396709901591142\n",
      "Epoch 132: val loss 0.822147\n",
      "\n",
      "Epoch %d: train loss %f 133 0.3945918033520381\n",
      "Epoch 133: val loss 0.752482\n",
      "\n",
      "Epoch %d: train loss %f 134 0.39520230144262314\n",
      "Epoch 134: val loss 0.788991\n",
      "\n",
      "Epoch %d: train loss %f 135 0.4142950897415479\n",
      "Epoch 135: val loss 0.786973\n",
      "\n",
      "Epoch %d: train loss %f 136 0.42346957822640735\n",
      "Epoch 136: val loss 0.797547\n",
      "\n",
      "Epoch %d: train loss %f 137 0.4018749271829923\n",
      "Epoch 137: val loss 0.787719\n",
      "\n",
      "Epoch %d: train loss %f 138 0.3476252071559429\n",
      "Epoch 138: val loss 0.860854\n",
      "\n",
      "Epoch %d: train loss %f 139 0.4181983346740405\n",
      "Epoch 139: val loss 0.839574\n",
      "\n",
      "Epoch %d: train loss %f 140 0.37669652203718823\n",
      "Epoch 140: val loss 0.818257\n",
      "\n",
      "Epoch %d: train loss %f 141 0.42195191482702893\n",
      "Epoch 141: val loss 0.786445\n",
      "\n",
      "Epoch %d: train loss %f 142 0.38194336617986363\n",
      "Epoch 142: val loss 0.795050\n",
      "\n",
      "Epoch %d: train loss %f 143 0.3609641169508298\n",
      "Epoch 143: val loss 0.811839\n",
      "\n",
      "Epoch %d: train loss %f 144 0.3771741067369779\n",
      "Epoch 144: val loss 0.823392\n",
      "\n",
      "Epoch %d: train loss %f 145 0.40675511459509534\n",
      "Epoch 145: val loss 0.821614\n",
      "\n",
      "Epoch %d: train loss %f 146 0.4088206763068835\n",
      "Epoch 146: val loss 0.760663\n",
      "\n",
      "Epoch %d: train loss %f 147 0.38965171451369923\n",
      "Epoch 147: val loss 0.771942\n",
      "\n",
      "Epoch %d: train loss %f 148 0.36601169655720395\n",
      "Epoch 148: val loss 0.804410\n",
      "\n",
      "Epoch %d: train loss %f 149 0.39770791803797084\n",
      "Epoch 149: val loss 0.834088\n",
      "\n",
      "Epoch %d: train loss %f 150 0.43177630007267\n",
      "Epoch 150: val loss 0.842878\n",
      "\n",
      "Epoch %d: train loss %f 151 0.3986757770180702\n",
      "Epoch 151: val loss 0.837025\n",
      "\n",
      "Epoch %d: train loss %f 152 0.41455041617155075\n",
      "Epoch 152: val loss 0.790824\n",
      "\n",
      "Epoch %d: train loss %f 153 0.405082402129968\n",
      "Epoch 153: val loss 0.837953\n",
      "\n",
      "Epoch %d: train loss %f 154 0.40679799020290375\n",
      "Epoch 154: val loss 0.772572\n",
      "\n",
      "Epoch %d: train loss %f 155 0.3631596763928731\n",
      "Epoch 155: val loss 0.841417\n",
      "\n",
      "Epoch %d: train loss %f 156 0.38207213828961056\n",
      "Epoch 156: val loss 0.815154\n",
      "\n",
      "Epoch %d: train loss %f 157 0.4032542034983635\n",
      "Epoch 157: val loss 0.830813\n",
      "\n",
      "Epoch %d: train loss %f 158 0.3836914499600728\n",
      "Epoch 158: val loss 0.790411\n",
      "\n",
      "Epoch %d: train loss %f 159 0.35986736540993053\n",
      "Epoch 159: val loss 0.819123\n",
      "\n",
      "Epoch %d: train loss %f 160 0.38922401890158653\n",
      "Epoch 160: val loss 0.787690\n",
      "\n",
      "Epoch %d: train loss %f 161 0.38928315167625743\n",
      "Epoch 161: val loss 0.786542\n",
      "\n",
      "Epoch %d: train loss %f 162 0.37317710121472675\n",
      "Epoch 162: val loss 0.795981\n",
      "\n",
      "Epoch %d: train loss %f 163 0.33907175809144974\n",
      "Epoch 163: val loss 0.789532\n",
      "\n",
      "Epoch %d: train loss %f 164 0.3729112967848778\n",
      "Epoch 164: val loss 0.817341\n",
      "\n",
      "Epoch %d: train loss %f 165 0.37853191668788594\n",
      "Epoch 165: val loss 0.867610\n",
      "\n",
      "Epoch %d: train loss %f 166 0.34159788489341736\n",
      "Epoch 166: val loss 0.827396\n",
      "\n",
      "Epoch %d: train loss %f 167 0.35222605367501575\n",
      "Epoch 167: val loss 0.825239\n",
      "\n",
      "Epoch %d: train loss %f 168 0.38326558222373325\n",
      "Epoch 168: val loss 0.831273\n",
      "\n",
      "Epoch %d: train loss %f 169 0.3448231890797615\n",
      "Epoch 169: val loss 0.894412\n",
      "\n",
      "Epoch %d: train loss %f 170 0.3791850705941518\n",
      "Epoch 170: val loss 0.803590\n",
      "\n",
      "Epoch %d: train loss %f 171 0.4069330617785454\n",
      "Epoch 171: val loss 0.867975\n",
      "\n",
      "Epoch %d: train loss %f 172 0.3325773775577545\n",
      "Epoch 172: val loss 0.866466\n",
      "\n",
      "Epoch %d: train loss %f 173 0.39308125029007596\n",
      "Epoch 173: val loss 0.838558\n",
      "\n",
      "Epoch %d: train loss %f 174 0.3358304277062416\n",
      "Epoch 174: val loss 0.826366\n",
      "\n",
      "Epoch %d: train loss %f 175 0.3659003749489784\n",
      "Epoch 175: val loss 0.862691\n",
      "\n",
      "Epoch %d: train loss %f 176 0.3573360393444697\n",
      "Epoch 176: val loss 0.826587\n",
      "\n",
      "Epoch %d: train loss %f 177 0.34437256306409836\n",
      "Epoch 177: val loss 0.849958\n",
      "\n",
      "Epoch %d: train loss %f 178 0.358395313223203\n",
      "Epoch 178: val loss 0.865908\n",
      "\n",
      "Epoch %d: train loss %f 179 0.3463534191250801\n",
      "Epoch 179: val loss 0.834477\n",
      "\n",
      "Epoch %d: train loss %f 180 0.34573106716076535\n",
      "Epoch 180: val loss 0.841209\n",
      "\n",
      "Epoch %d: train loss %f 181 0.36090920865535736\n",
      "Epoch 181: val loss 0.881462\n",
      "\n",
      "Epoch %d: train loss %f 182 0.34281564007202786\n",
      "Epoch 182: val loss 0.890241\n",
      "\n",
      "Epoch %d: train loss %f 183 0.361930962651968\n",
      "Epoch 183: val loss 0.857328\n",
      "\n",
      "Epoch %d: train loss %f 184 0.3847636456290881\n",
      "Epoch 184: val loss 0.864393\n",
      "\n",
      "Epoch %d: train loss %f 185 0.3881908059120178\n",
      "Epoch 185: val loss 0.873041\n",
      "\n",
      "Epoch %d: train loss %f 186 0.378556019316117\n",
      "Epoch 186: val loss 0.854153\n",
      "\n",
      "Epoch %d: train loss %f 187 0.3741840223471324\n",
      "Epoch 187: val loss 0.865951\n",
      "\n",
      "Epoch %d: train loss %f 188 0.35396627833445865\n",
      "Epoch 188: val loss 0.826660\n",
      "\n",
      "Epoch %d: train loss %f 189 0.38353704412778217\n",
      "Epoch 189: val loss 0.867173\n",
      "\n",
      "Epoch %d: train loss %f 190 0.39167481909195584\n",
      "Epoch 190: val loss 0.841842\n",
      "\n",
      "Epoch %d: train loss %f 191 0.35897431522607803\n",
      "Epoch 191: val loss 0.819099\n",
      "\n",
      "Epoch %d: train loss %f 192 0.3449113517999649\n",
      "Epoch 192: val loss 0.826258\n",
      "\n",
      "Epoch %d: train loss %f 193 0.3624115337928136\n",
      "Epoch 193: val loss 0.883931\n",
      "\n",
      "Epoch %d: train loss %f 194 0.335919829706351\n",
      "Epoch 194: val loss 0.868384\n",
      "\n",
      "Epoch %d: train loss %f 195 0.38493967801332474\n",
      "Epoch 195: val loss 0.854616\n",
      "\n",
      "Epoch %d: train loss %f 196 0.36314775546391803\n",
      "Epoch 196: val loss 0.848300\n",
      "\n",
      "Epoch %d: train loss %f 197 0.3603462278842926\n",
      "Epoch 197: val loss 0.832893\n",
      "\n",
      "Epoch %d: train loss %f 198 0.3598335149387519\n",
      "Epoch 198: val loss 0.880290\n",
      "\n",
      "Epoch %d: train loss %f 199 0.35542724281549454\n",
      "Epoch 199: val loss 0.829821\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6927344352006912\n",
      "Epoch 0: val loss 0.692978\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6888452569643656\n",
      "Epoch 1: val loss 0.692698\n",
      "\n",
      "Epoch %d: train loss %f 2 0.6837454438209534\n",
      "Epoch 2: val loss 0.691800\n",
      "\n",
      "Epoch %d: train loss %f 3 0.6785322974125544\n",
      "Epoch 3: val loss 0.689815\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6641603211561838\n",
      "Epoch 4: val loss 0.684457\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6594280103842417\n",
      "Epoch 5: val loss 0.678786\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6404571831226349\n",
      "Epoch 6: val loss 0.685154\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6466430127620697\n",
      "Epoch 7: val loss 0.677202\n",
      "\n",
      "Epoch %d: train loss %f 8 0.6363445371389389\n",
      "Epoch 8: val loss 0.671081\n",
      "\n",
      "Epoch %d: train loss %f 9 0.643607551852862\n",
      "Epoch 9: val loss 0.664006\n",
      "\n",
      "Epoch %d: train loss %f 10 0.632184719045957\n",
      "Epoch 10: val loss 0.667026\n",
      "\n",
      "Epoch %d: train loss %f 11 0.621572936574618\n",
      "Epoch 11: val loss 0.668231\n",
      "\n",
      "Epoch %d: train loss %f 12 0.6240716228882471\n",
      "Epoch 12: val loss 0.662249\n",
      "\n",
      "Epoch %d: train loss %f 13 0.6133432586987814\n",
      "Epoch 13: val loss 0.663372\n",
      "\n",
      "Epoch %d: train loss %f 14 0.6156532168388367\n",
      "Epoch 14: val loss 0.662171\n",
      "\n",
      "Epoch %d: train loss %f 15 0.6165985663731893\n",
      "Epoch 15: val loss 0.658612\n",
      "\n",
      "Epoch %d: train loss %f 16 0.6177540918191274\n",
      "Epoch 16: val loss 0.662790\n",
      "\n",
      "Epoch %d: train loss %f 17 0.6103019515673319\n",
      "Epoch 17: val loss 0.658856\n",
      "\n",
      "Epoch %d: train loss %f 18 0.6067300140857697\n",
      "Epoch 18: val loss 0.662366\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5982968037327131\n",
      "Epoch 19: val loss 0.659090\n",
      "\n",
      "Epoch %d: train loss %f 20 0.596088320016861\n",
      "Epoch 20: val loss 0.659584\n",
      "\n",
      "Epoch %d: train loss %f 21 0.6020735402901968\n",
      "Epoch 21: val loss 0.658759\n",
      "\n",
      "Epoch %d: train loss %f 22 0.5995956907669703\n",
      "Epoch 22: val loss 0.657822\n",
      "\n",
      "Epoch %d: train loss %f 23 0.5963036864995956\n",
      "Epoch 23: val loss 0.654280\n",
      "\n",
      "Epoch %d: train loss %f 24 0.5914093653361002\n",
      "Epoch 24: val loss 0.649796\n",
      "\n",
      "Epoch %d: train loss %f 25 0.5986240829030672\n",
      "Epoch 25: val loss 0.650051\n",
      "\n",
      "Epoch %d: train loss %f 26 0.5946150670448939\n",
      "Epoch 26: val loss 0.649393\n",
      "\n",
      "Epoch %d: train loss %f 27 0.58321679631869\n",
      "Epoch 27: val loss 0.644648\n",
      "\n",
      "Epoch %d: train loss %f 28 0.590095117688179\n",
      "Epoch 28: val loss 0.651053\n",
      "\n",
      "Epoch %d: train loss %f 29 0.5834975192944208\n",
      "Epoch 29: val loss 0.652470\n",
      "\n",
      "Epoch %d: train loss %f 30 0.5817667072017988\n",
      "Epoch 30: val loss 0.649446\n",
      "\n",
      "Epoch %d: train loss %f 31 0.5691138903299967\n",
      "Epoch 31: val loss 0.641990\n",
      "\n",
      "Epoch %d: train loss %f 32 0.5784418632586797\n",
      "Epoch 32: val loss 0.646006\n",
      "\n",
      "Epoch %d: train loss %f 33 0.5689514180024465\n",
      "Epoch 33: val loss 0.645934\n",
      "\n",
      "Epoch %d: train loss %f 34 0.5665056531627973\n",
      "Epoch 34: val loss 0.633936\n",
      "\n",
      "Epoch %d: train loss %f 35 0.5824100598692894\n",
      "Epoch 35: val loss 0.643659\n",
      "\n",
      "Epoch %d: train loss %f 36 0.5572065611680349\n",
      "Epoch 36: val loss 0.646205\n",
      "\n",
      "Epoch %d: train loss %f 37 0.5648680304487547\n",
      "Epoch 37: val loss 0.643732\n",
      "\n",
      "Epoch %d: train loss %f 38 0.5664283211032549\n",
      "Epoch 38: val loss 0.628187\n",
      "\n",
      "Epoch %d: train loss %f 39 0.5597136095166206\n",
      "Epoch 39: val loss 0.638639\n",
      "\n",
      "Epoch %d: train loss %f 40 0.5543591131766638\n",
      "Epoch 40: val loss 0.645059\n",
      "\n",
      "Epoch %d: train loss %f 41 0.5575305049618086\n",
      "Epoch 41: val loss 0.647095\n",
      "\n",
      "Epoch %d: train loss %f 42 0.5433195481697718\n",
      "Epoch 42: val loss 0.647628\n",
      "\n",
      "Epoch %d: train loss %f 43 0.5564574301242828\n",
      "Epoch 43: val loss 0.627868\n",
      "\n",
      "Epoch %d: train loss %f 44 0.5571717371543249\n",
      "Epoch 44: val loss 0.641087\n",
      "\n",
      "Epoch %d: train loss %f 45 0.5503145083785057\n",
      "Epoch 45: val loss 0.638898\n",
      "\n",
      "Epoch %d: train loss %f 46 0.5675999124844869\n",
      "Epoch 46: val loss 0.628681\n",
      "\n",
      "Epoch %d: train loss %f 47 0.5516582280397415\n",
      "Epoch 47: val loss 0.618539\n",
      "\n",
      "Epoch %d: train loss %f 48 0.5583250870307287\n",
      "Epoch 48: val loss 0.624750\n",
      "\n",
      "Epoch %d: train loss %f 49 0.5471044331789017\n",
      "Epoch 49: val loss 0.625587\n",
      "\n",
      "Epoch %d: train loss %f 50 0.5360338240861893\n",
      "Epoch 50: val loss 0.623367\n",
      "\n",
      "Epoch %d: train loss %f 51 0.5373409738143285\n",
      "Epoch 51: val loss 0.628818\n",
      "\n",
      "Epoch %d: train loss %f 52 0.5353363032142321\n",
      "Epoch 52: val loss 0.638941\n",
      "\n",
      "Epoch %d: train loss %f 53 0.5253116860985756\n",
      "Epoch 53: val loss 0.634510\n",
      "\n",
      "Epoch %d: train loss %f 54 0.5256935209035873\n",
      "Epoch 54: val loss 0.628928\n",
      "\n",
      "Epoch %d: train loss %f 55 0.545865868528684\n",
      "Epoch 55: val loss 0.618925\n",
      "\n",
      "Epoch %d: train loss %f 56 0.5525833865006765\n",
      "Epoch 56: val loss 0.638286\n",
      "\n",
      "Epoch %d: train loss %f 57 0.5412362068891525\n",
      "Epoch 57: val loss 0.606849\n",
      "\n",
      "Epoch %d: train loss %f 58 0.5271278793613116\n",
      "Epoch 58: val loss 0.615558\n",
      "\n",
      "Epoch %d: train loss %f 59 0.5550112103422483\n",
      "Epoch 59: val loss 0.630216\n",
      "\n",
      "Epoch %d: train loss %f 60 0.5064915294448534\n",
      "Epoch 60: val loss 0.627300\n",
      "\n",
      "Epoch %d: train loss %f 61 0.5397146865725517\n",
      "Epoch 61: val loss 0.624079\n",
      "\n",
      "Epoch %d: train loss %f 62 0.5276067157586416\n",
      "Epoch 62: val loss 0.631173\n",
      "\n",
      "Epoch %d: train loss %f 63 0.5122723976771036\n",
      "Epoch 63: val loss 0.644101\n",
      "\n",
      "Epoch %d: train loss %f 64 0.5418966859579086\n",
      "Epoch 64: val loss 0.641080\n",
      "\n",
      "Epoch %d: train loss %f 65 0.5179594829678535\n",
      "Epoch 65: val loss 0.633702\n",
      "\n",
      "Epoch %d: train loss %f 66 0.5374342675010363\n",
      "Epoch 66: val loss 0.619896\n",
      "\n",
      "Epoch %d: train loss %f 67 0.5113188251852989\n",
      "Epoch 67: val loss 0.608378\n",
      "\n",
      "Epoch %d: train loss %f 68 0.5094769348700842\n",
      "Epoch 68: val loss 0.613886\n",
      "\n",
      "Epoch %d: train loss %f 69 0.5084878529111544\n",
      "Epoch 69: val loss 0.625331\n",
      "\n",
      "Epoch %d: train loss %f 70 0.5094280044237772\n",
      "Epoch 70: val loss 0.616403\n",
      "\n",
      "Epoch %d: train loss %f 71 0.5040212621291479\n",
      "Epoch 71: val loss 0.629127\n",
      "\n",
      "Epoch %d: train loss %f 72 0.5307228366533915\n",
      "Epoch 72: val loss 0.601577\n",
      "\n",
      "Epoch %d: train loss %f 73 0.48465078820784885\n",
      "Epoch 73: val loss 0.599286\n",
      "\n",
      "Epoch %d: train loss %f 74 0.4827005863189697\n",
      "Epoch 74: val loss 0.610555\n",
      "\n",
      "Epoch %d: train loss %f 75 0.48612867047389346\n",
      "Epoch 75: val loss 0.628429\n",
      "\n",
      "Epoch %d: train loss %f 76 0.5021801143884659\n",
      "Epoch 76: val loss 0.642414\n",
      "\n",
      "Epoch %d: train loss %f 77 0.5054245218634605\n",
      "Epoch 77: val loss 0.622729\n",
      "\n",
      "Epoch %d: train loss %f 78 0.505094068745772\n",
      "Epoch 78: val loss 0.629069\n",
      "\n",
      "Epoch %d: train loss %f 79 0.4893779307603836\n",
      "Epoch 79: val loss 0.602271\n",
      "\n",
      "Epoch %d: train loss %f 80 0.5101054285963377\n",
      "Epoch 80: val loss 0.614113\n",
      "\n",
      "Epoch %d: train loss %f 81 0.5028977617621422\n",
      "Epoch 81: val loss 0.633770\n",
      "\n",
      "Epoch %d: train loss %f 82 0.49363108227650326\n",
      "Epoch 82: val loss 0.626245\n",
      "\n",
      "Epoch %d: train loss %f 83 0.5143167749047279\n",
      "Epoch 83: val loss 0.625964\n",
      "\n",
      "Epoch %d: train loss %f 84 0.49396001795927685\n",
      "Epoch 84: val loss 0.626275\n",
      "\n",
      "Epoch %d: train loss %f 85 0.5176690717538198\n",
      "Epoch 85: val loss 0.614879\n",
      "\n",
      "Epoch %d: train loss %f 86 0.5111200734972954\n",
      "Epoch 86: val loss 0.640162\n",
      "\n",
      "Epoch %d: train loss %f 87 0.5108514179786047\n",
      "Epoch 87: val loss 0.631083\n",
      "\n",
      "Epoch %d: train loss %f 88 0.5025952781240145\n",
      "Epoch 88: val loss 0.643688\n",
      "\n",
      "Epoch %d: train loss %f 89 0.5129844024777412\n",
      "Epoch 89: val loss 0.637481\n",
      "\n",
      "Epoch %d: train loss %f 90 0.5252216657002767\n",
      "Epoch 90: val loss 0.626234\n",
      "\n",
      "Epoch %d: train loss %f 91 0.47941263268391293\n",
      "Epoch 91: val loss 0.629651\n",
      "\n",
      "Epoch %d: train loss %f 92 0.4968530361851056\n",
      "Epoch 92: val loss 0.635983\n",
      "\n",
      "Epoch %d: train loss %f 93 0.49783384799957275\n",
      "Epoch 93: val loss 0.618483\n",
      "\n",
      "Epoch %d: train loss %f 94 0.49405184636513394\n",
      "Epoch 94: val loss 0.606909\n",
      "\n",
      "Epoch %d: train loss %f 95 0.4987554649511973\n",
      "Epoch 95: val loss 0.616550\n",
      "\n",
      "Epoch %d: train loss %f 96 0.4806755681832631\n",
      "Epoch 96: val loss 0.613276\n",
      "\n",
      "Epoch %d: train loss %f 97 0.4886365830898285\n",
      "Epoch 97: val loss 0.627901\n",
      "\n",
      "Epoch %d: train loss %f 98 0.46837177872657776\n",
      "Epoch 98: val loss 0.610779\n",
      "\n",
      "Epoch %d: train loss %f 99 0.47531359642744064\n",
      "Epoch 99: val loss 0.635541\n",
      "\n",
      "Epoch %d: train loss %f 100 0.4803234164913495\n",
      "Epoch 100: val loss 0.630240\n",
      "\n",
      "Epoch %d: train loss %f 101 0.4829394494493802\n",
      "Epoch 101: val loss 0.639769\n",
      "\n",
      "Epoch %d: train loss %f 102 0.46807073056697845\n",
      "Epoch 102: val loss 0.644326\n",
      "\n",
      "Epoch %d: train loss %f 103 0.45770401880145073\n",
      "Epoch 103: val loss 0.654375\n",
      "\n",
      "Epoch %d: train loss %f 104 0.47207143406073254\n",
      "Epoch 104: val loss 0.617438\n",
      "\n",
      "Epoch %d: train loss %f 105 0.5293270026644071\n",
      "Epoch 105: val loss 0.630788\n",
      "\n",
      "Epoch %d: train loss %f 106 0.49965516726175946\n",
      "Epoch 106: val loss 0.637489\n",
      "\n",
      "Epoch %d: train loss %f 107 0.4470829988519351\n",
      "Epoch 107: val loss 0.594996\n",
      "\n",
      "Epoch %d: train loss %f 108 0.47224725782871246\n",
      "Epoch 108: val loss 0.632889\n",
      "\n",
      "Epoch %d: train loss %f 109 0.4763936797777812\n",
      "Epoch 109: val loss 0.613375\n",
      "\n",
      "Epoch %d: train loss %f 110 0.4364059542616208\n",
      "Epoch 110: val loss 0.610957\n",
      "\n",
      "Epoch %d: train loss %f 111 0.43268975615501404\n",
      "Epoch 111: val loss 0.634776\n",
      "\n",
      "Epoch %d: train loss %f 112 0.46300242841243744\n",
      "Epoch 112: val loss 0.621942\n",
      "\n",
      "Epoch %d: train loss %f 113 0.42804982513189316\n",
      "Epoch 113: val loss 0.634436\n",
      "\n",
      "Epoch %d: train loss %f 114 0.4543292000889778\n",
      "Epoch 114: val loss 0.634302\n",
      "\n",
      "Epoch %d: train loss %f 115 0.5022654458880424\n",
      "Epoch 115: val loss 0.623210\n",
      "\n",
      "Epoch %d: train loss %f 116 0.44856996089220047\n",
      "Epoch 116: val loss 0.601735\n",
      "\n",
      "Epoch %d: train loss %f 117 0.46172675242026645\n",
      "Epoch 117: val loss 0.644967\n",
      "\n",
      "Epoch %d: train loss %f 118 0.43286266177892685\n",
      "Epoch 118: val loss 0.633169\n",
      "\n",
      "Epoch %d: train loss %f 119 0.4484509825706482\n",
      "Epoch 119: val loss 0.644095\n",
      "\n",
      "Epoch %d: train loss %f 120 0.469770222902298\n",
      "Epoch 120: val loss 0.651629\n",
      "\n",
      "Epoch %d: train loss %f 121 0.4719935357570648\n",
      "Epoch 121: val loss 0.625636\n",
      "\n",
      "Epoch %d: train loss %f 122 0.45619384944438934\n",
      "Epoch 122: val loss 0.660077\n",
      "\n",
      "Epoch %d: train loss %f 123 0.43567022929588956\n",
      "Epoch 123: val loss 0.658344\n",
      "\n",
      "Epoch %d: train loss %f 124 0.44882985204458237\n",
      "Epoch 124: val loss 0.656735\n",
      "\n",
      "Epoch %d: train loss %f 125 0.44494641572237015\n",
      "Epoch 125: val loss 0.643146\n",
      "\n",
      "Epoch %d: train loss %f 126 0.4191319942474365\n",
      "Epoch 126: val loss 0.631647\n",
      "\n",
      "Epoch %d: train loss %f 127 0.4217629060149193\n",
      "Epoch 127: val loss 0.617373\n",
      "\n",
      "Epoch %d: train loss %f 128 0.4512593671679497\n",
      "Epoch 128: val loss 0.619696\n",
      "\n",
      "Epoch %d: train loss %f 129 0.45840145895878476\n",
      "Epoch 129: val loss 0.636476\n",
      "\n",
      "Epoch %d: train loss %f 130 0.42121919244527817\n",
      "Epoch 130: val loss 0.637329\n",
      "\n",
      "Epoch %d: train loss %f 131 0.4267353042960167\n",
      "Epoch 131: val loss 0.618931\n",
      "\n",
      "Epoch %d: train loss %f 132 0.4549396112561226\n",
      "Epoch 132: val loss 0.629233\n",
      "\n",
      "Epoch %d: train loss %f 133 0.43283328662316006\n",
      "Epoch 133: val loss 0.679219\n",
      "\n",
      "Epoch %d: train loss %f 134 0.4368848179777463\n",
      "Epoch 134: val loss 0.659786\n",
      "\n",
      "Epoch %d: train loss %f 135 0.44920244067907333\n",
      "Epoch 135: val loss 0.678006\n",
      "\n",
      "Epoch %d: train loss %f 136 0.49169356872638065\n",
      "Epoch 136: val loss 0.655580\n",
      "\n",
      "Epoch %d: train loss %f 137 0.4509049927194913\n",
      "Epoch 137: val loss 0.624015\n",
      "\n",
      "Epoch %d: train loss %f 138 0.4257731984059016\n",
      "Epoch 138: val loss 0.656949\n",
      "\n",
      "Epoch %d: train loss %f 139 0.43332957476377487\n",
      "Epoch 139: val loss 0.640692\n",
      "\n",
      "Epoch %d: train loss %f 140 0.42129699637492496\n",
      "Epoch 140: val loss 0.646194\n",
      "\n",
      "Epoch %d: train loss %f 141 0.39679967736204463\n",
      "Epoch 141: val loss 0.662726\n",
      "\n",
      "Epoch %d: train loss %f 142 0.4214695592721303\n",
      "Epoch 142: val loss 0.642009\n",
      "\n",
      "Epoch %d: train loss %f 143 0.4231283937891324\n",
      "Epoch 143: val loss 0.645462\n",
      "\n",
      "Epoch %d: train loss %f 144 0.38609732190767926\n",
      "Epoch 144: val loss 0.652532\n",
      "\n",
      "Epoch %d: train loss %f 145 0.40806671728690463\n",
      "Epoch 145: val loss 0.655066\n",
      "\n",
      "Epoch %d: train loss %f 146 0.42712459216515225\n",
      "Epoch 146: val loss 0.655356\n",
      "\n",
      "Epoch %d: train loss %f 147 0.4178517113129298\n",
      "Epoch 147: val loss 0.643201\n",
      "\n",
      "Epoch %d: train loss %f 148 0.41036469240983325\n",
      "Epoch 148: val loss 0.628179\n",
      "\n",
      "Epoch %d: train loss %f 149 0.4129650567968686\n",
      "Epoch 149: val loss 0.662332\n",
      "\n",
      "Epoch %d: train loss %f 150 0.40587545682986576\n",
      "Epoch 150: val loss 0.650545\n",
      "\n",
      "Epoch %d: train loss %f 151 0.41360611220200855\n",
      "Epoch 151: val loss 0.643737\n",
      "\n",
      "Epoch %d: train loss %f 152 0.4120033010840416\n",
      "Epoch 152: val loss 0.668336\n",
      "\n",
      "Epoch %d: train loss %f 153 0.47248661518096924\n",
      "Epoch 153: val loss 0.668079\n",
      "\n",
      "Epoch %d: train loss %f 154 0.41901278123259544\n",
      "Epoch 154: val loss 0.695263\n",
      "\n",
      "Epoch %d: train loss %f 155 0.42791907489299774\n",
      "Epoch 155: val loss 0.635993\n",
      "\n",
      "Epoch %d: train loss %f 156 0.44409984226028126\n",
      "Epoch 156: val loss 0.673949\n",
      "\n",
      "Epoch %d: train loss %f 157 0.4379959578315417\n",
      "Epoch 157: val loss 0.658062\n",
      "\n",
      "Epoch %d: train loss %f 158 0.4470979298154513\n",
      "Epoch 158: val loss 0.624285\n",
      "\n",
      "Epoch %d: train loss %f 159 0.46509918322165805\n",
      "Epoch 159: val loss 0.633572\n",
      "\n",
      "Epoch %d: train loss %f 160 0.41210218022267026\n",
      "Epoch 160: val loss 0.638143\n",
      "\n",
      "Epoch %d: train loss %f 161 0.4060739725828171\n",
      "Epoch 161: val loss 0.668780\n",
      "\n",
      "Epoch %d: train loss %f 162 0.40646569803357124\n",
      "Epoch 162: val loss 0.661422\n",
      "\n",
      "Epoch %d: train loss %f 163 0.38841338579853374\n",
      "Epoch 163: val loss 0.640470\n",
      "\n",
      "Epoch %d: train loss %f 164 0.4394807356099288\n",
      "Epoch 164: val loss 0.653670\n",
      "\n",
      "Epoch %d: train loss %f 165 0.4388909141222636\n",
      "Epoch 165: val loss 0.646596\n",
      "\n",
      "Epoch %d: train loss %f 166 0.41920439153909683\n",
      "Epoch 166: val loss 0.648964\n",
      "\n",
      "Epoch %d: train loss %f 167 0.36859477683901787\n",
      "Epoch 167: val loss 0.646210\n",
      "\n",
      "Epoch %d: train loss %f 168 0.4258829777439435\n",
      "Epoch 168: val loss 0.666435\n",
      "\n",
      "Epoch %d: train loss %f 169 0.43448928495248157\n",
      "Epoch 169: val loss 0.634094\n",
      "\n",
      "Epoch %d: train loss %f 170 0.3946179300546646\n",
      "Epoch 170: val loss 0.644326\n",
      "\n",
      "Epoch %d: train loss %f 171 0.4073569638033708\n",
      "Epoch 171: val loss 0.641418\n",
      "\n",
      "Epoch %d: train loss %f 172 0.3940267711877823\n",
      "Epoch 172: val loss 0.633446\n",
      "\n",
      "Epoch %d: train loss %f 173 0.3764037067691485\n",
      "Epoch 173: val loss 0.656767\n",
      "\n",
      "Epoch %d: train loss %f 174 0.39471614360809326\n",
      "Epoch 174: val loss 0.693298\n",
      "\n",
      "Epoch %d: train loss %f 175 0.3744937404990196\n",
      "Epoch 175: val loss 0.673037\n",
      "\n",
      "Epoch %d: train loss %f 176 0.4448057884971301\n",
      "Epoch 176: val loss 0.640579\n",
      "\n",
      "Epoch %d: train loss %f 177 0.38988332202037174\n",
      "Epoch 177: val loss 0.650675\n",
      "\n",
      "Epoch %d: train loss %f 178 0.4137565642595291\n",
      "Epoch 178: val loss 0.659015\n",
      "\n",
      "Epoch %d: train loss %f 179 0.39982906480630237\n",
      "Epoch 179: val loss 0.645996\n",
      "\n",
      "Epoch %d: train loss %f 180 0.396732601026694\n",
      "Epoch 180: val loss 0.669619\n",
      "\n",
      "Epoch %d: train loss %f 181 0.4379459234575431\n",
      "Epoch 181: val loss 0.668908\n",
      "\n",
      "Epoch %d: train loss %f 182 0.43503418068091076\n",
      "Epoch 182: val loss 0.664718\n",
      "\n",
      "Epoch %d: train loss %f 183 0.4110131983955701\n",
      "Epoch 183: val loss 0.652144\n",
      "\n",
      "Epoch %d: train loss %f 184 0.41163866221904755\n",
      "Epoch 184: val loss 0.642893\n",
      "\n",
      "Epoch %d: train loss %f 185 0.3951639657219251\n",
      "Epoch 185: val loss 0.628975\n",
      "\n",
      "Epoch %d: train loss %f 186 0.4039580225944519\n",
      "Epoch 186: val loss 0.650424\n",
      "\n",
      "Epoch %d: train loss %f 187 0.4152761747439702\n",
      "Epoch 187: val loss 0.673152\n",
      "\n",
      "Epoch %d: train loss %f 188 0.38191554695367813\n",
      "Epoch 188: val loss 0.650841\n",
      "\n",
      "Epoch %d: train loss %f 189 0.41521019488573074\n",
      "Epoch 189: val loss 0.666567\n",
      "\n",
      "Epoch %d: train loss %f 190 0.419174628953139\n",
      "Epoch 190: val loss 0.687843\n",
      "\n",
      "Epoch %d: train loss %f 191 0.3883204683661461\n",
      "Epoch 191: val loss 0.671555\n",
      "\n",
      "Epoch %d: train loss %f 192 0.39112454156080884\n",
      "Epoch 192: val loss 0.650473\n",
      "\n",
      "Epoch %d: train loss %f 193 0.40767985582351685\n",
      "Epoch 193: val loss 0.667009\n",
      "\n",
      "Epoch %d: train loss %f 194 0.4212728962302208\n",
      "Epoch 194: val loss 0.662438\n",
      "\n",
      "Epoch %d: train loss %f 195 0.45599980652332306\n",
      "Epoch 195: val loss 0.644477\n",
      "\n",
      "Epoch %d: train loss %f 196 0.4221612426141898\n",
      "Epoch 196: val loss 0.644964\n",
      "\n",
      "Epoch %d: train loss %f 197 0.40849089870850247\n",
      "Epoch 197: val loss 0.663305\n",
      "\n",
      "Epoch %d: train loss %f 198 0.42979944000641507\n",
      "Epoch 198: val loss 0.672092\n",
      "\n",
      "Epoch %d: train loss %f 199 0.38424931342403096\n",
      "Epoch 199: val loss 0.658550\n",
      "\n",
      "Epoch %d: train loss %f 0 0.6906374196211497\n",
      "Epoch 0: val loss 0.692936\n",
      "\n",
      "Epoch %d: train loss %f 1 0.6857575674851736\n",
      "Epoch 1: val loss 0.691424\n",
      "\n",
      "Epoch %d: train loss %f 2 0.677934134999911\n",
      "Epoch 2: val loss 0.686196\n",
      "\n",
      "Epoch %d: train loss %f 3 0.663741742571195\n",
      "Epoch 3: val loss 0.674760\n",
      "\n",
      "Epoch %d: train loss %f 4 0.6461655000845591\n",
      "Epoch 4: val loss 0.660281\n",
      "\n",
      "Epoch %d: train loss %f 5 0.6280220051606497\n",
      "Epoch 5: val loss 0.653442\n",
      "\n",
      "Epoch %d: train loss %f 6 0.6243660499652227\n",
      "Epoch 6: val loss 0.654993\n",
      "\n",
      "Epoch %d: train loss %f 7 0.6204798370599747\n",
      "Epoch 7: val loss 0.651999\n",
      "\n",
      "Epoch %d: train loss %f 8 0.6234521120786667\n",
      "Epoch 8: val loss 0.660077\n",
      "\n",
      "Epoch %d: train loss %f 9 0.613492359717687\n",
      "Epoch 9: val loss 0.654416\n",
      "\n",
      "Epoch %d: train loss %f 10 0.611151859164238\n",
      "Epoch 10: val loss 0.648621\n",
      "\n",
      "Epoch %d: train loss %f 11 0.6162575781345367\n",
      "Epoch 11: val loss 0.660463\n",
      "\n",
      "Epoch %d: train loss %f 12 0.6010844012101492\n",
      "Epoch 12: val loss 0.652380\n",
      "\n",
      "Epoch %d: train loss %f 13 0.6111669341723124\n",
      "Epoch 13: val loss 0.646595\n",
      "\n",
      "Epoch %d: train loss %f 14 0.5914758269985517\n",
      "Epoch 14: val loss 0.646448\n",
      "\n",
      "Epoch %d: train loss %f 15 0.5891488939523697\n",
      "Epoch 15: val loss 0.649155\n",
      "\n",
      "Epoch %d: train loss %f 16 0.5873295466105143\n",
      "Epoch 16: val loss 0.642804\n",
      "\n",
      "Epoch %d: train loss %f 17 0.5916044414043427\n",
      "Epoch 17: val loss 0.654132\n",
      "\n",
      "Epoch %d: train loss %f 18 0.5771930565436681\n",
      "Epoch 18: val loss 0.652370\n",
      "\n",
      "Epoch %d: train loss %f 19 0.5934631824493408\n",
      "Epoch 19: val loss 0.649129\n",
      "\n",
      "Epoch %d: train loss %f 20 0.5812005748351415\n",
      "Epoch 20: val loss 0.644926\n",
      "\n",
      "Epoch %d: train loss %f 21 0.5719368035594622\n",
      "Epoch 21: val loss 0.651066\n",
      "\n",
      "Epoch %d: train loss %f 22 0.569575309753418\n",
      "Epoch 22: val loss 0.649868\n",
      "\n",
      "Epoch %d: train loss %f 23 0.5681761751572291\n",
      "Epoch 23: val loss 0.656593\n",
      "\n",
      "Epoch %d: train loss %f 24 0.5633562008539835\n",
      "Epoch 24: val loss 0.646750\n",
      "\n",
      "Epoch %d: train loss %f 25 0.5483834172288576\n",
      "Epoch 25: val loss 0.646188\n",
      "\n",
      "Epoch %d: train loss %f 26 0.5752672602732977\n",
      "Epoch 26: val loss 0.668534\n",
      "\n",
      "Epoch %d: train loss %f 27 0.5849871834119161\n",
      "Epoch 27: val loss 0.658153\n",
      "\n",
      "Epoch %d: train loss %f 28 0.5616063599785169\n",
      "Epoch 28: val loss 0.647568\n",
      "\n",
      "Epoch %d: train loss %f 29 0.5505289932092031\n",
      "Epoch 29: val loss 0.648956\n",
      "\n",
      "Epoch %d: train loss %f 30 0.5466064140200615\n",
      "Epoch 30: val loss 0.656020\n",
      "\n",
      "Epoch %d: train loss %f 31 0.5705213248729706\n",
      "Epoch 31: val loss 0.660836\n",
      "\n",
      "Epoch %d: train loss %f 32 0.5320274954040846\n",
      "Epoch 32: val loss 0.637739\n",
      "\n",
      "Epoch %d: train loss %f 33 0.5783928458889326\n",
      "Epoch 33: val loss 0.648665\n",
      "\n",
      "Epoch %d: train loss %f 34 0.5439652055501938\n",
      "Epoch 34: val loss 0.660149\n",
      "\n",
      "Epoch %d: train loss %f 35 0.5412427658836046\n",
      "Epoch 35: val loss 0.655495\n",
      "\n",
      "Epoch %d: train loss %f 36 0.5513406048218409\n",
      "Epoch 36: val loss 0.642682\n",
      "\n",
      "Epoch %d: train loss %f 37 0.5356035853425661\n",
      "Epoch 37: val loss 0.644271\n",
      "\n",
      "Epoch %d: train loss %f 38 0.5257354527711868\n",
      "Epoch 38: val loss 0.662567\n",
      "\n",
      "Epoch %d: train loss %f 39 0.5532707472642263\n",
      "Epoch 39: val loss 0.635823\n",
      "\n",
      "Epoch %d: train loss %f 40 0.5296760325630506\n",
      "Epoch 40: val loss 0.651659\n",
      "\n",
      "Epoch %d: train loss %f 41 0.5305825819571813\n",
      "Epoch 41: val loss 0.671383\n",
      "\n",
      "Epoch %d: train loss %f 42 0.5479133029778799\n",
      "Epoch 42: val loss 0.661293\n",
      "\n",
      "Epoch %d: train loss %f 43 0.5343611985445023\n",
      "Epoch 43: val loss 0.672979\n",
      "\n",
      "Epoch %d: train loss %f 44 0.5285500834385554\n",
      "Epoch 44: val loss 0.652713\n",
      "\n",
      "Epoch %d: train loss %f 45 0.517756330470244\n",
      "Epoch 45: val loss 0.670279\n",
      "\n",
      "Epoch %d: train loss %f 46 0.5242901717623075\n",
      "Epoch 46: val loss 0.677606\n",
      "\n",
      "Epoch %d: train loss %f 47 0.5253342439730962\n",
      "Epoch 47: val loss 0.680930\n",
      "\n",
      "Epoch %d: train loss %f 48 0.5249138822158178\n",
      "Epoch 48: val loss 0.677613\n",
      "\n",
      "Epoch %d: train loss %f 49 0.5159372389316559\n",
      "Epoch 49: val loss 0.665415\n",
      "\n",
      "Epoch %d: train loss %f 50 0.5119980151454607\n",
      "Epoch 50: val loss 0.658324\n",
      "\n",
      "Epoch %d: train loss %f 51 0.5174416601657867\n",
      "Epoch 51: val loss 0.687758\n",
      "\n",
      "Epoch %d: train loss %f 52 0.5124048093954722\n",
      "Epoch 52: val loss 0.668246\n",
      "\n",
      "Epoch %d: train loss %f 53 0.5079642782608668\n",
      "Epoch 53: val loss 0.660059\n",
      "\n",
      "Epoch %d: train loss %f 54 0.5126015618443489\n",
      "Epoch 54: val loss 0.677924\n",
      "\n",
      "Epoch %d: train loss %f 55 0.501008706788222\n",
      "Epoch 55: val loss 0.683292\n",
      "\n",
      "Epoch %d: train loss %f 56 0.5097412392497063\n",
      "Epoch 56: val loss 0.670683\n",
      "\n",
      "Epoch %d: train loss %f 57 0.5267648895581564\n",
      "Epoch 57: val loss 0.663486\n",
      "\n",
      "Epoch %d: train loss %f 58 0.5148483787973722\n",
      "Epoch 58: val loss 0.665981\n",
      "\n",
      "Epoch %d: train loss %f 59 0.5059361929694811\n",
      "Epoch 59: val loss 0.669843\n",
      "\n",
      "Epoch %d: train loss %f 60 0.4715427632133166\n",
      "Epoch 60: val loss 0.667550\n",
      "\n",
      "Epoch %d: train loss %f 61 0.4921383087833722\n",
      "Epoch 61: val loss 0.686165\n",
      "\n",
      "Epoch %d: train loss %f 62 0.4937910462419192\n",
      "Epoch 62: val loss 0.672078\n",
      "\n",
      "Epoch %d: train loss %f 63 0.5021551474928856\n",
      "Epoch 63: val loss 0.680060\n",
      "\n",
      "Epoch %d: train loss %f 64 0.49726051092147827\n",
      "Epoch 64: val loss 0.671952\n",
      "\n",
      "Epoch %d: train loss %f 65 0.5262596209843954\n",
      "Epoch 65: val loss 0.663700\n",
      "\n",
      "Epoch %d: train loss %f 66 0.49743381639321643\n",
      "Epoch 66: val loss 0.665331\n",
      "\n",
      "Epoch %d: train loss %f 67 0.5111633489529291\n",
      "Epoch 67: val loss 0.673471\n",
      "\n",
      "Epoch %d: train loss %f 68 0.4947059129675229\n",
      "Epoch 68: val loss 0.671182\n",
      "\n",
      "Epoch %d: train loss %f 69 0.4891041765610377\n",
      "Epoch 69: val loss 0.668065\n",
      "\n",
      "Epoch %d: train loss %f 70 0.4811932221055031\n",
      "Epoch 70: val loss 0.668788\n",
      "\n",
      "Epoch %d: train loss %f 71 0.4703482463955879\n",
      "Epoch 71: val loss 0.673930\n",
      "\n",
      "Epoch %d: train loss %f 72 0.49433257679144543\n",
      "Epoch 72: val loss 0.684708\n",
      "\n",
      "Epoch %d: train loss %f 73 0.5000824357072512\n",
      "Epoch 73: val loss 0.690845\n",
      "\n",
      "Epoch %d: train loss %f 74 0.5003029505411783\n",
      "Epoch 74: val loss 0.687980\n",
      "\n",
      "Epoch %d: train loss %f 75 0.5035636524359385\n",
      "Epoch 75: val loss 0.671604\n",
      "\n",
      "Epoch %d: train loss %f 76 0.4719471757610639\n",
      "Epoch 76: val loss 0.658471\n",
      "\n",
      "Epoch %d: train loss %f 77 0.49005890885988873\n",
      "Epoch 77: val loss 0.676917\n",
      "\n",
      "Epoch %d: train loss %f 78 0.476884866754214\n",
      "Epoch 78: val loss 0.679846\n",
      "\n",
      "Epoch %d: train loss %f 79 0.4786373699704806\n",
      "Epoch 79: val loss 0.698931\n",
      "\n",
      "Epoch %d: train loss %f 80 0.4998154615362485\n",
      "Epoch 80: val loss 0.696225\n",
      "\n",
      "Epoch %d: train loss %f 81 0.4730239932735761\n",
      "Epoch 81: val loss 0.709999\n",
      "\n",
      "Epoch %d: train loss %f 82 0.48467956731716794\n",
      "Epoch 82: val loss 0.687539\n",
      "\n",
      "Epoch %d: train loss %f 83 0.45421792318423587\n",
      "Epoch 83: val loss 0.678667\n",
      "\n",
      "Epoch %d: train loss %f 84 0.45434803515672684\n",
      "Epoch 84: val loss 0.697051\n",
      "\n",
      "Epoch %d: train loss %f 85 0.44111715257167816\n",
      "Epoch 85: val loss 0.711856\n",
      "\n",
      "Epoch %d: train loss %f 86 0.44792552292346954\n",
      "Epoch 86: val loss 0.688367\n",
      "\n",
      "Epoch %d: train loss %f 87 0.46687227487564087\n",
      "Epoch 87: val loss 0.692281\n",
      "\n",
      "Epoch %d: train loss %f 88 0.49171056350072223\n",
      "Epoch 88: val loss 0.669115\n",
      "\n",
      "Epoch %d: train loss %f 89 0.44120099395513535\n",
      "Epoch 89: val loss 0.677824\n",
      "\n",
      "Epoch %d: train loss %f 90 0.4701799526810646\n",
      "Epoch 90: val loss 0.694275\n",
      "\n",
      "Epoch %d: train loss %f 91 0.47444521884123486\n",
      "Epoch 91: val loss 0.690083\n",
      "\n",
      "Epoch %d: train loss %f 92 0.46698735902706784\n",
      "Epoch 92: val loss 0.691083\n",
      "\n",
      "Epoch %d: train loss %f 93 0.48330465455849964\n",
      "Epoch 93: val loss 0.714271\n",
      "\n",
      "Epoch %d: train loss %f 94 0.44544470061858493\n",
      "Epoch 94: val loss 0.701430\n",
      "\n",
      "Epoch %d: train loss %f 95 0.4781295706828435\n",
      "Epoch 95: val loss 0.749397\n",
      "\n",
      "Epoch %d: train loss %f 96 0.4800085127353668\n",
      "Epoch 96: val loss 0.701043\n",
      "\n",
      "Epoch %d: train loss %f 97 0.47215071444710094\n",
      "Epoch 97: val loss 0.689258\n",
      "\n",
      "Epoch %d: train loss %f 98 0.4882942736148834\n",
      "Epoch 98: val loss 0.678551\n",
      "\n",
      "Epoch %d: train loss %f 99 0.46264712512493134\n",
      "Epoch 99: val loss 0.701235\n",
      "\n",
      "Epoch %d: train loss %f 100 0.45861948281526566\n",
      "Epoch 100: val loss 0.702112\n",
      "\n",
      "Epoch %d: train loss %f 101 0.5101088508963585\n",
      "Epoch 101: val loss 0.737316\n",
      "\n",
      "Epoch %d: train loss %f 102 0.4411270245909691\n",
      "Epoch 102: val loss 0.703929\n",
      "\n",
      "Epoch %d: train loss %f 103 0.45587992916504544\n",
      "Epoch 103: val loss 0.695375\n",
      "\n",
      "Epoch %d: train loss %f 104 0.4565294235944748\n",
      "Epoch 104: val loss 0.705712\n",
      "\n",
      "Epoch %d: train loss %f 105 0.4937436953186989\n",
      "Epoch 105: val loss 0.696970\n",
      "\n",
      "Epoch %d: train loss %f 106 0.45146341621875763\n",
      "Epoch 106: val loss 0.692587\n",
      "\n",
      "Epoch %d: train loss %f 107 0.4511155957976977\n",
      "Epoch 107: val loss 0.724330\n",
      "\n",
      "Epoch %d: train loss %f 108 0.47140784313281375\n",
      "Epoch 108: val loss 0.705908\n",
      "\n",
      "Epoch %d: train loss %f 109 0.4607199802994728\n",
      "Epoch 109: val loss 0.697858\n",
      "\n",
      "Epoch %d: train loss %f 110 0.44317735979954404\n",
      "Epoch 110: val loss 0.691492\n",
      "\n",
      "Epoch %d: train loss %f 111 0.4568157022198041\n",
      "Epoch 111: val loss 0.703718\n",
      "\n",
      "Epoch %d: train loss %f 112 0.4341631978750229\n",
      "Epoch 112: val loss 0.714277\n",
      "\n",
      "Epoch %d: train loss %f 113 0.4446878507733345\n",
      "Epoch 113: val loss 0.744808\n",
      "\n",
      "Epoch %d: train loss %f 114 0.4675929831961791\n",
      "Epoch 114: val loss 0.727565\n",
      "\n",
      "Epoch %d: train loss %f 115 0.4630148460467656\n",
      "Epoch 115: val loss 0.726477\n",
      "\n",
      "Epoch %d: train loss %f 116 0.4563292885820071\n",
      "Epoch 116: val loss 0.717525\n",
      "\n",
      "Epoch %d: train loss %f 117 0.4706582799553871\n",
      "Epoch 117: val loss 0.726812\n",
      "\n",
      "Epoch %d: train loss %f 118 0.4796772102514903\n",
      "Epoch 118: val loss 0.740524\n",
      "\n",
      "Epoch %d: train loss %f 119 0.4613054891427358\n",
      "Epoch 119: val loss 0.706214\n",
      "\n",
      "Epoch %d: train loss %f 120 0.45681711037953693\n",
      "Epoch 120: val loss 0.736612\n",
      "\n",
      "Epoch %d: train loss %f 121 0.43724850316842395\n",
      "Epoch 121: val loss 0.729817\n",
      "\n",
      "Epoch %d: train loss %f 122 0.4378010382254918\n",
      "Epoch 122: val loss 0.734475\n",
      "\n",
      "Epoch %d: train loss %f 123 0.4090629853308201\n",
      "Epoch 123: val loss 0.706472\n",
      "\n",
      "Epoch %d: train loss %f 124 0.42766959965229034\n",
      "Epoch 124: val loss 0.745609\n",
      "\n",
      "Epoch %d: train loss %f 125 0.42917679498593014\n",
      "Epoch 125: val loss 0.720969\n",
      "\n",
      "Epoch %d: train loss %f 126 0.413174827893575\n",
      "Epoch 126: val loss 0.740064\n",
      "\n",
      "Epoch %d: train loss %f 127 0.42656274139881134\n",
      "Epoch 127: val loss 0.715866\n",
      "\n",
      "Epoch %d: train loss %f 128 0.43628234912951785\n",
      "Epoch 128: val loss 0.754778\n",
      "\n",
      "Epoch %d: train loss %f 129 0.4620274603366852\n",
      "Epoch 129: val loss 0.740840\n",
      "\n",
      "Epoch %d: train loss %f 130 0.43179046114285785\n",
      "Epoch 130: val loss 0.748583\n",
      "\n",
      "Epoch %d: train loss %f 131 0.4431990037361781\n",
      "Epoch 131: val loss 0.715787\n",
      "\n",
      "Epoch %d: train loss %f 132 0.42191170404354733\n",
      "Epoch 132: val loss 0.735456\n",
      "\n",
      "Epoch %d: train loss %f 133 0.4364193181196849\n",
      "Epoch 133: val loss 0.747996\n",
      "\n",
      "Epoch %d: train loss %f 134 0.4201352844635646\n",
      "Epoch 134: val loss 0.740220\n",
      "\n",
      "Epoch %d: train loss %f 135 0.44486529380083084\n",
      "Epoch 135: val loss 0.747571\n",
      "\n",
      "Epoch %d: train loss %f 136 0.4221979354818662\n",
      "Epoch 136: val loss 0.740472\n",
      "\n",
      "Epoch %d: train loss %f 137 0.4285626485943794\n",
      "Epoch 137: val loss 0.737520\n",
      "\n",
      "Epoch %d: train loss %f 138 0.4225360030929248\n",
      "Epoch 138: val loss 0.742241\n",
      "\n",
      "Epoch %d: train loss %f 139 0.4205196077624957\n",
      "Epoch 139: val loss 0.741496\n",
      "\n",
      "Epoch %d: train loss %f 140 0.45792968819538754\n",
      "Epoch 140: val loss 0.720817\n",
      "\n",
      "Epoch %d: train loss %f 141 0.4437987332542737\n",
      "Epoch 141: val loss 0.738964\n",
      "\n",
      "Epoch %d: train loss %f 142 0.42428599173823994\n",
      "Epoch 142: val loss 0.757188\n",
      "\n",
      "Epoch %d: train loss %f 143 0.428316630423069\n",
      "Epoch 143: val loss 0.801859\n",
      "\n",
      "Epoch %d: train loss %f 144 0.44056705633799237\n",
      "Epoch 144: val loss 0.749947\n",
      "\n",
      "Epoch %d: train loss %f 145 0.43652895589669544\n",
      "Epoch 145: val loss 0.745309\n",
      "\n",
      "Epoch %d: train loss %f 146 0.4079698746403058\n",
      "Epoch 146: val loss 0.743011\n",
      "\n",
      "Epoch %d: train loss %f 147 0.41240975509087247\n",
      "Epoch 147: val loss 0.743900\n",
      "\n",
      "Epoch %d: train loss %f 148 0.4172530584037304\n",
      "Epoch 148: val loss 0.762224\n",
      "\n",
      "Epoch %d: train loss %f 149 0.42801245550314587\n",
      "Epoch 149: val loss 0.743635\n",
      "\n",
      "Epoch %d: train loss %f 150 0.3988473489880562\n",
      "Epoch 150: val loss 0.738181\n",
      "\n",
      "Epoch %d: train loss %f 151 0.4357077044745286\n",
      "Epoch 151: val loss 0.760918\n",
      "\n",
      "Epoch %d: train loss %f 152 0.41402557740608853\n",
      "Epoch 152: val loss 0.733823\n",
      "\n",
      "Epoch %d: train loss %f 153 0.4393111765384674\n",
      "Epoch 153: val loss 0.763118\n",
      "\n",
      "Epoch %d: train loss %f 154 0.4120686451594035\n",
      "Epoch 154: val loss 0.774604\n",
      "\n",
      "Epoch %d: train loss %f 155 0.4054669638474782\n",
      "Epoch 155: val loss 0.764063\n",
      "\n",
      "Epoch %d: train loss %f 156 0.4390437702337901\n",
      "Epoch 156: val loss 0.767604\n",
      "\n",
      "Epoch %d: train loss %f 157 0.40162405371665955\n",
      "Epoch 157: val loss 0.778127\n",
      "\n",
      "Epoch %d: train loss %f 158 0.41599077358841896\n",
      "Epoch 158: val loss 0.777226\n",
      "\n",
      "Epoch %d: train loss %f 159 0.40765204280614853\n",
      "Epoch 159: val loss 0.767926\n",
      "\n",
      "Epoch %d: train loss %f 160 0.3821401869257291\n",
      "Epoch 160: val loss 0.743735\n",
      "\n",
      "Epoch %d: train loss %f 161 0.4002757693330447\n",
      "Epoch 161: val loss 0.741349\n",
      "\n",
      "Epoch %d: train loss %f 162 0.38842616478602093\n",
      "Epoch 162: val loss 0.772280\n",
      "\n",
      "Epoch %d: train loss %f 163 0.3949764221906662\n",
      "Epoch 163: val loss 0.783044\n",
      "\n",
      "Epoch %d: train loss %f 164 0.44012869894504547\n",
      "Epoch 164: val loss 0.749495\n",
      "\n",
      "Epoch %d: train loss %f 165 0.37815768271684647\n",
      "Epoch 165: val loss 0.815908\n",
      "\n",
      "Epoch %d: train loss %f 166 0.3951549381017685\n",
      "Epoch 166: val loss 0.762391\n",
      "\n",
      "Epoch %d: train loss %f 167 0.4140189712246259\n",
      "Epoch 167: val loss 0.824536\n",
      "\n",
      "Epoch %d: train loss %f 168 0.46205518146355945\n",
      "Epoch 168: val loss 0.813563\n",
      "\n",
      "Epoch %d: train loss %f 169 0.3790195360779762\n",
      "Epoch 169: val loss 0.762079\n",
      "\n",
      "Epoch %d: train loss %f 170 0.40031787504752475\n",
      "Epoch 170: val loss 0.756075\n",
      "\n",
      "Epoch %d: train loss %f 171 0.4144578278064728\n",
      "Epoch 171: val loss 0.760769\n",
      "\n",
      "Epoch %d: train loss %f 172 0.4116004928946495\n",
      "Epoch 172: val loss 0.772075\n",
      "\n",
      "Epoch %d: train loss %f 173 0.37753452981511754\n",
      "Epoch 173: val loss 0.782440\n",
      "\n",
      "Epoch %d: train loss %f 174 0.3830727661649386\n",
      "Epoch 174: val loss 0.795837\n",
      "\n",
      "Epoch %d: train loss %f 175 0.3798762373626232\n",
      "Epoch 175: val loss 0.821698\n",
      "\n",
      "Epoch %d: train loss %f 176 0.40403976663947105\n",
      "Epoch 176: val loss 0.796340\n",
      "\n",
      "Epoch %d: train loss %f 177 0.3688696088890235\n",
      "Epoch 177: val loss 0.829346\n",
      "\n",
      "Epoch %d: train loss %f 178 0.40915607164303464\n",
      "Epoch 178: val loss 0.792010\n",
      "\n",
      "Epoch %d: train loss %f 179 0.41420811911424\n",
      "Epoch 179: val loss 0.852955\n",
      "\n",
      "Epoch %d: train loss %f 180 0.42402832955121994\n",
      "Epoch 180: val loss 0.805340\n",
      "\n",
      "Epoch %d: train loss %f 181 0.3828890124956767\n",
      "Epoch 181: val loss 0.794278\n",
      "\n",
      "Epoch %d: train loss %f 182 0.3896649827559789\n",
      "Epoch 182: val loss 0.859868\n",
      "\n",
      "Epoch %d: train loss %f 183 0.3744817301630974\n",
      "Epoch 183: val loss 0.831980\n",
      "\n",
      "Epoch %d: train loss %f 184 0.3751906578739484\n",
      "Epoch 184: val loss 0.827053\n",
      "\n",
      "Epoch %d: train loss %f 185 0.36509935806194943\n",
      "Epoch 185: val loss 0.853681\n",
      "\n",
      "Epoch %d: train loss %f 186 0.41614824409286183\n",
      "Epoch 186: val loss 0.864245\n",
      "\n",
      "Epoch %d: train loss %f 187 0.41476993014415103\n",
      "Epoch 187: val loss 0.836996\n",
      "\n",
      "Epoch %d: train loss %f 188 0.41219573964675266\n",
      "Epoch 188: val loss 0.842907\n",
      "\n",
      "Epoch %d: train loss %f 189 0.40447277079025906\n",
      "Epoch 189: val loss 0.803429\n",
      "\n",
      "Epoch %d: train loss %f 190 0.4167102575302124\n",
      "Epoch 190: val loss 0.817355\n",
      "\n",
      "Epoch %d: train loss %f 191 0.3928718902170658\n",
      "Epoch 191: val loss 0.806446\n",
      "\n",
      "Epoch %d: train loss %f 192 0.409795418381691\n",
      "Epoch 192: val loss 0.855125\n",
      "\n",
      "Epoch %d: train loss %f 193 0.4187745191156864\n",
      "Epoch 193: val loss 0.811242\n",
      "\n",
      "Epoch %d: train loss %f 194 0.41314417372147244\n",
      "Epoch 194: val loss 0.827043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment_res_dir = './res/'\n",
    "subjs_test_stats = {}\n",
    "for train_subject in all_subjects:\n",
    "    path_to_subj = prepare_dirs(experiment_res_dir, train_subject)\n",
    "    x = subjects[train_subject][0]\n",
    "    x = x.transpose(0, 2, 1)[:, np.newaxis, :, :]\n",
    "    y=subjects[train_subject][1]\n",
    "    test_stats, model = cv_per_subj_test(x, y, params, path_to_subj,test_on_last_block=True, plot_fold_history=True)\n",
    "    subjs_test_stats[train_subject] = test_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece will save general information about training in file res.txt contating validating, test naive and test ensamble AUCS for each subject and mean AUC for all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results_table(subjs_test_stats, path_to_exp=experiment_res_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjs_test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(subjs_test_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
